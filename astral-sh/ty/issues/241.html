<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>add a test for `ty_benchmark` that asserts we have roughly the number of expected errors - astral-sh/ty #241</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>add a test for `ty_benchmark` that asserts we have roughly the number of expected errors</h1>

    <div class="meta">
        <span class="state-icon state-closed"></span>
        <a href="https://github.com/astral-sh/ty/issues/241">#241</a>
        opened by <a href="https://github.com/AlexWaygood">@AlexWaygood</a>
        on 2024-09-03 13:43
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header">Issue opened by <a href="https://github.com/AlexWaygood">@AlexWaygood</a> on 2024-09-03 13:43</div>
            <div class="timeline-body"><p>It seems like uv had some change in 0.4 that meant our red-knot benchmark silently stopped working, in that the dependencies for our benchmark projects weren't being installed into the right virtual environment: https://github.com/astral-sh/ruff/pull/13228#issuecomment-2326322306. (Probably a skill issue in our usage of uv rather than a breaking change from uv.)</p>
<p>It would be great if we could have some kind of test in CI that checks that the tools we're running emit roughly the number of errors we expect, so that it doesn't silently become invalid in the future.</p>
<p>Relatedly: mypy emits one error when checking black via our benchmark infrastructure, and we're not sure why that is. Black is compiled with mypyc and they run mypy in CI, so there should probably be 0 errors there! We're probably invoking mypy slightly wrong somehow? If we could get a clean run of mypy on black in our benchmark, that would make it significantly easier to add this kind of test.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">help wanted</span> added by @AlexWaygood on 2024-09-03 13:43</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">testing</span> added by @AlexWaygood on 2024-09-03 13:43</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/AlexWaygood">@AlexWaygood</a> on 2024-09-03 20:08</div>
            <div class="timeline-body"><p>https://github.com/astral-sh/ruff/pull/13235 fixes the issue where mypy was emitting an error on black when invoked from the benchmark script</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from "Add a test for `knot_benchmark` that asserts we have roughly the number of expected errors" to "[red-knot] add a test for `knot_benchmark` that asserts we have roughly the number of expected errors" by @carljm on 2025-03-27 19:00</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MatthewMckee4">@MatthewMckee4</a> on 2025-03-29 15:00</div>
            <div class="timeline-body"><p>Is this what <code>mypy_primer</code> does?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/AlexWaygood">@AlexWaygood</a> on 2025-04-01 12:34</div>
            <div class="timeline-body"><blockquote>
<p>Is this what <code>mypy_primer</code> does?</p>
</blockquote>
<p>no, <code>mypy_primer</code> is a separate tool to <a href="https://github.com/astral-sh/ruff/tree/main/scripts/knot_benchmark"><code>knot_benchmark</code></a>. This issue is about adding a regression test for <a href="https://github.com/astral-sh/ruff/tree/main/scripts/knot_benchmark"><code>knot_benchmark</code></a> that ensures that it doesn't unexpectedly stop working.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from "[red-knot] add a test for `knot_benchmark` that asserts we have roughly the number of expected errors" to "add a test for `knot_benchmark` that asserts we have roughly the number of expected errors" by @MichaReiser on 2025-05-07 15:27</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/alex-gregory-ds">@alex-gregory-ds</a> on 2025-10-19 20:31</div>
            <div class="timeline-body"><p>Is there still interest in this? If so I would be interested in contributing.</p>
<p>Also, has the <code>knot_benchmark</code> been renamed to the <code>ty_benchmark</code>? Looks like it was changed in this commit: <a href="https://github.com/astral-sh/ruff/tree/b51c4f82ea6b6e40dcf3622dd009d463bb7e2247">b51c4f82</a>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from "add a test for `knot_benchmark` that asserts we have roughly the number of expected errors" to "add a test for `ty_benchmark` that asserts we have roughly the number of expected errors" by @MichaReiser on 2025-10-20 06:21</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/AlexWaygood">@AlexWaygood</a> on 2025-10-23 13:18</div>
            <div class="timeline-body"><p>Hey @alex-gregory-ds -- yes, I still think it would probably be useful to have some kind of CI check to enforce this! Feel free to have at it :-)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Assigned to @alex-gregory-ds by @AlexWaygood on 2025-10-23 13:18</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/alex-gregory-ds">@alex-gregory-ds</a> on 2025-10-26 09:43</div>
            <div class="timeline-body"><p>Great to hear. I have a few ideas for implementing this but it would be good to get your advice.</p>
<p>We could capture the output for each hyperfine run and count the number of errors and warnings in stdout. The lines we'd have to change are <a href="https://github.com/astral-sh/ruff/blob/64ab79e5721ec6fdd2182fbf9d39a26534ccca43/scripts/ty_benchmark/src/benchmark/__init__.py#L73">here</a>. To capture the <code>ty</code> outputs from hypervine we would have to add the <code>--show-output</code> argument which according to the hypervine documentation should only be used for debugging. This may affect the benchmark times. Here is the relevant hypervine documentation:</p>
<pre><code>--show-output
    Print the stdout and stderr of the benchmark instead of suppressing
    it. This will increase the time it takes for benchmarks to run, so it
    should only be used for debugging purposes or when trying to benchmark
    output speed.
</code></pre>
<p>To ensure the benchmark times are unaffected by counting the number of warnings/errors in stdout, should we run an extra run at the end of benchmarking that captures the standard output from which we can then compute the number of warnings and errors?</p>
<p>Finally, I suspect that counting the number of instances of the words Error/Warning may not be the best way to count the number of errors/warnings. But, I cannot find a <code>ty</code> option that returns the number of errors/warnings. Should we consider adding a command line option to <code>ty</code> that returns the number of errors/warnings that were raised?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2025-10-27 06:52</div>
            <div class="timeline-body"><blockquote>
<p>To ensure the benchmark times are unaffected by counting the number of warnings/errors in stdout, should we run an extra run at the end of benchmarking that captures the standard output from which we can then compute the number of warnings and errors?</p>
</blockquote>
<p>I agree, we should avoid any unnecessary work when running hyperfine for a fair comparison. I suggest a separate run before running hyperfine as it doesn't make much sense to perform any measurement if it is incorrect anyway.</p>
<blockquote>
<p>Finally, I suspect that counting the number of instances of the words Error/Warning may not be the best way to count the number of errors/warnings. But, I cannot find a ty option that returns the number of errors/warnings. Should we consider adding a command line option to ty that returns the number of errors/warnings that were raised?</p>
</blockquote>
<p>I'm leaning towards using <code>--output-format=concise</code> and simply counting the lines. We should also check that ty exits with 0 and use <code>--exit-zero</code> (that's even something that hyperfine can do for us)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Added to milestone "Z post-stable" by @carljm on 2025-11-13 15:48</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Removed from milestone "Z post-stable" by @carljm on 2025-11-18 16:10</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2025-11-20 09:42</div>
            <div class="timeline-body"><p>I'm inclined to close this. https://github.com/astral-sh/ruff/pull/21536 adds some improvements to catch non-type-checking errors. E.g. if a CLI option no longer works.</p>
<p>It's also important that we update the benchmarks periodically to use newer mypy, pyrefly, ty, and pyright versions and update the benchmarked projects. Whenever we do that, we'll have to review every single project manually to see if there are new missing dependencies, whether the Python versions are still correct, whether the project layout has changed, etc. This process is entirely manual (I use <code>uv run benchmark --tool ty -v --project &lt;each_project&gt;</code> for every project and tool) and I added documentation mentioning the expected diagnostics when the tools were last updated.</p>
<p>While a more automated way of asserting the diagnostics would certainly be nice, I'm not sure it's worth the complexity, especially since updating still requires manual verification.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/AlexWaygood">@AlexWaygood</a> on 2025-11-20 10:02</div>
            <div class="timeline-body"><p>I'm fine with that. This definitely turned out to be more complex than I anticipated.</p>
<p>Thanks for the contribution anyway, @alex-gregory-ds â€” really appreciate it! I'm sorry I led you astray here :-(</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2025-11-20 10:41</div>
            <div class="timeline-body"><blockquote>
<p>Thanks for the contribution anyway, @alex-gregory-ds â€” really appreciate it! I'm sorry I led you astray here :-(</p>
</blockquote>
<p>I still think it would be cool to snapshot the diagnostics somehow ðŸ˜† Maybe by having a different command mode <code>uv run benchmark --snapshot</code>? This might not even be that hard because all it requires is to snapshot the outputs for each benchmark and tool. The files might just get very very large</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by @MichaReiser on 2025-11-25 07:58</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-10 02:10:57 UTC
    </footer>
</body>
</html>
