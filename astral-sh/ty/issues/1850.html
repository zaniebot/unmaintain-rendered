<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ignoring module types entirely - astral-sh/ty #1850</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>Ignoring module types entirely</h1>

    <div class="meta">
        <span class="state-icon state-closed"></span>
        <a href="https://github.com/astral-sh/ty/issues/1850">#1850</a>
        opened by <a href="https://github.com/galah92">@galah92</a>
        on 2025-12-11 07:43
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header">Issue opened by <a href="https://github.com/galah92">@galah92</a> on 2025-12-11 07:43</div>
            <div class="timeline-body"><h3>Question</h3>
<p>Using <code>from transformers import AutoModel, AutoTokenizer</code>, since <code>transformers</code> is poorly typed, I'm getting many usage errors from <code>AutoModel</code> and <code>AutoTokenizer</code>. Is it possible to ignore the entire <code>transformers</code> package from <code>ty</code> perspective?</p>
<h3>Version</h3>
<p><em>No response</em></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">question</span> added by @galah92 on 2025-12-11 07:43</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/sharkdp">@sharkdp</a> on 2025-12-11 07:47</div>
            <div class="timeline-body"><p>Thank you for reporting this.</p>
<blockquote>
<p>I'm getting many usage errors from <code>AutoModel</code> and <code>AutoTokenizer</code></p>
</blockquote>
<p>Can you please share some code and some of the errors that you are seeing?</p>
<blockquote>
<p>since <code>transformers</code> is poorly typed</p>
</blockquote>
<p>Ideally, ty shouldn't raise a lot of errors when using classes and functions from <em>untyped</em> libraries. What exactly do you mean by &quot;poorly typed&quot;?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/galah92">@galah92</a> on 2025-12-11 08:10</div>
            <div class="timeline-body"><blockquote>
<p>Can you please share some code and some of the errors that you are seeing?</p>
</blockquote>
<pre><code class="language-python">from transformers import AutoModel, AutoTokenizer

# Global cache pattern (common in ML code)
_model: AutoModel | None = None
_tokenizer: AutoTokenizer | None = None


def load_model() -&gt; tuple[AutoModel, AutoTokenizer]:
    &quot;&quot;&quot;ty error: invalid-return-type due to None narrowing not working.&quot;&quot;&quot;
    global _model, _tokenizer
    if _model is not None:
        # ty: tuple[..., AutoTokenizer | None] not assignable to tuple[..., AutoTokenizer]
        return _model, _tokenizer
    _tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
    # ty warning: possibly-missing-attribute on from_pretrained
    _model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
    return _model, _tokenizer


def count_tokens(text: str, tokenizer: AutoTokenizer | None) -&gt; int:
    &quot;&quot;&quot;ty error: unresolved-attribute on encode().&quot;&quot;&quot;
    if tokenizer:
        # ty: Object of type `AutoTokenizer` has no attribute `encode`
        return len(tokenizer.encode(text))
    return len(text.split())


def run_inference(model: AutoModel, tokenizer: AutoTokenizer) -&gt; str:
    &quot;&quot;&quot;ty error: unresolved-attribute on custom methods like infer().&quot;&quot;&quot;
    # ty: Object of type `AutoModel | AutoModel` has no attribute `infer`
    # (DeepSeek-OCR adds infer() method, but even standard methods cause issues)
    output = model.infer(tokenizer, prompt=&quot;Hello&quot;)
    return output
</code></pre>
<p>It looks like <code>transformers</code> doesn't provide better classes to use that has <code>from_pretrained</code> / <code>encode</code>.</p>
<blockquote>
<p>Ideally, ty shouldn't raise a lot of errors when using classes and functions from <em>untyped</em> libraries. What exactly do you mean by &quot;poorly typed&quot;?</p>
</blockquote>
<p>It's not that classes are untyped entirely, it's just &quot;poorly&quot;. I expected <code>AutoModel</code> to have <code>from_pretrained</code> or to inherit from a base class that has that, for example.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/sharkdp">@sharkdp</a> on 2025-12-11 08:29</div>
            <div class="timeline-body"><p>I see, thank you for providing examples.</p>
<blockquote>
<pre><code class="language-py">global _model, _tokenizer
    if _model is not None:
        # ty: tuple[..., AutoTokenizer | None] not assignable to tuple[..., AutoTokenizer]
</code></pre>
</blockquote>
<p>This is not related to <code>transformers</code>. You probably want to check <code>if _model is not None and _tokenizer is not None</code>, or otherwise <code>ty</code> can't narrow <code>_tokenizer</code> from <code>AutoTokenizer | None</code> to <code>AutoTokenizer</code></p>
<blockquote>
<pre><code class="language-py">    _tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
    # ty warning: possibly-missing-attribute on from_pretrained
</code></pre>
</blockquote>
<p>I can't reproduce this. It also doesn't seem like an error that ty would throw? Did you see a <code>possibly-missing-attribute</code> on <code>AutoTokenizer</code> or <code>AutoModel</code> instead?</p>
<blockquote>
<pre><code class="language-py">        # ty: Object of type `AutoTokenizer` has no attribute `encode`
        return len(tokenizer.encode(text))
</code></pre>
</blockquote>
<p>Does this code work? I couldn't find this method in the documentation, and if I try it at runtime, it fails:</p>
<pre><code class="language-pycon">&gt;&gt;&gt; from transformers import AutoTokenizer
None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
&gt;&gt;&gt; AutoTokenizer.encode
Traceback (most recent call last):
  File &quot;&lt;python-input-1&gt;&quot;, line 1, in &lt;module&gt;
    AutoTokenizer.encode
AttributeError: type object 'AutoTokenizer' has no attribute 'encode'
</code></pre>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/galah92">@galah92</a> on 2025-12-11 08:49</div>
            <div class="timeline-body"><blockquote>
<blockquote>
<p>global _model, _tokenizer
if _model is not None:
# ty: tuple[..., AutoTokenizer | None] not assignable to tuple[..., AutoTokenizer]</p>
</blockquote>
<p>This is not related to <code>transformers</code>. You probably want to check <code>if _model is not None and _tokenizer is not None</code>, or otherwise <code>ty</code> can't narrow <code>_tokenizer</code> from <code>AutoTokenizer | None</code> to <code>AutoTokenizer</code></p>
</blockquote>
<p>Correct, sorry for that.</p>
<blockquote>
<blockquote>
<p>_tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
# ty warning: possibly-missing-attribute on from_pretrained</p>
</blockquote>
<p>I can't reproduce this. It also doesn't seem like an error that ty would throw? Did you see a <code>possibly-missing-attribute</code> on <code>AutoTokenizer</code> or <code>AutoModel</code> instead?</p>
</blockquote>
<p>Yes, on <code>AutoModel</code>.</p>
<blockquote>
<blockquote>
<pre><code class="language-python"># ty: Object of type `AutoTokenizer` has no attribute `encode`
        return len(tokenizer.encode(text))
</code></pre>
</blockquote>
<p>Does this code work? I couldn't find this method in the documentation, and if I try it at runtime, it fails:</p>
</blockquote>
<p>It works at runtime, I guess it depends on the exact tokenizer that's being loaded, so it's like a factory.</p>
<hr />
<p>EDIT: to clarify, I'm working with https://huggingface.co/deepseek-ai/DeepSeek-OCR.</p>
<p>It looks like they defined their own <code>DeepSeek</code> types but these might not be available until a newer <code>transformers</code> version will be released, which is why <code>AutoModel</code> &amp; <code>AutoTokenizer</code> are helpful. So they aim to operate without type checking, and given that, I wonder if it's possible to &quot;shut-down&quot; <code>ty</code> for the entire package.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/sharkdp">@sharkdp</a> on 2025-12-11 09:08</div>
            <div class="timeline-body"><blockquote>
<blockquote>
<p>I can't reproduce this. It also doesn't seem like an error that ty would throw? Did you see a <code>possibly-missing-attribute</code> on <code>AutoTokenizer</code> or <code>AutoModel</code> instead?</p>
</blockquote>
<p>Yes, on <code>AutoModel</code>.</p>
</blockquote>
<p>In that case, can you please share the actual ty error message? Ideally, the full diagnostic that you get when running <code>ty check</code> from the terminal?</p>
<blockquote>
<blockquote>
<blockquote>
<pre><code class="language-py"># ty: Object of type `AutoTokenizer` has no attribute `encode`
        return len(tokenizer.encode(text))
</code></pre>
</blockquote>
<p>Does this code work? I couldn't find this method in the documentation, and if I try it at runtime, it fails:</p>
</blockquote>
<p>It works at runtime, I guess it depends on the exact tokenizer that's being loaded, so it's like a factory.</p>
</blockquote>
<p>One trick to work around this would be to add a new class that inherits from <code>AutoTokenizer</code> and <code>Any</code>. This will still give you autocompletion for all methods that actually exist on <code>AutoTokenizer</code>, but silence any errors if you access attributes that do not (seem to) exist:</p>
<pre><code class="language-py">from transformers import AutoTokenizer as _AutoTokenizer

class AutoTokenizerAutoTokenizer, Any):
    pass

def f(tokenizer: AutoTokenizer):
    tokenizer.encode()  # no error here
</code></pre>
<p>See full example in this playground: https://play.ty.dev/625acadd-e05c-40cb-a547-d2029ee43282</p>
<p>A related method that is specific to ty is to use intersection types. This way, you could intersect <code>AutoTokenizer &amp; Any</code> to achieve the same effect:</p>
<pre><code class="language-py">if TYPE_CHECKING:
    from ty_extensions import Intersection
    type AutoTokenizer = Intersection[_AutoTokenizer, Any]
</code></pre>
<p>See full example in this playground: https://play.ty.dev/392d85ad-5916-42fe-be92-6289b111c604</p>
<p>And finally, you can always just type <code>tokenizer</code> as <code>Any</code>. This does not give you auto-completion, but also silences all errors.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/galah92">@galah92</a> on 2025-12-11 09:28</div>
            <div class="timeline-body"><p>Ok cool, this definitely answers my use case. Thanks!</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by @sharkdp on 2025-12-11 09:29</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-10 02:11:52 UTC
    </footer>
</body>
</html>
