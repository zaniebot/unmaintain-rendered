<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Automatic CPU/GPU wheel variant selection for all packages (not just PyTorch) - astral-sh/uv #16522</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>Automatic CPU/GPU wheel variant selection for all packages (not just PyTorch)</h1>

    <div class="meta">
        <span class="state-icon state-open"></span>
        <a href="https://github.com/astral-sh/uv/issues/16522">#16522</a>
        opened by <a href="https://github.com/muhammad-fiaz">@muhammad-fiaz</a>
        on 2025-10-30 18:49
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/muhammad-fiaz">@muhammad-fiaz</a></div>
            <div class="timeline-body"><p>I am requesting a feature that allows <code>uv</code> to automatically select the correct <strong>CPU or GPU backend wheel variant</strong> for packages that ship multiple prebuilt versions - not only for PyTorch, but also for ecosystem libraries like <strong><code>xformers</code></strong> and <strong><code>flash-attn</code></strong>.</p>
<p>Currently:</p>
<ul>
<li><p><code>torch-backend = &quot;auto&quot;</code> correctly installs CUDA / ROCm / CPU variants for <strong>PyTorch</strong></p>
</li>
<li><p>But companion packages like <code>xformers</code> and <code>flash-attn</code> are <strong>not matched</strong> to the selected backend</p>
</li>
<li><p>This frequently results in:</p>
<ul>
<li><strong>CUDA PyTorch</strong></li>
<li><strong>CPU-only xformers</strong></li>
<li>-&gt; leading to missing CUDA kernels and runtime errors</li>
</ul>
</li>
</ul>
<p>This happens because the CUDA wheels for <code>xformers</code> and <code>flash-attn</code> are not served fully on PyPI when cuda enabled , so <code>uv</code> (like pip) defaults to CPU wheels.
And because <code>xformers</code> is installed without CUDA support, the resolver may downgrade PyTorch to CPU, or install mismatched wheel variants, causing conflicts and inconsistent environments. Avoiding this currently requires optional dependencies, platform markers, or manual wheel URLs, which creates unnecessary overhead.</p>
<hr>
<strong>Requested Behavior</strong>
<p>Extend <code>torch-backend</code> logic into a <strong>general backend-aware resolver</strong>, e.g.:</p>
<pre><code># uv.toml
[tool.uv.pip]
backend = &quot;auto&quot;                        # Detect CPU / CUDA / ROCm
auto-match-packages = [&quot;xformers&quot;, &quot;flash-attn&quot;]
</code></pre>
<p>Then:</p>
<pre><code>uv sync
</code></pre>
<p><code>uv</code> should:</p>
<ul>
<li>Detect the system backend (CPU / CUDA version / ROCm)</li>
<li>Select the correct wheel sources</li>
<li>Install <strong>matching</strong> GPU/CPU wheel variants across packages consistently</li>
</ul>
<hr>
<p>Additionally, I am requesting support for <strong>backend-aware index selection</strong> when multiple CUDA/CPU indexes are defined, e.g.:</p>
<pre><code>[tool.uv.sources]
torch = [
  { index = &quot;pytorch-cpu&quot; },
  { index = &quot;pytorch-cu128&quot;, marker = &quot;sys_platform == &#x27;linux&#x27; or sys_platform == &#x27;win32&#x27;&quot; },
  { index = &quot;pytorch-cu129&quot;, marker = &quot;sys_platform == &#x27;linux&#x27; or sys_platform == &#x27;win32&#x27;&quot; },
  { index = &quot;pytorch-cu130&quot;, marker = &quot;sys_platform == &#x27;linux&#x27; or sys_platform == &#x27;win32&#x27;&quot; },
]

torchvision = [
  { index = &quot;pytorch-cpu&quot; },
  { index = &quot;pytorch-cu128&quot;, marker = &quot;sys_platform == &#x27;linux&#x27; or sys_platform == &#x27;win32&#x27;&quot; },
  { index = &quot;pytorch-cu129&quot;, marker = &quot;sys_platform == &#x27;linux&#x27; or sys_platform == &#x27;win32&#x27;&quot; },
  { index = &quot;pytorch-cu130&quot;, marker = &quot;sys_platform == &#x27;linux&#x27; or sys_platform == &#x27;win32&#x27;&quot; },
]

[[tool.uv.index]]
name = &quot;pytorch-cu128&quot;
url = &quot;https://download.pytorch.org/whl/cu128&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu129&quot;
url = &quot;https://download.pytorch.org/whl/cu129&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu130&quot;
url = &quot;https://download.pytorch.org/whl/cu130&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cpu&quot;
url = &quot;https://download.pytorch.org/whl/cpu&quot;
explicit = true
</code></pre>
<p><code>uv</code> should automatically select the correct index based on detected backend, and ensure that companion packages (such as <code>xformers</code> and <code>flash-attn</code>) resolve against the same backend index without requiring manual markers or optional dependency trees.</p>
<blockquote>
<p>Note: this feature may eliminate the need for optional dependencies for simplicity :)</p>
</blockquote>
<strong>Why This Matters</strong>
<ul>
<li><p><code>xformers</code> GPU wheels must match <strong>PyTorch&#x27;s CUDA version</strong></p>
</li>
<li><p>Incorrect fallback to CPU wheels causes:</p>
<ul>
<li>Failed fused attention ops</li>
<li>Silent CPU fallback and performance drops</li>
<li>Common runtime errors (<code>No available CUDA kernel</code>)</li>
</ul>
</li>
<li><p>This is currently the <strong>failure point</strong> for new ML users setting up environments</p>
</li>
</ul>
<hr>
<strong>I am aware of existing docs</strong>
<p>This request <strong>builds upon</strong>:
<a href="https://docs.astral.sh/uv/guides/integration/pytorch/">https://docs.astral.sh/uv/guides/integration/pytorch/</a>
<a href="https://docs.astral.sh/uv/reference/settings/#pip_torch-backend">https://docs.astral.sh/uv/reference/settings/#pip_torch-backend</a>
<a href="https://docs.astral.sh/uv/concepts/projects/config/#augmenting-build-dependencies">https://docs.astral.sh/uv/concepts/projects/config/#augmenting-build-dependencies</a></p>
<p>But expands automatic backend resolution <strong>beyond PyTorch</strong> to the entire CUDA/ROCm ecosystem.</p>
Example
<p>Current behavior:</p>
<pre><code>uv add torch xformers flash-attn
</code></pre>
<p>Since no CUDA index is provided, this installs:</p>
<pre><code>torch (CPU)
xformers (CPU)
flash-attn (CPU)
</code></pre>
<hr>
<p>Trying to force CUDA manually:</p>
<pre><code>uv add torch xformers flash-attn --index-url https://download.pytorch.org/whl/cu128
</code></pre>
<p>Current result with uv:</p>
<pre><code>torch (CUDA 12.8)
xformers (CPU-only or mismatched version)
flash-attn (CPU-only or fails to resolve)
</code></pre>
<p>Because <code>uv</code> only applies the index to <code>torch</code> and not to related ecosystem packages, leading to conflicts and inconsistent backends.</p>
<hr>
<p>With the requested feature:</p>
<pre><code># uv.toml
[tool.uv.pip]
backend = &quot;auto&quot;
auto-match-packages = [&quot;xformers&quot;, &quot;flash-attn&quot;]
</code></pre>
<pre><code>uv sync
</code></pre>
<p>Result:</p>
<pre><code>torch (CUDA)
xformers (CUDA)
flash-attn (CUDA)
</code></pre>
<p>All resolved to the <strong>same detected backend</strong>, with <strong>no manual index flags</strong>, <strong>no platform markers</strong>, and <strong>no optional dependency trees</strong>.</p>
<p>Thank you for reviewing this request :)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">enhancement</span> added by <a href="https://github.com/muhammad-fiaz">@muhammad-fiaz</a> on 2025-10-30 18:49</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/zanieb">@zanieb</a> on 2025-10-30 19:24</div>
            <div class="timeline-body"><p>You may be interested in https://astral.sh/blog/wheel-variants</p>
<p>The problem with doing this for arbitrary packages is that they encode which GPU variant they&#x27;re compatible with in different ways. We&#x27;re working to standardize this, so it&#x27;ll &quot;just work&quot; for the whole ecosystem.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/muhammad-fiaz">@muhammad-fiaz</a> on 2025-10-30 19:53</div>
            <div class="timeline-body"><p>@zanieb Thanks for reply! yeah that feature is good and as you said it just works! but the problem for me is with conflicts between them based on versions even if its can be ignored by uv settings for isolation it kind of hard to work with for multiple versioned projects such as Library projects (it almost consuming 500+ lines of optional dependencies with its artifacts url) and that why i am requesting i like the <code>uv</code> method like <code>[tool.uv.sources]</code> and <code>[tool.uv.index]</code> which eliminate the needed to multiple versioned urls into one index and also <code>uv</code> have features for <code>universal=true</code> on option settings <code>pyproject.toml</code> i hope that this feature may even added or simply process for optional dependencies in future</p>
<p>BTW i have some <strong>Example Use Case Scenarios:</strong> with <a href="https://github.com/unslothai/unsloth/blob/main/pyproject.toml">unsloth pyproject.toml</a></p>
 Click to Open Example 

<pre><code>
[project.optional-dependencies]
triton = [
    &quot;triton&gt;=3.0.0 ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;triton-windows ; (sys_platform == &#x27;win32&#x27;) and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
]
huggingfacenotorch = [
    &quot;wheel&gt;=0.42.0&quot;,
    &quot;packaging&quot;,
    &quot;numpy&quot;,
    &quot;tqdm&quot;,
    &quot;psutil&quot;,
    &quot;tyro&quot;,
    &quot;protobuf&quot;,
    &quot;sentencepiece&gt;=0.2.0&quot;,
    &quot;datasets&gt;=3.4.1,!=4.0.*,!=4.1.0&quot;,
    &quot;accelerate&gt;=0.34.1&quot;,
    &quot;peft&gt;=0.7.1,!=0.11.0&quot;,
    &quot;huggingface_hub&gt;=0.34.0&quot;,
    &quot;hf_transfer&quot;,
    &quot;diffusers&quot;,
    &quot;transformers&gt;=4.51.3,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,&lt;=4.57.2&quot;,
    &quot;trl&gt;=0.18.2,!=0.19.0,&lt;=0.23.0&quot;,
]
huggingface = [
    &quot;unsloth[huggingfacenotorch]&quot;,
    &quot;unsloth_zoo&gt;=2025.10.13&quot;,
    &quot;torchvision&quot;,
    &quot;unsloth[triton]&quot;,
]
windows = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0 ; (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers&gt;=0.0.22.post7 ; (sys_platform == &#x27;win32&#x27;)&quot;,
]
base = [
    &quot;unsloth[huggingface]&quot;,
]
cu118only = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121only = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu118onlytorch211 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121onlytorch211 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu118onlytorch212 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121onlytorch212 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu118onlytorch220 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121onlytorch220 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu118onlytorch230 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121onlytorch230 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp312-cp312-manylinux2014_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu118onlytorch240 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121onlytorch240 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu124onlytorch240 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu118onlytorch250 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121onlytorch250 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu124onlytorch250 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu118onlytorch251 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu121onlytorch251 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu124onlytorch251 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu118onlytorch260 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
]
cu124onlytorch260 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu126onlytorch260 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu118onlytorch270 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu126onlytorch270 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu128onlytorch270 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.9&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.10&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.11&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version==&#x27;3.12&#x27; and (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version==&#x27;3.9&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version==&#x27;3.10&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version==&#x27;3.11&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version==&#x27;3.12&#x27; and (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu118onlytorch271 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.31.post1-cp39-abi3-win_amd64.whl ; (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu126onlytorch271 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.31.post1-cp39-abi3-win_amd64.whl ; (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu128onlytorch271 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.31.post1-cp39-abi3-win_amd64.whl ; (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu118onlytorch280 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.32.post2-cp39-abi3-win_amd64.whl ; (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu126onlytorch280 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.32.post2-cp39-abi3-win_amd64.whl ; (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu128onlytorch280 = [
    &quot;xformers @ https://download.pytorch.org/whl/cu129/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;xformers @ https://download.pytorch.org/whl/cu129/xformers-0.0.32.post2-cp39-abi3-win_amd64.whl ; (sys_platform == &#x27;win32&#x27;)&quot;,
]
cu130onlytorch280 = [
]
cu126onlytorch290 = [
]
cu128onlytorch290 = [
]
cu130onlytorch290 = [
]
cu118 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118only]&quot;,
]
cu121 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121only]&quot;,
]
cu118-torch211 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu118onlytorch211]&quot;,
]
cu121-torch211 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu121onlytorch211]&quot;,
]
cu118-torch212 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu118onlytorch212]&quot;,
]
cu121-torch212 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu121onlytorch212]&quot;,
]
cu118-torch220 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch220]&quot;,
]
cu121-torch220 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch220]&quot;,
]
cu118-torch230 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch230]&quot;,
]
cu121-torch230 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch230]&quot;,
]
cu118-torch240 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch240]&quot;,
]
cu121-torch240 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch240]&quot;,
]
cu124-torch240 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch240]&quot;,
]
cu118-torch250 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch250]&quot;,
]
cu121-torch250 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch250]&quot;,
]
cu124-torch250 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch250]&quot;,
]
cu118-torch251 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch251]&quot;,
]
cu121-torch251 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch251]&quot;,
]
cu124-torch251 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch251]&quot;,
]
cu118-torch260 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch260]&quot;,
]
cu124-torch260 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch260]&quot;,
]
cu126-torch260 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch260]&quot;,
]
cu118-torch270 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch270]&quot;,
]
cu126-torch270 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch270]&quot;,
]
cu128-torch270 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch270]&quot;,
]
cu118-torch271 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch271]&quot;,
]
cu126-torch271 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch271]&quot;,
]
cu128-torch271 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch271]&quot;,
]
cu118-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch280]&quot;,
]
cu126-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch280]&quot;,
]
cu128-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch280]&quot;,
]
cu130-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu130onlytorch280]&quot;,
]
cu126-torch290 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch290]&quot;,
]
cu128-torch290 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch290]&quot;,
]
cu130-torch290 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu130onlytorch290]&quot;,
]
kaggle = [
    &quot;unsloth[huggingface]&quot;,
]
kaggle-new = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
]
conda = [
    &quot;unsloth[huggingface]&quot;,
]
colab-torch211 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu121onlytorch211]&quot;,
]
colab-ampere-torch211 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu121onlytorch211]&quot;,
    &quot;packaging&quot;,
    &quot;ninja&quot;,
    &quot;flash-attn&gt;=2.6.3 ; (&#x27;linux&#x27; in sys_platform)&quot;,
]
colab-torch220 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch220]&quot;,
]
colab-ampere-torch220 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch220]&quot;,
    &quot;packaging&quot;,
    &quot;ninja&quot;,
    &quot;flash-attn&gt;=2.6.3 ; (&#x27;linux&#x27; in sys_platform)&quot;,
]
colab-new = [
    &quot;unsloth_zoo&gt;=2025.10.13&quot;,
    &quot;packaging&quot;,
    &quot;tyro&quot;,
    &quot;transformers&gt;=4.51.3,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,&lt;=4.57.2&quot;,
    &quot;datasets&gt;=3.4.1,!=4.0.*,!=4.1.0&quot;,
    &quot;sentencepiece&gt;=0.2.0&quot;,
    &quot;tqdm&quot;,
    &quot;psutil&quot;,
    &quot;wheel&gt;=0.42.0&quot;,
    &quot;numpy&quot;,
    &quot;protobuf&quot;,
    &quot;huggingface_hub&gt;=0.34.0&quot;,
    &quot;hf_transfer&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[triton]&quot;,
]
colab-no-deps = [
    &quot;accelerate&gt;=0.34.1&quot;,
    &quot;trl&gt;=0.18.2,!=0.19.0,&lt;=0.23.0&quot;,
    &quot;peft&gt;=0.7.1&quot;,
    &quot;xformers ; (&#x27;linux&#x27; in sys_platform or sys_platform == &#x27;win32&#x27;) and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;protobuf&quot;,
]
colab = [
    &quot;unsloth[cu121]&quot;,
]
flashattention = [
    &quot;packaging ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;ninja ; (&#x27;linux&#x27; in sys_platform)&quot;,
    &quot;flash-attn&gt;=2.6.3 ; (&#x27;linux&#x27; in sys_platform)&quot;,
]
colab-ampere = [
    &quot;unsloth[colab-ampere-torch220]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118only]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu121-ampere = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121only]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch211 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu118onlytorch211]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu121-ampere-torch211 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes==0.45.5&quot;,
    &quot;unsloth[cu121onlytorch211]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch220 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch220]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu121-ampere-torch220 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch220]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch230 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch230]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu121-ampere-torch230 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch230]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch240 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch240]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu121-ampere-torch240 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch240]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu124-ampere-torch240 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch240]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch250 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch250]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu121-ampere-torch250 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch250]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu124-ampere-torch250 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch250]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch251 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch251]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu121-ampere-torch251 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu121onlytorch251]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu124-ampere-torch251 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch251]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch260 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch260]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu124-ampere-torch260 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu124onlytorch260]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu126-ampere-torch260 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch260]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch270 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch270]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu126-ampere-torch270 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch270]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu128-ampere-torch270 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch270]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch271 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch271]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu126-ampere-torch271 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch271]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu128-ampere-torch271 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch271]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu118-ampere-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu118onlytorch280]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu126-ampere-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch280]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu128-ampere-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch280]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu130-ampere-torch280 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu130onlytorch280]&quot;,
    &quot;unsloth[flashattention]&quot;,
]
cu126-ampere-torch290 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu126onlytorch290]&quot;,
]
cu128-ampere-torch290 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu128onlytorch290]&quot;,
]
cu130-ampere-torch290 = [
    &quot;unsloth[huggingface]&quot;,
    &quot;bitsandbytes&gt;=0.45.5,!=0.46.0,!=0.48.0&quot;,
    &quot;unsloth[cu130onlytorch290]&quot;,
]
flashattentiontorch260abiFALSEcu12x = [
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27;&quot;,
]
flashattentiontorch260abiTRUEcu12x = [
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27;&quot;,
]
flashattentiontorch250abiFALSEcu12x = [
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27;&quot;,
]
flashattentiontorch250abiTRUEcu12x = [
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27;&quot;,
]
flashattentiontorch240abiFALSEcu12x = [
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27;&quot;,
]
flashattentiontorch240abiTRUEcu12x = [
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27;&quot;,
    &quot;flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27;&quot;,
]
intelgputorch260 = [
    &quot;unsloth_zoo[intelgpu]&quot;,
    &quot;unsloth[huggingfacenotorch]&quot;,

    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp39-cp39-linux_x86_64.whl#sha256=147607f190a7d7aa24ba454def5977fbbfec792fdae18e4ed278cfec29b69271 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp310-cp310-linux_x86_64.whl#sha256=23aa423fa1542afc34f67eb3ba8ef20060f6d1b3a4697eaeab22b11c92b30f2b ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp311-cp311-linux_x86_64.whl#sha256=bcfa995229bbfd9ffd8d6c8d9f6428d393e876fa6e23ee3c20e3c0d73ca75ca5 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp312-cp312-linux_x86_64.whl#sha256=bd340903d03470708df3442438acb8b7e08087ab9e61fbe349b2872bf9257ab0 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp313-cp313-linux_x86_64.whl#sha256=814dccc8a07159e6eca74bed70091bc8fea2d9dd87b0d91845f9f38cde62f01c ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,

    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp39-cp39-linux_x86_64.whl#sha256=6a8adf6dc4c089406e8b3a7e58ab57a463bddf9b07130d2576e76eced43e92af ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp310-cp310-linux_x86_64.whl#sha256=ff4561cbf07c83bbccaa0f6e9bb0e6dcf721bacd53c9c43c4eb0e7331b4792f9 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp311-cp311-linux_x86_64.whl#sha256=12005f66b810ddd3ab93f86c4522bcfdd412cbd27fc9d189b661ff7509bc5e8a ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp312-cp312-linux_x86_64.whl#sha256=c4c5c67625cdacf35765c2b94e61fe166e3c3f4a14521b1212a59ad1b3eb0f2e ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp313-cp313-linux_x86_64.whl#sha256=e6864f7a60a5ecc43d5d38f59a16e5dd132384f73dfd3a697f74944026038f7b ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
]
intel-gpu-torch260 = [
    &quot;unsloth[intelgputorch260]&quot;
]
intelgputorch270 = [
    &quot;unsloth_zoo[intelgpu]&quot;,
    &quot;unsloth[huggingfacenotorch]&quot;,

    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=749a7098492c6a27b356c97149a4a62973b953eae60bc1b6259260974f344913 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=44362e80abd752471a08341093321955b066daa2cfb4810e73b8e3b240850f93 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=faa6b8c945a837a080f641bc8ccc77a98fa66980dcd7e62e715fd853737343fd ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=40f6fb65b345dc9a61813abe7ac9a585f2c9808f414d140cc2a5f11f53ee063c ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp313-cp313t-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=9821fe059de58e827ffc6aa10d69369b16c2f8c2a988b86bef9c2c6e396ab3aa ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,

    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp39-cp39-linux_x86_64.whl#sha256=f8ee75e50fcbb37ed5b498299ca2264da99ab278a93fae2358e921e4a6e28273 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.9&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp310-cp310-linux_x86_64.whl#sha256=d6fdc342961d98fdcd9d03dfd491a3208bb5f7fbb435841f8f72ce9fdcd2d026 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.10&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp311-cp311-linux_x86_64.whl#sha256=74d07f9357df5cf2bf223ad3c84de16346bfaa0504f988fdd5590d3e177e5e86 ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.11&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp312-cp312-linux_x86_64.whl#sha256=c806d44aa2ca5d225629f6fbc6c994d5deaac2d2cde449195bc8e3522ddd219a ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.12&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp313-cp313-linux_x86_64.whl#sha256=25d8277b7f01d42e2e014ccbab57a2692b6ec4eff8dcf894eda1b297407cf97a ; (&#x27;linux&#x27; in sys_platform) and python_version == &#x27;3.13&#x27; and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
]
intel-gpu-torch270 = [
    &quot;unsloth[intelgputorch270]&quot;
]
intelgputorch280 = [
    &quot;unsloth_zoo[intelgpu]&quot;,
    &quot;unsloth[huggingfacenotorch]&quot;,

    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.4.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=ac4d8e33986b1c3c5e48151640539272b2187e83016985853111b46fb82c3c94 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.9&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=999fef4c1f711092b9d3086525920545df490de476ecebe899ffc777019ae17f ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.10&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=57b09c8c492985ff6a27cd3a22b08e8f7b96b407bd8030967b6efbb9f63b80cf ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.11&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=df4bb3282bac9a3b90231700077110d8680b338416de03c2b7c6133c9b602649 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.12&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=60da63c99ca827bdcb0df28e0298bf7d066dc607454c6d6176783cb4e79d838b ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.13&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,

    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.8.0%2Bxpu-cp39-cp39-linux_x86_64.whl ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.9&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.8.0%2Bxpu-cp310-cp310-linux_x86_64.whl ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.10&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.8.0%2Bxpu-cp311-cp311-linux_x86_64.whl ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.11&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.8.0%2Bxpu-cp312-cp312-linux_x86_64.whl ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.12&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.8.0%2Bxpu-cp313-cp313-linux_x86_64.whl ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.13&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,

    &quot;bitsandbytes @ https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-manylinux_2_24_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and (platform_machine == &#x27;x86_64&#x27;)&quot;,

    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.23.0%2Bxpu-cp39-cp39-manylinux_2_28_x86_64.whl#sha256=6e981c192045fc249c008441179ff237bb00174d818b875b0475730b63f0eaca ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.9&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.23.0%2Bxpu-cp310-cp310-manylinux_2_28_x86_64.whl#sha256=e5ba4805969277175ebfd59cc717093528cc6e3ada89ac2725fc7a3c1fee6169 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.10&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.23.0%2Bxpu-cp311-cp311-manylinux_2_28_x86_64.whl#sha256=74c39c144104416bc4c5ad8c26ab0c169dc5cc6be58059e01bc3665dd0ef676f ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.11&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.23.0%2Bxpu-cp312-cp312-manylinux_2_28_x86_64.whl#sha256=0acec355b80c3899841184084f365df336c508602812e34a44007b8b60d53af4 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.12&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.23.0%2Bxpu-cp313-cp313-manylinux_2_28_x86_64.whl#sha256=e2109ae773dad27b98ca17681044b4f876563c37f2382b75de3a371399edcff8 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.13&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
]
intel-gpu-torch280 = [
    &quot;unsloth[intelgputorch280]&quot;
]
intelgputorch290 = [
    &quot;unsloth_zoo[intelgpu]&quot;,
    &quot;unsloth[huggingfacenotorch]&quot;,

    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.5.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=c169a1de14c19673b17c751290d467fa282fc90fa5da4314b2e5cdab1f553146 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.10&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.5.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=013d9dd5d6479bd22983161f462e61c8dbe1d82e6730624a7a8d5945507eaa61 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.11&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=afc8cabfbf7ed51fd278d1e0f88d6afc157b0201bad4b99d681e4d542f9e66d4 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.12&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.5.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=0d24c1716088f2764d0d24c64227732195b6a42706c3c5fc89eeb4904bfa0818 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.13&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,

    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.9.0%2Bxpu-cp310-cp310-linux_x86_64.whl#sha256=5afbe860ce991825a36b75706a523601087e414b77598ef0d9d3d565741c277d ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.10&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.9.0%2Bxpu-cp311-cp311-linux_x86_64.whl#sha256=607fe419c32d6e8e0556f745742e7cff1d0babce51f54be890e0c1422359c442 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.11&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.9.0%2Bxpu-cp312-cp312-linux_x86_64.whl#sha256=376bae584d89980b8e59934d248c38d5fa3b7d4687a4df1a19f4bc1d23dcc8c1 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.12&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torch @ https://download.pytorch.org/whl/xpu/torch-2.9.0%2Bxpu-cp313-cp313-linux_x86_64.whl#sha256=98d6a06dd7fb185874367b18bd609f05f16fdce4142a5980ca94461949965cd2 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.13&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,

    &quot;bitsandbytes @ https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-manylinux_2_24_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and (platform_machine == &#x27;x86_64&#x27;)&quot;,

    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.24.0%2Bxpu-cp310-cp310-manylinux_2_28_x86_64.whl#sha256=cbfae2b79b7549fd368c2462fc8e94f8f26cc450782ee72138e908077c09a519 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.10&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.24.0%2Bxpu-cp311-cp311-manylinux_2_28_x86_64.whl#sha256=044fa36ef4b6b43edcd490b75c853fa4b3eb033c2bded29f8fbcf27734713c67 ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.11&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.24.0%2Bxpu-cp312-cp312-manylinux_2_28_x86_64.whl#sha256=4b91e4bec1d740a6211f02578a79888550b73f3a4e1383035f8f6d72f587212c ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.12&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
    &quot;torchvision @ https://download.pytorch.org/whl/xpu/torchvision-0.24.0%2Bxpu-cp313-cp313-manylinux_2_28_x86_64.whl#sha256=88239e73ca37254bec84f29cd5887e10ff712de7edbbda3fbb3609cd6190d99e ; platform_system == &#x27;Linux&#x27; and python_version == &#x27;3.13&#x27; and platform_machine == &#x27;x86_64&#x27;&quot;,
]
intel-gpu-torch290 = [
    &quot;unsloth[intelgputorch290]&quot;
]
intel = [
    &quot;unsloth[intelgputorch280]&quot;,
]
amd = [
    &quot;unsloth[huggingfacenotorch]&quot;,
    &quot;bitsandbytes @ https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-manylinux_2_24_x86_64.whl ; (&#x27;linux&#x27; in sys_platform) and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;bitsandbytes @ https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-win_amd64.whl ; (sys_platform == &#x27;win32&#x27;) and (platform_machine == &#x27;AMD64&#x27; or platform_machine == &#x27;x86_64&#x27;)&quot;,
    &quot;bitsandbytes @ https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-manylinux_2_24_aarch64.whl ; (&#x27;linux&#x27; in sys_platform) and (platform_machine == &#x27;aarch64&#x27;)&quot;,
]
</code></pre>


<p>as you can see there is huge optional dependencies which are hard manage</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/mrs83">@mrs83</a> on 2025-11-04 19:12</div>
            <div class="timeline-body"><p>it may help also for bitsandbytes https://github.com/bitsandbytes-foundation/bitsandbytes/releases/tag/continuous-release_multi-backend-refactor</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/muhammad-fiaz">@muhammad-fiaz</a> on 2025-11-04 19:42</div>
            <div class="timeline-body"><blockquote>
<p>it may help also for bitsandbytes https://github.com/bitsandbytes-foundation/bitsandbytes/releases/tag/continuous-release_multi-backend-refactor</p>
</blockquote>
<p>Hello @mrs83 Yes also i tried many ways but some works by forced for one platform alone but not for universal platform</p>
<p>below is example workflow i tried for <code>multiple versioned dependencies support</code>!</p>
<pre><code>[build-system]
requires = [&quot;uv_build&gt;=0.9.6,&lt;0.10.0&quot;]
build-backend = &quot;uv_build&quot;
 
dependencies = [
    &quot;torch&gt;=2.5.0&quot;,
    &quot;torchvision&gt;=0.20.0&quot;,
]


[[tool.uv.index]]
name = &quot;pytorch-cpu&quot;
url = &quot;https://download.pytorch.org/whl/cpu&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu118&quot;
url = &quot;https://download.pytorch.org/whl/cu118&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu121&quot;
url = &quot;https://download.pytorch.org/whl/cu121&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu124&quot;
url = &quot;https://download.pytorch.org/whl/cu124&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu126&quot;
url = &quot;https://download.pytorch.org/whl/cu126&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu128&quot;
url = &quot;https://download.pytorch.org/whl/cu128&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu130&quot;
url = &quot;https://download.pytorch.org/whl/cu130&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-rocm&quot;
url = &quot;https://download.pytorch.org/whl/rocm6.2&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-xpu&quot;
url = &quot;https://download.pytorch.org/whl/xpu&quot;
explicit = true
[tool.uv.sources]

torch = [
    { index = &quot;pytorch-cu130&quot;, marker = &quot;extra == &#x27;cuda-130&#x27;&quot; },
    { index = &quot;pytorch-cu128&quot;, marker = &quot;extra == &#x27;cuda-128&#x27; and extra != &#x27;cuda-130&#x27;&quot; },
    { index = &quot;pytorch-cu126&quot;, marker = &quot;extra == &#x27;cuda-126&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27;&quot; },
    { index = &quot;pytorch-cu124&quot;, marker = &quot;extra == &#x27;cuda-124&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27;&quot; },
    { index = &quot;pytorch-cu121&quot;, marker = &quot;extra == &#x27;cuda-121&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27;&quot; },
    { index = &quot;pytorch-cu118&quot;, marker = &quot;extra == &#x27;cuda-118&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27;&quot; },
    { index = &quot;pytorch-rocm&quot;, marker = &quot;extra == &#x27;rocm&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27; and extra != &#x27;cuda-118&#x27;&quot; },
    { index = &quot;pytorch-xpu&quot;, marker = &quot;extra == &#x27;xpu&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27; and extra != &#x27;cuda-118&#x27; and extra != &#x27;rocm&#x27;&quot; },
]

torchvision = [
    { index = &quot;pytorch-cu130&quot;, marker = &quot;extra == &#x27;cuda-130&#x27;&quot; },
    { index = &quot;pytorch-cu128&quot;, marker = &quot;extra == &#x27;cuda-128&#x27; and extra != &#x27;cuda-130&#x27;&quot; },
    { index = &quot;pytorch-cu126&quot;, marker = &quot;extra == &#x27;cuda-126&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27;&quot; },
    { index = &quot;pytorch-cu124&quot;, marker = &quot;extra == &#x27;cuda-124&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27;&quot; },
    { index = &quot;pytorch-cu121&quot;, marker = &quot;extra == &#x27;cuda-121&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27;&quot; },
    { index = &quot;pytorch-cu118&quot;, marker = &quot;extra == &#x27;cuda-118&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27;&quot; },
    { index = &quot;pytorch-rocm&quot;, marker = &quot;extra == &#x27;rocm&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27; and extra != &#x27;cuda-118&#x27;&quot; },
    { index = &quot;pytorch-xpu&quot;, marker = &quot;extra == &#x27;xpu&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27; and extra != &#x27;cuda-118&#x27; and extra != &#x27;rocm&#x27;&quot; },
]

xformers = [
    { index = &quot;pytorch-cu130&quot;, marker = &quot;extra == &#x27;cuda-130&#x27;&quot; },
    { index = &quot;pytorch-cu128&quot;, marker = &quot;extra == &#x27;cuda-128&#x27; and extra != &#x27;cuda-130&#x27;&quot; },
    { index = &quot;pytorch-cu126&quot;, marker = &quot;extra == &#x27;cuda-126&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27;&quot; },
    { index = &quot;pytorch-cu124&quot;, marker = &quot;extra == &#x27;cuda-124&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27;&quot; },
    { index = &quot;pytorch-cu121&quot;, marker = &quot;extra == &#x27;cuda-121&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27;&quot; },
    { index = &quot;pytorch-cu118&quot;, marker = &quot;extra == &#x27;cuda-118&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27;&quot; },
    { index = &quot;pytorch-rocm&quot;, marker = &quot;extra == &#x27;rocm&#x27; and extra != &#x27;cuda-130&#x27; and extra != &#x27;cuda-128&#x27; and extra != &#x27;cuda-126&#x27; and extra != &#x27;cuda-124&#x27; and extra != &#x27;cuda-121&#x27; and extra != &#x27;cuda-118&#x27;&quot; },
]


[tool.uv.extra-build-dependencies]
# xformers requires torch and ninja for compilation
xformers = [&quot;torch&quot;, &quot;ninja&quot;]

# flash-attn requires torch, packaging, and ninja for compilation
flash-attn = [&quot;torch&gt;=2.0&quot;, &quot;packaging&quot;, &quot;ninja&quot;]

[project.optional-dependencies]

# Flash Attention support (Linux only) - separate extra
flash-attn = [
    &quot;flash-attn&gt;=2.6.3 ; sys_platform == &#x27;linux&#x27;&quot;,
]

# CUDA 11.8 support
cuda-118 = [
    &quot;xformers&gt;=0.0.22&quot;,
]

# CUDA 12.1 support  
cuda-121 = [
    &quot;xformers&gt;=0.0.22&quot;,
]

# CUDA 12.4 support
cuda-124 = [
    &quot;xformers&gt;=0.0.28&quot;,
]

# CUDA 12.6 support
cuda-126 = [
    &quot;xformers&gt;=0.0.28&quot;,
]

# CUDA 12.8 support
cuda-128 = [
    &quot;xformers&gt;=0.0.28&quot;,
]

# CUDA 13.0 support (latest)
cuda-130 = [
    &quot;xformers&gt;=0.0.28&quot;,
]

# AMD ROCm support (PyTorch routed via index)
rocm = []

# Intel XPU support (works on both Windows and Linux x86_64)
# Note: Only installed when explicitly requesting the xpu extra
xpu = [
    &quot;pytorch-triton-xpu&quot;,
]

</code></pre>
<p>i tried like this also it does not work as intented and always have any conflicts output by <code>uv</code>
also for my i don&#x27;t know why it does not even installing <code>pytorch</code>  BTW! worked when adding individually pytorch to each <code>optional-dependencies</code> but not installing correct extra version like <code>uv sync --extra cuda-130</code> or <code>uv sync --extra cuda-128</code> like this but it always install fixed on one version and also conflict occurs!, as for <code>single versioned</code> for multiple platform usecase with xformer i found one solution #16532</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 19:40:51 UTC
    </footer>
</body>
</html>
