<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>`--torch-backend=auto` ignores CUDA compute capability when installing `torch` for GTX 1080 Ti - astral-sh/uv #14742</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1><code>--torch-backend=auto</code> ignores CUDA compute capability when installing <code>torch</code> for GTX 1080 Ti</h1>

    <div class="meta">
        <span class="state-icon state-open"></span>
        <a href="https://github.com/astral-sh/uv/issues/14742">#14742</a>
        opened by <a href="https://github.com/hamzaq2000">@hamzaq2000</a>
        on 2025-07-18 22:54
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/hamzaq2000">@hamzaq2000</a></div>
            <div class="timeline-body">Summary
<p>Tested the following command on a system with a GTX 1080 Ti and CUDA 12.8 driver installed (what shows in <code>nvidia-smi</code>).</p>
<pre><code>uv pip install torch --torch-backend=auto --preview
</code></pre>
<p>Then ran this test script:</p>
<pre><code>import torch
tensor = torch.randn(3, 4, device=&#x27;cuda&#x27;)
print(tensor)
</code></pre>
<p>And got this:</p>
<pre><code>  cpu = _conversion_method_template(device=torch.device(&quot;cpu&quot;))
/root/env/lib/python3.11/site-packages/torch/cuda/__init__.py:262: UserWarning:
    Found GPU0 NVIDIA GeForce GTX 1080 Ti which is of cuda capability 6.1.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability supported by this library is 7.5.

  warnings.warn(
/root/env/lib/python3.11/site-packages/torch/cuda/__init__.py:287: UserWarning:
NVIDIA GeForce GTX 1080 Ti with CUDA capability sm_61 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120 compute_120.
If you want to use the NVIDIA GeForce GTX 1080 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(
Traceback (most recent call last):
  File &quot;/root/torch_test.py&quot;, line 4, in &lt;module&gt;
    tensor = torch.randn(3, 4, device=&#x27;cuda&#x27;)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>I think just looking at the installed CUDA version/driver might not be enough, the CUDA compute capability supported by different torch versions is of relevance as well. In this case, we have a GTX 1080 Ti with CUDA 12.8 drivers installed, but we can&#x27;t actually use the <code>cu128</code> torch wheel, which <code>--torch-backend=auto</code> installs, as it doesn&#x27;t support GPUs with CUDA compute capability &lt; 7.5.</p>
<p>Some relevant discussion I found:
https://discuss.pytorch.org/t/gpu-compute-capability-support-for-each-pytorch-version/62434/4
https://github.com/moi90/pytorch_compute_capabilities</p>
<p>I then also tested without <code>--torch-backend=auto</code>, so simply:</p>
<pre><code>uv pip install torch
</code></pre>
<p>This seems to install the <code>cu126</code> wheel (going off the <a href="https://pytorch.org/get-started/locally/">PyTorch website</a>), and with this the test script worked just fine, meaning the <code>cu126</code> wheel has not yet dropped support for CUDA compute capability 6.1 devices.</p>
Platform
<p>Linux 5.15.0-142-generic x86_64 GNU/Linux</p>
Version
<p>0.8.0</p>
Python version
<p>Python 3.11.13</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">bug</span> added by <a href="https://github.com/hamzaq2000">@hamzaq2000</a> on 2025-07-18 22:54</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/DEKHTIARJonathan">@DEKHTIARJonathan</a> on 2025-07-18 23:14</div>
            <div class="timeline-body"><blockquote>
<p>Unfortunately I&#x27;m not knowledgeable enough to comment on the feasibility of this, so I won&#x27;t. But I thought it was worth putting out there; hopefully it is fixable and <code>--torch-backend=auto</code> can cleanly install the the correct torch wheel from the correct index for all NVIDIA GPUs.</p>
</blockquote>
<p>Downgrade your driver to CUDA 12.6 and it will fix the problem.</p>
<p>Charlie may address it - unfortunately it&#x27;s like chasing a rabbit and a lot of if/else because it will be on a per-release basis.
Make your life easy and downgrade the driver (12.8 won&#x27;t benefit you in this case given that you must use 12.6 artifacts)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-07-18 23:17</div>
            <div class="timeline-body"><p>I suspect downgrading the user-mode driver would still result in the same behavior with <code>--torch-backend=auto</code> since they would still be above the minimum required kernel driver for the PyTorch 2.8 wheels, right? Like, they&#x27;d still be on a kernel driver of above 525.60.13, so we&#x27;d still assume CUDA 12.8 compatibility.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Assigned to <a href="https://github.com/charliermarsh">@charliermarsh</a> by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-07-18 23:34</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/hamzaq2000">@hamzaq2000</a> on 2025-07-18 23:52</div>
            <div class="timeline-body"><p>@DEKHTIARJonathan</p>
<blockquote>
<p>Downgrade your driver to CUDA 12.6 and it will fix the problem</p>
</blockquote>
<p>As I said, all works fine if I just do <code>uv pip install torch</code> instead of <code>uv pip install torch --torch-backend=auto --preview</code>. I.e. keep the driver the same at 12.8, but just install the <code>cu126</code> wheel instead of <code>cu128</code>. So getting it running is not an issue.</p>
<p>The issue is that I&#x27;d like it if <code>--torch-backend=auto</code> could do this instead of manual intervention. I&#x27;m writing a library in which it&#x27;d be nice to have just a single <code>uv pip install</code> to install it instead of multiple cases where the user has to first check the compute capability of their GPU.</p>
<p>The workaround I&#x27;ve come up with for now is just using the <code>cu128</code> wheel outright. I&#x27;ll just tell the user to make sure to have a CUDA 12 driver and a GPU with compute capability &gt;= 7.5. For users that have a CUDA 12 driver but a GPU with compute capability &lt; 7.5, they can run:</p>
<pre><code>uv pip install library[nvidia-old]
</code></pre>
<p>and this will install the <code>cu126</code> wheel.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-07-18 23:56</div>
            <div class="timeline-body"><p>Yeah it would be nice if <code>auto</code> handled this. I think it&#x27;s possible (likely easier if we use <code>nvml</code> as Jonathan suggested elsewhere).</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/hamzaq2000">@hamzaq2000</a> on 2025-07-19 00:07</div>
            <div class="timeline-body"><p>This is just me speculating, but would it also not be required to keep a table of each torch wheel and the CUDA compute capabilities it supports? For example, the error message I got said the following (<code>cu128</code> wheel):</p>
<pre><code>NVIDIA GeForce GTX 1080 Ti with CUDA capability sm_61 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120 compute_120.
</code></pre>
<p>Or perhaps there is some link between driver version, CUDA version, and torch wheels that I am unaware of, sidestepping the need to check for compute capability altogether.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/DEKHTIARJonathan">@DEKHTIARJonathan</a> on 2025-07-19 00:07</div>
            <div class="timeline-body"><p>@charliermarsh you can completely disregard the KMD. It&#x27;s extremely rare that it has any impact anywhere.</p>
<p>So rare we couldn&#x27;t find one package anywhere in the entire Python ecosystem that actually has a dependency on the KMD. Consequently we removed this feature from the NV variant plugin to avoid confusing people.</p>
<p>Just focus on libcuda.so and you&#x27;ll get the right answer you look for ðŸ¤˜</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/DEKHTIARJonathan">@DEKHTIARJonathan</a> on 2025-07-19 00:10</div>
            <div class="timeline-body"><p>Use this https://github.com/wheelnext/nvidia-variant-provider/blob/dev/nvidia_variant_provider/detect_cuda.py#L31</p>
<p>Simple and you can directly dlopen the library.</p>
<p>We use pynvml here for simplicity but you can access nvml directly from rust.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/DEKHTIARJonathan">@DEKHTIARJonathan</a> on 2025-07-19 00:17</div>
            <div class="timeline-body"><blockquote>
<p>This is just me speculating, but would it also not be required to keep a table of each torch wheel and the CUDA compute capabilities it supports? For example, the error message I got said the following (<code>cu128</code> wheel):</p>
<pre><code>NVIDIA GeForce GTX 1080 Ti with CUDA capability sm_61 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120 compute_120.
</code></pre>
<p>Or perhaps there is some link between driver version, CUDA version, and torch wheels that I am unaware of, sidestepping the need to check for compute capability altogether.</p>
</blockquote>
<p>Yes in theory... But what about the usecases where people have different GPUs on the machine... And that would mean updating and maintaining that table for a very long time.</p>
<p>Also SM compatibility rules are very complicated. There&#x27;s PTX/SASS compat, jit compilation, and a whole lot of family oriented compatibility. These rules are quite complicated to write if you don&#x27;t want them hyper restrictive.</p>
<p>I&#x27;ll give you an example. Torch has never been built AFAIK for SM 6.1 (GTX 1080) yet it works perfectly. So if there was a filtering on just &quot;SM 60&quot; like it&#x27;s built... You would not get anything. Yet built with SM 6.0 doesn&#x27;t mean compatible with the whole 6.x family... Complicated ðŸ˜…</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/hamzaq2000">@hamzaq2000</a> on 2025-07-19 00:23</div>
            <div class="timeline-body"><p>Wow, the rabbit hole goes deep. TIL ðŸ˜…</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 19:39:31 UTC
    </footer>
</body>
</html>
