```yaml
number: 14445
title: Consider feature to vendor-in distributions in workspace
type: issue
state: open
author: potiuk
labels:
  - enhancement
assignees: []
created_at: 2025-07-03T16:53:38Z
updated_at: 2025-12-20T20:54:30Z
url: https://github.com/astral-sh/uv/issues/14445
synced_at: 2026-01-10T01:57:33Z
```

# Consider feature to vendor-in distributions in workspace

---

_Issue opened by @potiuk on 2025-07-03 16:53_

### Summary

We have an interesting discussions in Airlfow about sharing some code between different distributions in the same workspace and we have a particular need that might be useful to consider as uv workspace feature - vendored-in distributions.

Basically what we want to achieve is to eat-cake and have it too when it comes to shared code. I.e.

* we would like to have a shared code that is not duplicated in the monorepo and can be used by different distributions
* but we want this code to be distriibuted "embedded" in the distribution that uses it, but in the way that you can install two distributions - each of them with a different version of that shared code embedded (basicallly the version that was present in the repo at the moment the using distribution was released

This resembles a bit npm/javascript ecosystem where different packages can use different versions of shared libraries - but we would like to do it without having such "generic" capability in Python - and only focus on shared code from our monorepo, rather than external distributions.

We want it to be available for coding and changing it and using it in our distributions when you just check-out "main" from the repo - it should not require any manual synchronisation - it should **just work** that when you modify the code in one place, then change should be automatically picked by the other distribution.

It boils down to having the same code available in different packages in different distributions. Say.

1) Package airflow-core uses "airflow.shared.logging".
2) Package airflow-task-sdk uses the exact same code (maybe with modified imports) but it is in `airflow.sdk.shared.logging" 

We have several options we can follow now:

* symlinking the code from one distribution to the other in a - this has a number of limitations - only relative imports can be used for such a shared code
* we have a POC where we use https://pypi.org/project/vendoring/ to automatically vendor in the code between distributions (which needs a bit of tweaking to work in uv)
* we can have regular "common" library, but that introduces a whole host compatiblity, versioning, fixing API etc. 

Maybe that would be possible / good idea to have a feature in `uv workspace` to mark a workspace as "shared" - and while localy you could refer to it as a dependency (say in "shared" dependency group), it would end up as actual code embedded in a designated package inside the distribution. 

Have you ever considered such feature? Would that be feasible to implement/ useful for others as well ?



### Example

_No response_

---

_Label `enhancement` added by @potiuk on 2025-07-03 16:53_

---

_Comment by @konstin on 2025-07-03 18:47_

Could you elaborate why you want to vendor the code, instead of use using a package and re-exports?

---

_Comment by @potiuk on 2025-07-03 19:17_

Sure:

imagine we have `logging` package with several modules:

* airflow.shared.logging/main.py
* airflow.shared.logging/communicate.py

And main.py does "from airflow.shared.logging.communicate import something"

And imagine we want to install two distributions:

* airflow-core
* airflow-task-sdk

One of those distributions (airlfow-core) is relased 3 months ago and the otther (airflow-sdk)  is released today. Between 3 months ago and now there were some changes in `airflow.shared.logging`  - and "airflow-sdk" wants to use some new features from this new code - so it wants to use "latest" version of "airflow.shared.logging".

At the same time the "airflow-core" package used version of "airflow.shared.logging" that was there 3 months ago - and potentially it means that the latest version of the "airflow.shared.logging" code that "airflow-task-sdk" uses might be incompatible with the "airflow-core" released 3 months ago (accidentally or intentionally).

AND (here the complexity is) we want to install "airflow-core" from 3 months ago with "airflow-task-sdk" today - in the same virtualenv.

Solution 1) make sure that there is no backwards compatiblity issues and turn "airflow.shared.logging" into a new distribution - and let it be used by both "airlfow-core" and "airflow-sdk" and do all the harness and testing to make sure that "airflow-core" released 3 months ago works with the latest version of the "shared.logging" library. 

This is the "typical" way of solving the problem but it has many traps and issues and is just complex to orchestrate, validate , run back-compatibility tests etc. 


Solution 2) - the one we envision is to change dynamically on-the-flight the package where the shared code is imported from and use a different version effectively in different distributions:

* airflow-core -> import from airflow.shared.logging
* airflow-task-skd -> import from airflow.sdk.shared.logging

If the python files in "logging" package are identical - we could just symlink them between the two distributions in our monorepo and be "done with it" - but that has a "hard" requirement that "main.py" (and other inter-dependencies in that shared code) use only relative imports (`from .commmunicate import ...`)  - which we don't particularly like for some good reasons that I could explain separately. And yes we could relax it and allow it and have symlinks, add pre-commits to make sure no-one adds absolute imports for those intra-shared-code references and be done with it.

But it would have been quite a bit more "proper" if we could say "this shared distribution should appear as if the code is in this package in the distribution that wants to use it" - and have all the heavy-lifting for replacing the imports etc. done by the workspace mechanism.

I hope it can shed some light and maybe @ashb might add more - because he is working on a hatchling plugin to do essentially something like this in his quest to avoid symlinking and relative imports and to have somethihing more "explicit" than symbolic links in a monorepo.



---

_Referenced in [apache/airflow#53506](../../apache/airflow/pulls/53506.md) on 2025-07-21 11:09_

---

_Comment by @potiuk on 2025-07-23 08:17_

Some follow up hare. 

In Airflow we have just a final set of discussions in https://github.com/apache/airflow/pull/53149 where we settled on the way how we are going to implement this "internal code vendoring" and I think we came up with a really, really nice (and rather simple) way of doing it - by utilsing relative imports and symlinking the code (and adding a number of safeguards, in the near feature some pre-commits to make sure dependencies, pyproject.toml and facade imports are synchronized) between "shared" distributions, and "using distributions" that are in the same `uv workspace`,

You might want to take a look there @charliermarsh @konstin @zanieb -> to see what kind of need I am talking about in this issue.

With all the discussions we had internally with @ashb @amoghrajesh @uranusjr and others in Airflow community I personally come to one analogy that is very easy to grasp. I think using "vendoring" in this context is a bit wrong (because it implies 3rd-party vendor). But this is really equivalent of "static libraries linking" from C and other libraries - or what javascript node dependnecy does - where effectively different projects can use different versions of shared libraries (our approach is much closer to C static linking as it does not even expect that the shared libraries used are actually released - we ship them as part of the "using" distribution).

Eventually what we came up is an  interesting "worskpace" scheme, that I think might be worth to consider by `astral` team to support natively (we will still have to do quite a number of custom code, pre-commits, ruff rules preventing misuse and some conventions that we will follow to achieve what we want).

But essentially what we are heading towards is:

* having a "shared" distribution that has a code that can be shared between different "using" distributions. 

* while theorethically (and even practically) we could make such library a dependency of all the "using' distributions - we do not want to do it, because it introduces coupling between multiple "using" distributions - they (that's how Python dependencies work) - all distributions using the shared code must use the exact same version that is installed as separate distribution. This has multiple advantage and disadvantages of course.

* so what we do instead is a scheme where distributions using the shared code might opt (or even currently in our case this is the default) to use them in "statically linked" way - i.e. the library code is part of the distribution using it and all the usage of the shared code is from there

Basically what we do - we "statically link" shared library in distributions using the shared code. 

I think this is a nice example of:

* actual need for it in big projects
* working implementation
* and eventually - call to `Astral` / `uv` team to make it a "feature" of uv workspaces

BTW. Not that I am suggesting it but I know @charliermarsh that you are looking for some ways of monetizing and building value of astral on top of the open-source uv, ruff, ty.... That might be a good "enterprise" feature of something else on top of uv - that does not have to be `uv` feature on it's own. It's a very niche need of open-source projects like ours - and probably many enterprise projects might have a similar need. 

Just to clarify about the recent stuff about python-requires, limits and Astral's role in setting standards as compared to Pypa, because it could be misunderstood.

I personally would LOVE for Astral to eventually find good and sustainable model that would allow to support uv, ruff, ty for a long time, but I will always oppose `uv` driving standards an bypassing `PyPa` which is the right body (even if slow and difficult to get consensus on a number of things). 

But we make a great use in the community from all the improved workflows, speed and philosophy of development that Astral and `uv` brought to the community and I would love to help in finding a good long-term sustainable model that would help Astral to be a fantastic steward of the Open-Source tools you build :).

This might be one such idea. 


---

_Comment by @bersace on 2025-11-20 15:08_

Interested for this feature.

---

_Comment by @Vollkornaffe on 2025-12-19 13:50_

Possibly related: https://pypi.org/project/vendorize/

---

_Comment by @potiuk on 2025-12-20 20:54_

Yep. We actually considerd "vendorize" in airflow - but it was rather complex as part of the "build" pipeline. I wrote about it in a series of articles recently - https://medium.com/apache-airflow/modern-python-monorepo-for-apache-airflow-part-4-c9d9393a696a has all the details of the way we've ended-up with. And Symbolic links solution with relative imports turned out (for us) way, way more simpler than using "vendorize" - which we actually tried and it turned out to be far too complex

---
