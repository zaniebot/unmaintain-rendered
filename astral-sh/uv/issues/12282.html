<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>uv rejects `flash-attn` wheels with local versions - astral-sh/uv #12282</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>uv rejects <code>flash-attn</code> wheels with local versions</h1>

    <div class="meta">
        <span class="state-icon state-closed"></span>
        <a href="https://github.com/astral-sh/uv/issues/12282">#12282</a>
        opened by <a href="https://github.com/charliermarsh">@charliermarsh</a>
        on 2025-03-18 14:28
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/charliermarsh">@charliermarsh</a></div>
            <div class="timeline-body"><p>This seems to have caused some problems with the flash-attn wheels.</p>
<pre><code>#pyproject.toml
...

[tool.uv]
environments = [&quot;sys_platform == 'darwin'&quot;, &quot;sys_platform == 'linux'&quot;]
constraint-dependencies = [&quot;torch==2.5.1&quot;]

[tool.uv.sources]
flash_attn = [
  { url = &quot;https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.5cxx11abiFalse-cp310-cp310-linux_x86_64.whl&quot;, marker = &quot;sys_platform == 'linux' and python_version == '3.10'&quot;},
  { url = &quot;https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.5cxx11abiFalse-cp311-cp311-linux_x86_64.whl&quot;, marker = &quot;sys_platform == 'linux' and python_version == '3.11'&quot;},
  { url = &quot;https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.5cxx11abiFalse-cp312-cp312-linux_x86_64.whl&quot;, marker = &quot;sys_platform == 'linux' and python_version == '3.12'&quot;},
  { url = &quot;https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.5cxx11abiFalse-cp313-cp313-linux_x86_64.whl&quot;, marker = &quot;sys_platform == 'linux' and python_version == '3.13'&quot;}
]
</code></pre>
<p>Now causes <code>uv sync</code> to error with</p>
<pre><code>error: Failed to parse `uv.lock`
  Caused by: The entry for package `flash-attn` v2.7.3 has wheel `flash_attn-2.7.3+cu12torch2.5cxx11abifalse-cp310-cp310-linux_x86_64.whl` with inconsistent version: v2.7.3+cu12torch2.5cxx11abifalse
</code></pre>
<p>Not sure if there is a better way to configure usage of these wheels.</p>
<p><em>Originally posted by @kleinhenz in https://github.com/astral-sh/uv/issues/12235#issuecomment-2733196655</em></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">bug</span> added by @charliermarsh on 2025-03-18 14:28</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Assigned to @charliermarsh by @charliermarsh on 2025-03-18 14:34</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by @charliermarsh on 2025-03-18 15:12</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by @charliermarsh on 2025-03-18 15:12</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-12 17:30:30 UTC
    </footer>
</body>
</html>
