<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>`uv` sometimes can't handle PyTorch nightly's cuda dependencies correctly. - astral-sh/uv #10119</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1><code>uv</code> sometimes can&#x27;t handle PyTorch nightly&#x27;s cuda dependencies correctly.</h1>

    <div class="meta">
        <span class="state-icon state-closed"></span>
        <a href="https://github.com/astral-sh/uv/issues/10119">#10119</a>
        opened by <a href="https://github.com/YouJiacheng">@YouJiacheng</a>
        on 2024-12-23 07:46
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/YouJiacheng">@YouJiacheng</a></div>
            <div class="timeline-body"><p>my pyproject.toml</p>
<pre><code>[project]
name = &quot;modded-nanogpt&quot;
version = &quot;0.1.0&quot;
description = &quot;Add your description here&quot;
readme = &quot;README.md&quot;
requires-python = &quot;&gt;=3.12&quot;
dependencies = [
    &quot;numpy&gt;=2.1.3&quot;,
    &quot;torch==2.6.0.dev20241222&quot;,
    &quot;pytorch-triton&gt;=3.2.0&quot;,
    &quot;huggingface-hub&gt;=0.26.2&quot;,
    &quot;tqdm&gt;=4.67.0&quot;,
]

[tool.uv]
environments = [
    &quot;platform_system == &#x27;Linux&#x27;&quot;,
]

[tool.uv.sources]
torch = [
    { index = &quot;pytorch-nightly-cu126&quot;},
]
pytorch-triton = [
    { index = &quot;pytorch-nightly-cu126&quot;},
]

[[tool.uv.index]]
name = &quot;pytorch-nightly-cu126&quot;
url = &quot;https://download.pytorch.org/whl/nightly/cu126&quot;
explicit = true

[[tool.uv.index]]
name = &quot;pytorch-cu124&quot;
url = &quot;https://download.pytorch.org/whl/cu124&quot;
explicit = true
</code></pre>
<p><code>uv tree</code> output</p>
<pre><code>Resolved 22 packages in 7ms
modded-nanogpt v0.1.0
├── huggingface-hub v0.27.0
│   ├── filelock v3.16.1
│   ├── fsspec v2024.10.0
│   ├── packaging v24.2
│   ├── pyyaml v6.0.2
│   ├── requests v2.32.3
│   │   ├── certifi v2024.12.14
│   │   ├── charset-normalizer v3.4.0
│   │   ├── idna v3.10
│   │   └── urllib3 v2.2.3
│   ├── tqdm v4.67.1
│   └── typing-extensions v4.12.2
├── numpy v2.2.0
├── pytorch-triton v3.2.0+git35c6c7c6
├── torch v2.6.0.dev20241222+cu126
│   ├── filelock v3.16.1
│   ├── fsspec v2024.10.0
│   ├── jinja2 v3.1.4
│   │   └── markupsafe v3.0.2
│   ├── networkx v3.4.2
│   ├── setuptools v75.6.0
│   ├── sympy v1.13.1
│   │   └── mpmath v1.3.0
│   └── typing-extensions v4.12.2
└── tqdm v4.67.1
</code></pre>
<p>expected output:</p>
<pre><code>Resolved 35 packages in 7ms
modded-nanogpt v0.1.0
├── huggingface-hub v0.27.0
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── packaging v24.2
│   ├── pyyaml v6.0.2
│   ├── requests v2.32.3
│   │   ├── certifi v2024.12.14
│   │   ├── charset-normalizer v3.4.0
│   │   ├── idna v3.10
│   │   └── urllib3 v2.3.0
│   ├── tqdm v4.67.1
│   └── typing-extensions v4.12.2
├── numpy v2.2.1
├── pytorch-triton v3.2.0+git35c6c7c6
├── torch v2.6.0.dev20241222+cu126
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── jinja2 v3.1.5
│   │   └── markupsafe v3.0.2
│   ├── networkx v3.4.2
│   ├── nvidia-cublas-cu12 v12.6.4.1
│   ├── nvidia-cuda-cupti-cu12 v12.6.80
│   ├── nvidia-cuda-nvrtc-cu12 v12.6.77
│   ├── nvidia-cuda-runtime-cu12 v12.6.77
│   ├── nvidia-cudnn-cu12 v9.5.1.17
│   │   └── nvidia-cublas-cu12 v12.6.4.1
│   ├── nvidia-cufft-cu12 v11.3.0.4
│   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-curand-cu12 v10.3.7.77
│   ├── nvidia-cusolver-cu12 v11.7.1.2
│   │   ├── nvidia-cublas-cu12 v12.6.4.1
│   │   ├── nvidia-cusparse-cu12 v12.5.4.2
│   │   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-cusparse-cu12 v12.5.4.2 (*)
│   ├── nvidia-cusparselt-cu12 v0.6.3
│   ├── nvidia-nccl-cu12 v2.21.5
│   ├── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-nvtx-cu12 v12.6.77
│   ├── setuptools v75.6.0
│   ├── sympy v1.13.1
│   │   └── mpmath v1.3.0
│   └── typing-extensions v4.12.2
└── tqdm v4.67.1
(*) Package tree already displayed
</code></pre>
<hr>
extra test 1
<p>In a new directory I run <code>uv init</code> <code>uv venv</code> then <code>uv pip install torch --index-url https://download.pytorch.org/whl/nightly/cu126</code>.
It can correctly install <code>nvidia-*</code>.</p>
<pre><code>Resolved 24 packages in 2.83s
Installed 24 packages in 127ms
 + filelock==3.16.1
 + fsspec==2024.10.0
 + jinja2==3.1.4
 + markupsafe==2.1.5
 + mpmath==1.3.0
 + networkx==3.4.2
 + nvidia-cublas-cu12==12.6.4.1
 + nvidia-cuda-cupti-cu12==12.6.80
 + nvidia-cuda-nvrtc-cu12==12.6.77
 + nvidia-cuda-runtime-cu12==12.6.77
 + nvidia-cudnn-cu12==9.5.1.17
 + nvidia-cufft-cu12==11.3.0.4
 + nvidia-curand-cu12==10.3.7.77
 + nvidia-cusolver-cu12==11.7.1.2
 + nvidia-cusparse-cu12==12.5.4.2
 + nvidia-cusparselt-cu12==0.6.3
 + nvidia-nccl-cu12==2.21.5
 + nvidia-nvjitlink-cu12==12.6.85
 + nvidia-nvtx-cu12==12.6.77
 + pytorch-triton==3.2.0+git0d4682f0
 + setuptools==70.2.0
 + sympy==1.13.1
 + torch==2.6.0.dev20241222+cu126
 + typing-extensions==4.12.2
</code></pre>
<p>and <code>uv pip tree</code> gives</p>
<pre><code>torch v2.6.0.dev20241222+cu126
├── filelock v3.16.1
├── fsspec v2024.10.0
├── jinja2 v3.1.4
│   └── markupsafe v2.1.5
├── networkx v3.4.2
├── nvidia-cublas-cu12 v12.6.4.1
├── nvidia-cuda-cupti-cu12 v12.6.80
├── nvidia-cuda-nvrtc-cu12 v12.6.77
├── nvidia-cuda-runtime-cu12 v12.6.77
├── nvidia-cudnn-cu12 v9.5.1.17
│   └── nvidia-cublas-cu12 v12.6.4.1
├── nvidia-cufft-cu12 v11.3.0.4
│   └── nvidia-nvjitlink-cu12 v12.6.85
├── nvidia-curand-cu12 v10.3.7.77
├── nvidia-cusolver-cu12 v11.7.1.2
│   ├── nvidia-cublas-cu12 v12.6.4.1
│   ├── nvidia-cusparse-cu12 v12.5.4.2
│   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   └── nvidia-nvjitlink-cu12 v12.6.85
├── nvidia-cusparse-cu12 v12.5.4.2 (*)
├── nvidia-cusparselt-cu12 v0.6.3
├── nvidia-nccl-cu12 v2.21.5
├── nvidia-nvjitlink-cu12 v12.6.85
├── nvidia-nvtx-cu12 v12.6.77
├── pytorch-triton v3.2.0+git0d4682f0
├── setuptools v70.2.0
├── sympy v1.13.1
│   └── mpmath v1.3.0
└── typing-extensions v4.12.2
(*) Package tree already displayed
</code></pre>
<hr>
extra test 2
<p><code>torch==2.6.0.dev20241203+cu126</code> is okay:</p>
<pre><code>Resolved 35 packages in 7ms
modded-nanogpt v0.1.0
├── huggingface-hub v0.27.0
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── packaging v24.2
│   ├── pyyaml v6.0.2
│   ├── requests v2.32.3
│   │   ├── certifi v2024.12.14
│   │   ├── charset-normalizer v3.4.0
│   │   ├── idna v3.10
│   │   └── urllib3 v2.3.0
│   ├── tqdm v4.67.1
│   └── typing-extensions v4.12.2
├── numpy v2.2.1
├── pytorch-triton v3.2.0+git35c6c7c6
├── torch v2.6.0.dev20241203+cu126
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── jinja2 v3.1.5
│   │   └── markupsafe v3.0.2
│   ├── networkx v3.4.2
│   ├── nvidia-cublas-cu12 v12.6.4.1
│   ├── nvidia-cuda-cupti-cu12 v12.6.80
│   ├── nvidia-cuda-nvrtc-cu12 v12.6.77
│   ├── nvidia-cuda-runtime-cu12 v12.6.77
│   ├── nvidia-cudnn-cu12 v9.5.1.17
│   │   └── nvidia-cublas-cu12 v12.6.4.1
│   ├── nvidia-cufft-cu12 v11.3.0.4
│   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-curand-cu12 v10.3.7.77
│   ├── nvidia-cusolver-cu12 v11.7.1.2
│   │   ├── nvidia-cublas-cu12 v12.6.4.1
│   │   ├── nvidia-cusparse-cu12 v12.5.4.2
│   │   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-cusparse-cu12 v12.5.4.2 (*)
│   ├── nvidia-cusparselt-cu12 v0.6.3
│   ├── nvidia-nccl-cu12 v2.21.5
│   ├── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-nvtx-cu12 v12.6.77
│   ├── setuptools v75.6.0
│   ├── sympy v1.13.1
│   │   └── mpmath v1.3.0
│   └── typing-extensions v4.12.2
└── tqdm v4.67.1
(*) Package tree already displayed
</code></pre>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-23 08:25</div>
            <div class="timeline-body"><p>there are these requirements in METADATA
<img src="https://github.com/user-attachments/assets/33feafa4-39ff-4efd-9c9d-b650deb4124d" alt="Image"></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-12-23 13:43</div>
            <div class="timeline-body"><p>Can you be more specific about the actual problem here? What&#x27;s going wrong? What happened that you did not expect?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">question</span> added by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-12-23 13:43</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-23 14:40</div>
            <div class="timeline-body"><p>I expect <code>uv sync</code> on the <code>pyproject.toml</code> I posted on this issue will install</p>
<pre><code> + nvidia-cublas-cu12==12.6.4.1
 + nvidia-cuda-cupti-cu12==12.6.80
 + nvidia-cuda-nvrtc-cu12==12.6.77
 + nvidia-cuda-runtime-cu12==12.6.77
 + nvidia-cudnn-cu12==9.5.1.17
 + nvidia-cufft-cu12==11.3.0.4
 + nvidia-curand-cu12==10.3.7.77
 + nvidia-cusolver-cu12==11.7.1.2
 + nvidia-cusparse-cu12==12.5.4.2
 + nvidia-cusparselt-cu12==0.6.3
 + nvidia-nccl-cu12==2.21.5
 + nvidia-nvjitlink-cu12==12.6.85
 + nvidia-nvtx-cu12==12.6.77
</code></pre>
<p>as if I used <code>torch==2.6.0.dev20241203</code> instead of <code>torch==2.6.0.dev20241222</code> in the <code>pyproject.toml</code>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-23 14:44</div>
            <div class="timeline-body"><p>Or:
expected <code>uv tree</code> output:</p>
<pre><code>modded-nanogpt v0.1.0
├── huggingface-hub v0.27.0
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── packaging v24.2
│   ├── pyyaml v6.0.2
│   ├── requests v2.32.3
│   │   ├── certifi v2024.12.14
│   │   ├── charset-normalizer v3.4.0
│   │   ├── idna v3.10
│   │   └── urllib3 v2.3.0
│   ├── tqdm v4.67.1
│   └── typing-extensions v4.12.2
├── numpy v2.2.1
├── pytorch-triton v3.2.0+git35c6c7c6
├── torch v2.6.0.dev20241222+cu126
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── jinja2 v3.1.5
│   │   └── markupsafe v3.0.2
│   ├── networkx v3.4.2
│   ├── nvidia-cublas-cu12 v12.6.4.1
│   ├── nvidia-cuda-cupti-cu12 v12.6.80
│   ├── nvidia-cuda-nvrtc-cu12 v12.6.77
│   ├── nvidia-cuda-runtime-cu12 v12.6.77
│   ├── nvidia-cudnn-cu12 v9.5.1.17
│   │   └── nvidia-cublas-cu12 v12.6.4.1
│   ├── nvidia-cufft-cu12 v11.3.0.4
│   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-curand-cu12 v10.3.7.77
│   ├── nvidia-cusolver-cu12 v11.7.1.2
│   │   ├── nvidia-cublas-cu12 v12.6.4.1
│   │   ├── nvidia-cusparse-cu12 v12.5.4.2
│   │   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   │   └── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-cusparse-cu12 v12.5.4.2 (*)
│   ├── nvidia-cusparselt-cu12 v0.6.3
│   ├── nvidia-nccl-cu12 v2.21.5
│   ├── nvidia-nvjitlink-cu12 v12.6.85
│   ├── nvidia-nvtx-cu12 v12.6.77
│   ├── setuptools v75.6.0
│   ├── sympy v1.13.1
│   │   └── mpmath v1.3.0
│   └── typing-extensions v4.12.2
└── tqdm v4.67.1
(*) Package tree already displayed
</code></pre>
<p>actual <code>uv tree</code> output</p>
<pre><code>modded-nanogpt v0.1.0
├── huggingface-hub v0.27.0
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── packaging v24.2
│   ├── pyyaml v6.0.2
│   ├── requests v2.32.3
│   │   ├── certifi v2024.12.14
│   │   ├── charset-normalizer v3.4.0
│   │   ├── idna v3.10
│   │   └── urllib3 v2.3.0
│   ├── tqdm v4.67.1
│   └── typing-extensions v4.12.2
├── numpy v2.2.1
├── pytorch-triton v3.2.0+git35c6c7c6
├── torch v2.6.0.dev20241222+cu126
│   ├── filelock v3.16.1
│   ├── fsspec v2024.12.0
│   ├── jinja2 v3.1.5
│   │   └── markupsafe v3.0.2
│   ├── networkx v3.4.2
│   ├── setuptools v75.6.0
│   ├── sympy v1.13.1
│   │   └── mpmath v1.3.0
│   └── typing-extensions v4.12.2
└── tqdm v4.67.1
</code></pre>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-23 14:47</div>
            <div class="timeline-body"><p>I also updated the description of the issue.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-23 14:49</div>
            <div class="timeline-body"><p>I think it&#x27;s very specific and clear now.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-12-23 15:25</div>
            <div class="timeline-body"><p>Thank you! I think this is something that basically has to be solved by PyTorch. The issue is that the wheels for <code>2.6.0.dev20241222+cu126</code> don&#x27;t have consistent metadata, and it&#x27;s a fundamental assumption of uv that the metadata for a given version <em>is</em> consistent.</p>
<p>For example, the <a href="https://download.pytorch.org/whl/nightly/cu126/torch-2.6.0.dev20241222%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl">manylinux version</a> has:</p>
<pre><code>Requires-Dist: filelock
Requires-Dist: typing-extensions&gt;=4.10.0
Requires-Dist: setuptools; python_version &gt;= &quot;3.12&quot;
Requires-Dist: sympy==1.13.1; python_version &gt;= &quot;3.9&quot;
Requires-Dist: networkx
Requires-Dist: jinja2
Requires-Dist: fsspec
Requires-Dist: nvidia-cuda-nvrtc-cu12==12.6.77; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cuda-runtime-cu12==12.6.77; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cuda-cupti-cu12==12.6.80; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cudnn-cu12==9.5.1.17; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cublas-cu12==12.6.4.1; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cufft-cu12==11.3.0.4; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-curand-cu12==10.3.7.77; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cusolver-cu12==11.7.1.2; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cusparse-cu12==12.5.4.2; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-cusparselt-cu12==0.6.3; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-nccl-cu12==2.21.5; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-nvtx-cu12==12.6.77; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: nvidia-nvjitlink-cu12==12.6.85; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot;
Requires-Dist: pytorch-triton==3.2.0+git0d4682f0; platform_system == &quot;Linux&quot; and platform_machine == &quot;x86_64&quot; and python_version != &quot;3.13t&quot;
</code></pre>
<p>But the <a href="https://download.pytorch.org/whl/nightly/cu126/torch-2.6.0.dev20241222%2Bcu126-cp312-cp312-linux_aarch64.whl">ARM Linux version</a> has:</p>
<pre><code>Requires-Dist: filelock
Requires-Dist: typing-extensions&gt;=4.10.0
Requires-Dist: setuptools; python_version &gt;= &quot;3.12&quot;
Requires-Dist: sympy==1.13.1; python_version &gt;= &quot;3.9&quot;
Requires-Dist: networkx
Requires-Dist: jinja2
Requires-Dist: fsspec
Provides-Extra: optree
Requires-Dist: optree&gt;=0.13.0; extra == &quot;optree&quot;
Provides-Extra: opt-einsum
Requires-Dist: opt-einsum&gt;=3.3; extra == &quot;opt-einsum&quot;
</code></pre>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">question</span> removed by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-12-23 15:25</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">upstream</span> added by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-12-23 15:25</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-23 16:09</div>
            <div class="timeline-body"><p>got it, is there any workaround if I only care about linux?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-23 16:12</div>
            <div class="timeline-body"><p>I remember there is a way to specify metadata manually, but I don&#x27;t want to do so.
Is there a way to specify the source of metadata to be a given wheel?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-12-23 16:15</div>
            <div class="timeline-body"><p>Yeah, you could do:</p>
<pre><code>[project]
name = &quot;modded-nanogpt&quot;
version = &quot;0.1.0&quot;
requires-python = &quot;&gt;=3.12&quot;
dependencies = [
    &quot;torch&quot;,
]

[tool.uv]
environments = [
    &quot;platform_system == &#x27;Linux&#x27;&quot;,
]

[tool.uv.sources]
torch = [
    { url = &quot;https://download.pytorch.org/whl/nightly/cu126/torch-2.6.0.dev20241222%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl&quot; }
]
</code></pre>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-12-23 19:07</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/YouJiacheng">@YouJiacheng</a> on 2024-12-25 05:25</div>
            <div class="timeline-body"><p>Hmmmm, it seems that this will cause <code>uv sync</code> hang/stuck after cache clean (or after some time).
I need to <code>rm uv.lock</code> and then <code>uv sync</code> will download the torch wheel again.
do you have any idea?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/tmm1">@tmm1</a> on 2025-01-16 08:29</div>
            <div class="timeline-body"><p>Was this reported upstream? Has it been happening consistently on versions other than <code>2.6.0.dev20241222-cu126</code>?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-01-16 15:00</div>
            <div class="timeline-body"><p>Sorry, I would need a more thorough reproduction to help out.</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 19:36:22 UTC
    </footer>
</body>
</html>
