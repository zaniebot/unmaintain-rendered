<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CUDA PyTorch wheel not auto-detected on GTX1650 (CUDA 7.5) - astral-sh/uv #17236</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>CUDA PyTorch wheel not auto-detected on GTX1650 (CUDA 7.5)</h1>

    <div class="meta">
        <span class="state state-closed">Closed</span>
        <a href="https://github.com/astral-sh/uv/issues/17236">#17236</a>
        opened by <a href="https://github.com/aliphys">@aliphys</a>
        on 2025-12-24 23:44
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header">Issue opened by <a href="https://github.com/aliphys">@aliphys</a> on 2025-12-24 23:44</div>
            <div class="timeline-body"><h3>Summary</h3>
<p><a href="https://x.com/charliermarsh/status/1926250345887396217">Charlie's Twitter post</a> states that <em>You can set <code>UV_TORCH_BACKEND=auto</code> and uv will automatically install the right CUDA-enabled PyTorch for your machine, zero configuration</em>.</p>
<p>Great! PyTorch wheel fatigue is a real thing, especially when you are developing for different systems/architectures and want to maintain a sense of ~portability~ sanity.</p>
<p>Following the <a href="https://docs.astral.sh/uv/guides/integration/pytorch/#automatic-backend-selection">Using uv with PyTorch</a> documentation, I tried two ways to get the CUDA wheel to auto install:</p>
<ol>
<li><strong>via pyproject.toml</strong>
This is the contents of <code>pyproject.toml</code>, which was followed by <code>uv sync</code>.</li>
</ol>
<pre><code>[project]
name = &quot;mypytorchenv&quot;
version = &quot;0.1.0&quot;
description = &quot;Experimental workspace for ML/DL projects&quot;
requires-python = &quot;&gt;=3.9,&lt;3.13&quot;
dependencies = [
    &quot;torch&quot;,
    &quot;torchaudio&quot;,
    &quot;torchvision&quot;,
]

[tool.uv.pip]
torch-backend = &quot;auto&quot;
</code></pre>
<ol start="2">
<li><strong>via terminal</strong>
Ran command <code>uv pip install &quot;torch&quot; --torch-backend=auto</code></li>
</ol>
<p>For both cases, this is the output I get. Note that the PyTorch wheel for 2.9.1+cpu has been installed.</p>
<pre><code>Python 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
Ctrl click to launch VS Code Native REPL
&gt;&gt;&gt; import torch
&gt;&gt;&gt; print(torch.__version__)
2.9.1+cpu
</code></pre>
<p>I would ideally expect that the latest wheel with support for CUDA 7.5 is instead installed, given that this is the <a href="https://developer.nvidia.com/cuda/gpus">most recent CUDA version that the GTX 1650 supports</a>.</p>
<p><img width="1102" height="729" alt="Image" src="https://github.com/user-attachments/assets/e845d796-0f7f-420b-8d2c-2c2698e90e3d" /></p>
<p>Could it be that since there are <a href="https://discuss.pytorch.org/t/where-to-get-a-pytorch-version-with-cuda-7-5/136446/3?u=aliphys">no official CUDA 7.5 windows binary packages for PyTorch</a>, you have decided to fall back to the cpu-only version instead? In which case, it would be great to have a warning telling the user <code>CUDA 7.5 device detected. No official PyTorch builds available, falling back to cpu build.</code></p>
<h3>Platform</h3>
<p>Windows 11 64 bit</p>
<h3>Version</h3>
<p>uv 0.9.18 (0cee76417 2025-12-16)</p>
<h3>Python version</h3>
<p>Python 3.12.10</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">bug</span> added by @aliphys on 2025-12-24 23:44</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-12-25 03:01</div>
            <div class="timeline-body"><p>Can you share the output with the <code>--verbose</code> flag?</p>
<p>Note that 7.5 is not a CUDA version -- it's a compute capability. You can use a significantly newer CUDA runtime version with that GPU, likely up to and including CUDA 13.1.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by @aliphys on 2025-12-25 14:22</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/aliphys">@aliphys</a> on 2025-12-25 14:35</div>
            <div class="timeline-body"><p><strong>(After the email correspondence, was able to attach response by moving --verbose log to gist. Attaching here for future reference)</strong></p>
<hr />
<p>Hello @charliermarsh ðŸ‘‹. Thanks for the heads up regarding the CUDA / compute capability distinction.</p>
<h2>CUDA wheel installation attempt with --verbose</h2>
<p>I ran the commands in a new directory, without any <code>.venv</code> or <code>uv lock</code> in the folder. This is the result of their verbose output.</p>
<details><summary>via pyproject.toml</summary>

<p>https://gist.github.com/aliphys/6201ebdbe847aa6504ea4e167c87175a</p>
</details>

<details><summary>via terminal</summary>

<p>https://gist.github.com/aliphys/a3667e847758710e8284553551858273</p>
</details>

<hr />
<h2>PyTorch Check</h2>
<details><summary>via pyproject.toml</summary>

<pre><code>(uvtestdir) PS C:\Users\user\Desktop\uvtestdir&gt; python
Python 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; print(torch.__version__)
2.9.1+cpu
</code></pre>
</details>

<details><summary>via terminal</summary>

<pre><code>(uvtestdir) PS C:\Users\user\Desktop\uvtestdir&gt; python
Python 3.13.5 (main, Jun 12 2025, 12:42:35) [MSC v.1943 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
C:\Users\user\Desktop\uvtestdir\.venv\Lib\site-packages\torch\_subclasses\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\utils\tensor_numpy.cpp:84.)
  cpu = _conversion_method_template(device=torch.device(&quot;cpu&quot;))
&gt;&gt;&gt; print(torch.__version__)
2.9.1+cu130
</code></pre>
</details>

<h2>Summary</h2>
<p>When running in a clean, new directory with NO <code>uv.lock</code> or <code>.venv</code> file:</p>
<p>|Item|<code>pyproject.toml</code>|<code>uv pip install &quot;torch&quot; --torch-backend=auto</code>|details|
|-|-|-|-|
|PyTorch wheel |2.9.1+cpu|2.9.1+cu130| The exact same version of PyTorch (2.9.1) is installed. However, the key difference here is that the terminal installs the CUDA version. Whereas the .toml approach does falls back to the cpu wheel. This is in spite of the fact that the verbose output does make an attempt to install a CUDA wheel, so it seems that the need for CUDA is recognised, but along the way our friend gets lost. ðŸ˜„|
|Numpy |2.4.0| N/A|Numpy is not specified to be installed manually, neither in the <code>.toml</code> file nor manually. It is expected that this is installed, with uv taking care of the version conflicts. The <code>.toml</code> approach does resolve this issue. <strong>(Note, just realised this could be because I specified torchvision to be installed in the <code>.toml</code> and not in the terminal)</strong> |
|Python | 3.12.10|3.13.5|OK!|
|sys_platform variable| &quot;linux&quot;| N/A| It seems that the <code>sys_platform</code> is identifying my Windows 11 system as a Linux system. I am NOT running uv under WSL. This additional variable, could be the reason why the CUDA wheels are not installed?|
|Detected cuBlas wheel|nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_aarch64.whl|N/A|uv detects a linux wheel for cublas. I didn't see a message about this for the terminal approach, so I could not compare.|</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/aliphys">@aliphys</a> on 2025-12-25 14:41</div>
            <div class="timeline-body"><p>@charliermarsh 's suggestion over email:</p>
<pre><code>Thanks for replying! What might be happening here: the `--torch-backend` flags are only supported on the `uv pip` commands right now. We haven't added support to `uv sync` yet. Can you try `uv venv` then `uv pip install --torch-backend auto ...`?
</code></pre>
<p>âœ… Based on this, I now have an interim solution which works well enough.</p>
<pre><code>uv venv --python '&gt;=3.9, &lt;3.13'; uv pip install -r requirements.txt --torch-backend=auto --verbose
</code></pre>
<ul>
<li>Terminal verbose output: https://gist.github.com/aliphys/4a946eeb391353abe9eb1ae4e5555f5a</li>
</ul>
<p>PyTorch wheel <code>2.9.1+cu130</code> together with Numpy <code>2.4.0</code> are both happily installed ðŸ˜ƒ</p>
<pre><code>PS C:\Users\user\Desktop\uvtestdir&gt; .\.venv\Scripts\activate
(uvtestdir) PS C:\Users\user\Desktop\uvtestdir&gt; python
Python 3.12.12 (main, Dec  9 2025, 19:02:55) [MSC v.1944 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; print(torch.__version__)
2.9.1+cu130
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; print(numpy.__version__)
2.4.0
</code></pre>
<p>As a user, I would prefer to use <code>pyproject.toml</code> instead of a <code>requirements.txt</code> when we have PyTorch CUDA wheels. Looking forward to seeing this implemented in the future ðŸ¤ž</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-09 23:35:06 UTC
    </footer>
</body>
</html>
