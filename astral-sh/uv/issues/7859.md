```yaml
number: 7859
title: Torch installation instructions for different platforms
type: issue
state: closed
author: chitralverma
labels:
  - duplicate
  - question
assignees: []
created_at: 2024-10-02T07:19:55Z
updated_at: 2024-10-18T16:34:07Z
url: https://github.com/astral-sh/uv/issues/7859
synced_at: 2026-01-10T01:57:17Z
```

# Torch installation instructions for different platforms

---

_Issue opened by @chitralverma on 2024-10-02 07:19_

Hi, coming from poetry, I am trying out uv due to poetry's super slow resolutions and bulk.

Our workflow is as follows,

- Devs develop a project on their Macs with dependency `sentence-transformers` which pulls in torch as well. 
- Macs do not have GPU, so by default, we'd like to run all development and even deployment on CPU only
- For the final product env, where GPUs are available, we would like to have an extras flag in the project, let's say 'cuda' which pulls in GPU dependencies.

In short, we'd like to use uv to create a project in which the project source code remains the same, it is developed on Mac and only pulls in CPU dependencies by default. But for the target deployment environment where GPUs are available, we should be able to deploy the same project with an extra flag that pulls in the torch's GPU dependencies as well.

In poetry, we were following something like [this](https://github.com/python-poetry/poetry/issues/6409#issuecomment-2203773939), any suggestions on how to do this uv/ rye as we are very new with this?

Also referencing, https://github.com/pytorch/pytorch/issues/136275

**Dev Env Details:**
uv == latest, 0.4.18
python >= 3.8
platform: mac, sequoia, cpu-only env


**Prod Env Details:**
uv == latest, 0.4.18
python >= 3.8
platform: linux, gpu env

cc @charliermarsh @mitsuhiko

---

_Comment by @zanieb on 2024-10-02 14:56_

This is a duplicate of #5945 â€” there's a fair bit of discussion there. We're expanding support for this in https://github.com/astral-sh/uv/pull/7769.

---

_Label `duplicate` added by @zanieb on 2024-10-02 14:56_

---

_Label `question` added by @zanieb on 2024-10-02 14:56_

---

_Referenced in [astral-sh/rye#1407](../../astral-sh/rye/issues/1407.md) on 2024-10-04 06:07_

---

_Comment by @chitralverma on 2024-10-16 09:24_

closed by #7769

---

_Closed by @chitralverma on 2024-10-16 09:24_

---

_Reopened by @chitralverma on 2024-10-18 09:17_

---

_Comment by @chitralverma on 2024-10-18 09:34_

@charliermarsh even though #7769 is merged, i was wondering how we can achieve the task mentioned in the description of this issue.

you may link index to source and source to dependency, but python markers do not support environment variables as markers and if i create an extra "cuda" i don't know how to specify source or index in the optional-dependencies section.

basically if i install my project with --extras cpu it should only pull torch from the CPU index and if i provide --extras cuda then it should pull torch from a cuda index.

this also brings up the question what is exactly the point of having `sources` separate from `indexes`. Won't it be easier to just have something like this?

```
[project]
dependencies = [
    "linetimer>=0.1.5",
    "numpy==1.24.1",
]
name = "uv-project"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.8"

[tool.uv]
managed = true
dev-dependencies = ["black>=24.8.0", "pytest>=8.3.3", "ruff>=0.6.8"]

[project.optional-dependencies]
cuda = [
    {name = "torch>=2.1.0", index = "pytorch-cuda", marker = "sys_platform != 'darwin'"},
    {name = "torch>=2.1.0", index = "pytorch-cpu", marker = "sys_platform == 'darwin'"},
    "sentence-transformers"
]

cpu = [
    {name = "torch>=2.1.0", index = "pytorch-cpu", marker = "sys_platform != 'darwin'"},
    {name = "torch>=2.1.0", index = "pytorch-cpu", marker = "sys_platform == 'darwin'"},
    "sentence-transformers"
]

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cuda"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv.pip]
generate-hashes = true
universal = true

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

```
 

---

_Comment by @charliermarsh on 2024-10-18 16:33_

@chitralverma -- The specific case around using _extras_ to manage this is not supported yet, because extras aren't mutually exclusive (you could enable both `--extra cpu` and `--extra cuda` in the above example, and then you'd get a conflict). We're working on support for conflicting extras, but managing them via features like that knowingly doesn't work yet.

---

_Comment by @charliermarsh on 2024-10-18 16:34_

I'm gonna combine this issue with https://github.com/astral-sh/uv/issues/5945.

---

_Closed by @charliermarsh on 2024-10-18 16:34_

---
