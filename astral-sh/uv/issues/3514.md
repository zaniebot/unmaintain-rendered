```yaml
number: 3514
title: Add support for retrying connection failures
type: issue
state: closed
author: yeswalrus
labels:
  - bug
  - help wanted
  - network
assignees: []
created_at: 2024-05-10T17:59:01Z
updated_at: 2024-10-12T11:22:12Z
url: https://github.com/astral-sh/uv/issues/3514
synced_at: 2026-01-10T01:57:07Z
```

# Add support for retrying connection failures

---

_Issue opened by @yeswalrus on 2024-05-10 17:59_

Tested with uv 0.1.41

While looking for workarounds to https://github.com/astral-sh/uv/issues/3512, I experimented with using --no-cache.
In our case, this actually *increased* the failure rate, as we use a local artifactory based pypi mirror which appears rate limited.
Using the same bash script as in the linked issue but with --no-cache added, and with significantly more packages, we began observing instances where the artifactory server would be overwhelmed and UV would fail with `connection reset by peer`.

Under pip, this was also reproducible though it required a larger number of simultaneous processes (~80), but rather that failing outright PIP would retry connection failures. Please add (ideally configurable) support for retrying connection failures when downloading or querying indexes

---

_Comment by @charliermarsh on 2024-05-10 18:08_

Can you share more information, e.g., a trace with `--verbose` to demonstrate what's failing? In general, we do already retry.

---

_Label `question` added by @charliermarsh on 2024-05-10 18:17_

---

_Comment by @yeswalrus on 2024-05-10 22:34_

This may be an issue with our artifactory server having an artificially low rate limit or something but I'm not entirely sure.
Logs here:
```
Using Python 3.10.12 interpreter at: /usr/bin/python3
Creating virtualenv at: test_venvs/6c38fa92-edb8-4dc3-8c34-6a395504bd9d/
Activate with: source test_venvs/6c38fa92-edb8-4dc3-8c34-6a395504bd9d/bin/activate
Built 6 editables in 1.74s
Resolved 114 packages in 10.33s
error: Failed to download distributions
  Caused by: Failed to fetch wheel: plotly==4.12.0
  Caused by: Failed to extract archive
  Caused by: error decoding response body
  Caused by: request or response body error
  Caused by: error reading a body from connection
  Caused by: Connection reset by peer (os error 104)
```



---

_Comment by @samypr100 on 2024-05-10 22:39_

Potentially related to #3456

(worth noting that requests are already retried about 3 times by default)

---

_Comment by @zanieb on 2024-05-11 02:06_

We don't retry on TCP layer e.g. connection reset errors afaik. I think we'll need to add custom middleware to retry these.

If someone could link or summarize the relevant details about what exactly is retried today that'd be really helpful.

---

_Assigned to @zanieb by @zanieb on 2024-05-11 02:06_

---

_Comment by @samypr100 on 2024-05-11 02:42_

Fair, I only did a quick look and it seemed it would retry based on [reqwest-retry](https://github.com/TrueLayer/reqwest-middleware/blob/reqwest-retry-v0.5.0/reqwest-retry/src/retryable_strategy.rs#L144), but `is_connect` is likely not what I thought it was and `io::ErrorKind::ConnectionReset` would need to be checked separately.

Relevant uv code is here https://github.com/astral-sh/uv/blob/main/crates/uv-client/src/base_client.rs#L169

---

_Comment by @samypr100 on 2024-05-11 02:47_

Looking at it again, it does seem like it would retry on resets, see [retryable_strategy.rs#L192](https://github.com/TrueLayer/reqwest-middleware/blob/reqwest-retry-v0.5.0/reqwest-retry/src/retryable_strategy.rs#L192)

---

_Comment by @yeswalrus on 2024-05-11 02:50_

Complete (slightly obfuscated) log of one such instance captured with --verbose enabled, in case it helps with debugging.
It's worth noting, this is a less serious issue - I only start seeing it reliably when I've got ~40 simultaneous venvs being created and --no-cache set, at least with my artificial benchmark, so this is something of an edge case. Still, having some ability to throttle or specify longer timeout windows might be helpful

[uv_err.log](https://github.com/astral-sh/uv/files/15281422/uv_err.log)

---

_Comment by @zanieb on 2024-05-11 12:51_

Thanks for the links @samypr100, cc @konstin ?

---

_Referenced in [meltano/meltano#8539](../../meltano/meltano/issues/8539.md) on 2024-05-13 16:42_

---

_Referenced in [astral-sh/uv#3572](../../astral-sh/uv/issues/3572.md) on 2024-05-14 10:42_

---

_Comment by @notatallshaw on 2024-05-30 16:29_

FYI, I'm also seeing this same error semi-often in my work environment:

```
â ™ defusedxml==0.7.1                                                                                                                                                                                                                                                                                                                              error: Failed to download `defusedxml==0.7.1`
  Caused by: request or response body error
  Caused by: error reading a body from connection
  Caused by: Connection reset by peer (os error 104)
```

I'm not sure the details, but we have some kind of Palo Alto Firewall that is almost certainly causing the issue, running the uv command again almost always ends in success.

I'm going to test setting the new `UV_CONCURRENT_DOWNLOADS` (https://github.com/astral-sh/uv/pull/3493) to 1 and see if that makes a difference in how often I see this error. Though pip also has issues in my work environment from time to time, so it may just reduce the frequency of issues.

---

_Referenced in [astral-sh/uv#3933](../../astral-sh/uv/pulls/3933.md) on 2024-05-31 09:48_

---

_Comment by @charliermarsh on 2024-06-02 18:24_

I assume those retries don't apply to streaming operations. Like, they apply to the initial request itself, but probably not the entirety of reading from the stream? We can fix this by adding our own retry around download operations.

---

_Referenced in [TrueLayer/reqwest-middleware#159](../../TrueLayer/reqwest-middleware/pulls/159.md) on 2024-06-11 10:36_

---

_Referenced in [astral-sh/uv#4402](../../astral-sh/uv/issues/4402.md) on 2024-06-18 20:22_

---

_Referenced in [deepmodeling/deepmd-kit#3889](../../deepmodeling/deepmd-kit/pulls/3889.md) on 2024-06-19 23:41_

---

_Comment by @aybidi on 2024-06-25 15:45_

@charliermarsh is this any work planned around this issue? We reconfigured the `UV_CONCURRENT_DOWNLOADS` to avoid running into this issue frequently. While it helped, our CI builds still run into connection failures here and there

---

_Comment by @zanieb on 2024-06-25 16:18_

Hi @aybidi â€” we think this is important to address but haven't started working on a fix yet. If anyone is willing to investigate and contribute in the meantime I'm happy to review.

---

_Label `question` removed by @zanieb on 2024-06-25 16:19_

---

_Label `bug` added by @zanieb on 2024-06-25 16:19_

---

_Label `help wanted` added by @zanieb on 2024-06-25 16:19_

---

_Comment by @notatallshaw on 2024-06-25 16:30_

> I'm going to test setting the new `UV_CONCURRENT_DOWNLOADS` (#3493) to 1 and see if that makes a difference in how often I see this error.

FWIW, for my use case setting this did not help, but one day the firewall just started behaving better and I've not seen the error again.

---

_Comment by @kujenga on 2024-07-01 16:59_

Here's an example of a similar class of error that I observed in a CI build that would be nice to have uv retry automatically:

```
error: Failed to download distributions
  Caused by: Failed to fetch wheel: torch==2.1.0
  Caused by: Failed to extract archive
  Caused by: request or response body error: error reading a body from connection: stream error received: unspecific protocol error detected
  Caused by: error reading a body from connection: stream error received: unspecific protocol error detected
  Caused by: stream error received: unspecific protocol error detected
```

---

_Label `network` added by @konstin on 2024-07-02 09:35_

---

_Referenced in [astral-sh/uv#4725](../../astral-sh/uv/pulls/4725.md) on 2024-07-02 09:49_

---

_Referenced in [astral-sh/uv#4739](../../astral-sh/uv/issues/4739.md) on 2024-07-02 20:53_

---

_Comment by @messense on 2024-07-09 07:44_

I'm also hitting this issue, it's rather painful when going through a corporate proxy that may have concurrent connection limits, retry manually doesn't always work in CI.

---

_Comment by @konstin on 2024-07-09 08:34_

@messense With the latest release, what's the error message, does it fail with retries or without?

---

_Comment by @messense on 2024-07-09 08:47_

I'm using uv 0.2.23, the error message is

```
error: Failed to prepare distributions
  Caused by: Failed to fetch wheel: jaxlib==0.4.16+cuda12.cudnn89
  Caused by: Failed to extract archive
  Caused by: error decoding response body
  Caused by: request or response body error
  Caused by: error reading a body from connection
  Caused by: Connection reset by peer (os error 104)
```

---

_Comment by @messense on 2024-07-10 02:51_

Judging from the [code](https://github.com/TrueLayer/reqwest-middleware/blob/cb2121478ad2de4ba04c6261416b5e11c4c7ebec/reqwest-retry/src/retryable_strategy.rs#L145-L150), `reqwest-retry` simply won't retry body errors.

---

_Comment by @charliermarsh on 2024-07-10 02:52_

Thanks, good catch! Let's fix that.

---

_Comment by @benjamin-hodgson on 2024-07-10 11:52_

Sounds like the fix would need to be on uv's end: https://github.com/TrueLayer/reqwest-middleware/issues/47#issuecomment-1170955570

---

_Referenced in [astral-sh/uv#4960](../../astral-sh/uv/pulls/4960.md) on 2024-07-10 14:21_

---

_Comment by @zanieb on 2024-07-10 14:25_

I put up a patch at #4960 if anyone wants to give it a try.

---

_Comment by @benjamin-hodgson on 2024-07-10 16:07_

I'll take [this build](https://github.com/astral-sh/uv/actions/runs/9876778011) for a spin on my testing farm. The networking issues I was suffering were infrequent so I won't be able to say for certain but if it shows up I'll let you know

---

_Comment by @zanieb on 2024-07-10 16:10_

We'll probably release it soon too! Thanks though :)

---

_Comment by @benjamin-hodgson on 2024-07-10 17:34_

That build consistently gets stack overflow errors for me. I'm on Windows.

```
> uv.exe pip install --verbose cowsay

DEBUG uv 0.2.23
DEBUG Searching for Python interpreter in system path or `py` launcher
DEBUG Found cpython 3.12.3 at `D:\CosmosAnalytics\private\cosmos\Scope\pyscope\.venv\Scripts\python.exe` (virtual environment)
DEBUG Using Python 3.12.3 environment at .venv\Scripts\python.exe
DEBUG Acquired lock for `.venv`
DEBUG At least one requirement is not satisfied: cowsay
DEBUG Using request timeout of 30s
DEBUG Solving with installed Python version: 3.12.3
DEBUG Adding direct dependency: cowsay*
DEBUG No cache entry for: https://pypi.org/simple/cowsay/
DEBUG Searching for a compatible version of cowsay (*)
DEBUG Selecting: cowsay==6.1 (cowsay-6.1-py3-none-any.whl)
DEBUG No cache entry for: https://files.pythonhosted.org/packages/f1/13/63c0a02c44024ee16f664e0b36eefeb22d54e93531630bd99e237986f534/cowsay-6.1-py3-none-any.whl.metadata

thread 'main' has overflowed its stack
```

---

_Comment by @charliermarsh on 2024-07-10 17:39_

Are you using a release build?

---

_Comment by @benjamin-hodgson on 2024-07-10 17:42_

I just grabbed the artifact from the CI build, is that not what I was meant to do?

---

_Comment by @benjamin-hodgson on 2024-07-10 18:33_

Found [this](https://github.com/astral-sh/uv/blob/631994c485560fa950bc576bc15c99a761c6b932/CONTRIBUTING.md#testing-on-windows), will try with that env var

---

_Comment by @messense on 2024-07-11 02:35_

@zanieb #4960 does not work for me, still gives the same error

```
installing uv...
done! âœ¨ ðŸŒŸ âœ¨
  installed package uv 0.2.24, installed using Python 3.10.12
...
...
Resolved 258 packages in 34.54s
error: Failed to prepare distributions
  Caused by: Failed to fetch wheel: nvidia-cudnn-cu12==8.9.2.26
  Caused by: Failed to extract archive
  Caused by: error decoding response body
  Caused by: request or response body error
  Caused by: error reading a body from connection
  Caused by: Connection reset by peer (os error 104)
```

---

_Comment by @zanieb on 2024-07-11 03:56_

That's so tragic ðŸ˜­ okay I'll dig deeper. I'll need to reproduce it with a fake server or something. If anyone has time to poke at a reproduction, let me know!

---

_Comment by @messense on 2024-07-12 06:22_

If I add a `panic!()` inside https://github.com/astral-sh/uv/blob/23c6cd774b466924d02de23add7101bcaa7b7c3e/crates/uv-client/src/base_client.rs#L291

then inject a connection reset error using `sudo iptables -A OUTPUT -p tcp --dport 3128 -j REJECT --reject-with tcp-reset` (our proxy is running on port 3128), the panic never happens, only the `Connection reset by peer` error message is printed, so my guess is that `reqwest-retry` or `reqwest-middleware` can't handle such kind of retry strategy at the moment.

---

_Comment by @samypr100 on 2024-07-12 12:12_

^ yea, I was able to achieve similar results using @hauntsaninja https://github.com/hauntsaninja/nginx_pypi_cache and @messense's approach
I also tried with `tc` linux utility by dropping/corrupting packets and gave me other types of unrelated errors such as BufErrors ðŸ˜‚

---

_Comment by @messense on 2024-07-16 01:40_

I think the issue is with `stream_wheel`, if I change https://github.com/astral-sh/uv/blob/9a44bc1d3567e0a2ba31675bc35c50392fc2f5ad/crates/uv-distribution/src/distribution_database.rs#L211 to remove the `if` to unconditional try `download_wheel` when `stream_wheel` fails with `Extract` error, `uv` correctly retries.

```
error: Failed to prepare distributions
  Caused by: Failed to fetch wheel: beautifulsoup4==4.12.3
  Caused by: Request failed after 3 retries
  Caused by: error sending request for url (https://files.pythonhosted.org/packages/b1/fe/e8c672695b37eecc5cbf43e1d0638d88d66ba3a44c4d321c796f4e59167f/beautifulsoup4-4.12.3-py3-none-any.whl)
  Caused by: client error (Connect)
  Caused by: tcp connect error: Connection refused (os error 111)
  Caused by: Connection refused (os error 111)
```

so my guess is that `reqwest-retry` does not support retrying streaming response? Can we have a config/option/env var to force `download_wheel`?

---

_Comment by @charliermarsh on 2024-07-16 01:49_

That's interesting, the actual download code is really similar for those two methods. They both use a `bytes_stream`, etc...

---

_Comment by @charliermarsh on 2024-07-16 01:51_

I guess I don't see what's different between those methods. The latter just streams the wheel as-is to disk then unzips it; the former unzips as it streams.

---

_Comment by @messense on 2024-07-16 01:53_

Maybe because the unzip error causing an early error return?

---

_Referenced in [astral-sh/uv#5094](../../astral-sh/uv/pulls/5094.md) on 2024-07-16 03:11_

---

_Comment by @messense on 2024-07-19 01:24_

Happy to report `uv` v0.2.26 runs smoothly for me, no more failures when downloading wheels.

---

_Comment by @samypr100 on 2024-07-19 01:39_

I'm unable to reproduce in my end either, curious if this resolves other's issues as well.

---

_Comment by @zanieb on 2024-07-19 01:47_

I'll close this while I'm here â€” I suspect we've fixed it. Feel free to chime in if you encounter this still!

Thanks @messense !

---

_Closed by @zanieb on 2024-07-19 01:47_

---

_Comment by @stinodego on 2024-07-26 21:57_

@zanieb I am seeing similar errors in our CI, e.g.:

https://github.com/pola-rs/polars/actions/runs/10096006345/job/27917486333?pr=17870

```
error: Failed to download `torch==2.4.0+cpu`
  Caused by: Failed to unzip wheel: torch-2.4.0+cpu-cp312-cp312-win_amd64.whl
  Caused by: an upstream reader returned an error: an error occurred during transport: error decoding response body
  Caused by: an error occurred during transport: error decoding response body
  Caused by: error decoding response body
  Caused by: request or response body error
  Caused by: error reading a body from connection
  Caused by: end of file before message length reached
```

This pops up sometimes, rerunning the workflow fixes it. Probably something to do with the custom index (https://download.pytorch.org/whl/cpu) having some stability issues. But in this case, I would expect a retry to fix the issue. But retry behavior doesn't seem configurable for uv.

Any pointers are appreciated!

---

_Referenced in [pola-rs/polars#17899](../../pola-rs/polars/issues/17899.md) on 2024-07-26 22:02_

---

_Comment by @charliermarsh on 2024-07-26 23:54_

@stinodego -- Just confirming that you're on the most recent version of uv?

---

_Comment by @stinodego on 2024-07-27 06:51_

@charliermarsh These failures happened with uv 0.2.29 (you can check the link to our GitHub Actions to see some non-verbose logs). I'm pretty sure I've seen it on earlier versions as well.

Haven't seen it yet with 0.2.30, but I can post here if I do see it. But I don't believe 0.2.30 contains any fixes related to this issue.

If it helps, I can set our CI to verbose to get better logs on this issue?

---

_Comment by @charliermarsh on 2024-07-27 12:27_

@stinodego -- Yeah I wouldn't expect any change in 0.2.30. Verbose could be helpful because I'm trying to understand if we're retrying the download or not.

---

_Comment by @charliermarsh on 2024-07-27 12:34_

Interesting, that error indicates that we tried to download the wheel during resolution which is also slightly confusing. That would mean we failed to fetch the metadata from the index and had to fall back to downloading the wheel itself.

---

_Comment by @stinodego on 2024-07-27 20:04_

> Verbose could be helpful because I'm trying to understand if we're retrying the download or not.

I set our CI to verbose mode - will report back if I spot the error again.

---

_Referenced in [astral-sh/uv#5555](../../astral-sh/uv/pulls/5555.md) on 2024-07-29 11:33_

---

_Comment by @konstin on 2024-07-29 11:33_

I think https://github.com/astral-sh/uv/pull/5555 should fix this.

---

_Comment by @laurence-kobold on 2024-08-30 21:11_

I'm running into transient network issues when installing packages via git. Looking at the logs, there doesn't seem to be any retries for failures of `git clone` operations. This is on v0.3.1 of uv. Is it possible to add retries here as well?

---

_Comment by @notatallshaw-gts on 2024-09-10 16:45_

I've seen a regression in this recently, in the last few days, using the latest version of uv, I've started seeing:

```
â ¼ defusedxml==0.7.1                                                                                                                                                                                                                                          error: Failed to download `defusedxml==0.7.1`
  Caused by: request or response body error
  Caused by: error reading a body from connection
  Caused by: Connection reset by peer (os error 104)
```

I run again and it's fine, but I thought uv was now retrying these low level network errors? 

It could of course just be my corporate network environment getting worse, worth reporting a new issue?

---

_Comment by @konstin on 2024-09-10 18:17_

Do you have more details at which phase and with which index this happens? I simulated some connection errors but could only trigger cases with retries.

---

_Comment by @notatallshaw-gts on 2024-09-10 18:28_

All the errors were related to running `uv pip compile`, so it was only trying to collect metadata? I don't have any additional output than what I posted.

The index is https://pypi.org/, but the network involves a Palo Alto firewall that will be decrypting and encrypting traffic, and it seems occasionally this will just fail (either kill the connection or send an empty body).

---

_Comment by @jgehrcke on 2024-10-12 11:22_

> I've seen a regression in this recently, in the last few days, using the latest version of uv,

I think I can confirm. Just opened https://github.com/astral-sh/uv/issues/8144 before I found this issue here.

---

_Referenced in [astral-sh/uv#8144](../../astral-sh/uv/issues/8144.md) on 2024-10-12 16:08_

---

_Referenced in [astral-sh/uv#10570](../../astral-sh/uv/issues/10570.md) on 2025-01-13 16:07_

---

_Referenced in [seanmonstar/reqwest#2819](../../seanmonstar/reqwest/issues/2819.md) on 2025-09-18 07:21_

---
