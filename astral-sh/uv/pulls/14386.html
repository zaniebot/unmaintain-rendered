<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Add auto-detection for Intel GPUs - astral-sh/uv #14386</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>Add auto-detection for Intel GPUs</h1>

    <div class="meta">
        <span class="state-icon state-merged"></span>
        <a href="https://github.com/astral-sh/uv/pull/14386">#14386</a>
        opened by <a href="https://github.com/guangyey">@guangyey</a>
        on 2025-07-01 07:17
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/guangyey">@guangyey</a></div>
            <div class="timeline-body">

Summary
<p>This PR intends to enable <code>--torch-backend=auto</code> to detect Intel GPUs automatically:</p>
<ul>
<li>On Linux, detection is performed using the <code>lspci</code> command via <code>Display controller</code> id.</li>
<li>On Windows, ~~detection is done via a <code>powershell</code> query to <code>Win32_VideoController</code>~~. Skip support for nowâ€”revisit once a better solution is available.</li>
</ul>
<p>Currently, Intel GPUs (XPU) do not rely on specific driver or toolkit versions to distribute different PyTorch wheels.</p>
Test Plan


<p>On Linux:
<img src="https://github.com/user-attachments/assets/f7f238e3-a797-42ea-b8fa-9b028dfd4db5" alt="image">
~~On Windows:
<img src="https://github.com/user-attachments/assets/a10d774e-1cb9-431b-bb85-e3e8225df98f" alt="image">~~</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from &quot;Add auto-detection for Intel GPUs&quot; to &quot;[WIP] Add auto-detection for Intel GPUs&quot; by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-01 07:18</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/konstin">@konstin</a> on 2025-07-01 09:28</div>
            <div class="timeline-body"><p>Do we need to spawn a <code>std::process::Command</code> to check this or is there e.g. a file we could read that has the same information?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from <a href="https://github.com/charliermarsh">@charliermarsh</a> by <a href="https://github.com/konstin">@konstin</a> on 2025-07-01 09:29</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">enhancement</span> added by <a href="https://github.com/konstin">@konstin</a> on 2025-07-01 09:29</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-01 09:53</div>
            <div class="timeline-body"><blockquote>
<p>Do we need to spawn a <code>std::process::Command</code> to check this or is there e.g. a file we could read that has the same information?</p>
</blockquote>
<p>OK, I will try it.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/konstin">@konstin</a> on 2025-07-01 09:56</div>
            <div class="timeline-body"><p>If you have links to websites (e.g. from Intel) about how to detect these cards, please share them, it helps with reviewing this code.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Converted to draft by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-01 10:00</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Marked ready for review by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-02 10:16</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from &quot;[WIP] Add auto-detection for Intel GPUs&quot; to &quot;Add auto-detection for Intel GPUs&quot; by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-02 10:17</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-02 10:22</div>
            <div class="timeline-body"><blockquote>
<p>Do we need to spawn a <code>std::process::Command</code> to check this or is there e.g. a file we could read that has the same information?</p>
</blockquote>
<p>At the moment, we lack a stable and consistent method to retrieve Intel driver or GPU details from a file. So I intend to use system calls to keep the approach stable across platforms. May I know if it is reasonable and acceptable to you @konstin @charliermarsh</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from <a href="https://github.com/geofft">@geofft</a> by <a href="https://github.com/zanieb">@zanieb</a> on 2025-07-02 14:52</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/geofft">@geofft</a> on 2025-07-02 15:34</div>
            <div class="timeline-body"><p>Overall adding this detection seems good, but I agree with avoiding the subprocess.</p>
<p>On the Linux side, you can do this by using sysfs (which is what lspci is going to do anyway), here&#x27;s some very rough Python for it that I&#x27;ll let someone else convert to Rust ðŸ˜„:</p>
<pre><code>import pathlib

PCI_BASE_CLASS_MASK = 0xff0000
PCI_BASE_CLASS_DISPLAY = 0x030000
PCI_VENDOR_ID_INTEL = 0x8086

def has_intel_graphics():
    try:
        for device in pathlib.Path(&quot;/sys/bus/pci/devices&quot;).iterdir():
            try:
                pci_class = int((device / &quot;class&quot;).read_text(), 16)
                vendor = int((device / &quot;vendor&quot;).read_text(), 16)
            except IOError:
                continue # maybe log something?
            if (pci_class &amp; PCI_BASE_CLASS_MASK == PCI_BASE_CLASS_DISPLAY) and (vendor == PCI_VENDOR_ID_INTEL):
                return True
    except IOError:
        pass # maybe log something?
    return False
</code></pre>
<p>The Windows side looks like it&#x27;s using WMI and I would expect there&#x27;s an equivalent API approach for this, too, to save the subprocess.</p>
<p>Maybe the <a href="https://docs.rs/pci-info/">pci-info crate</a> is helpful? It claims to be cross platform and probably looking at PCI is correct on Windows too?</p>
<p>(If we absolutely must go with lspci, let&#x27;s use <code>lspci -m -n -d 8086::03xx</code> to get machine-parseable output and have lspci do the filtering, instead of using a shell and grep. Among other things, the current code matches the word &quot;Intelligent&quot; which shows up in various non-Intel product names in the PCI DB.)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/geofft">@geofft</a> on 2025-07-02 15:55</div>
            <div class="timeline-body"><p>Also - should we be trying to do something to detect whether we have an Intel GPU actually capable of interesting GPGPU work? I assume a lot of users will have a very boring Intel integrated GPU, and this change as written will switch them to the XPU download instead of the CPU one because we&#x27;ll see an Intel-brand display device. Will they actually benefit from the XPU download?</p>
<p>I saw that WMI has some info on whether &quot;accelerators&quot; are present, is it worth trying to parse that? Or should we look for whether <code>/dev/dri*/render</code> is present and accessible to the current user? Or should we compare against the list of PCI device IDs at https://dgpu-docs.intel.com/devices/hardware-table.html?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/geofft">@geofft</a> on 2025-07-02 16:20</div>
            <div class="timeline-body"><p>OK, from a bit of very quick poking - I think the only useful info WMI gives us is the (human-readable) device name and a parseable string with PCI vendor/device IDs. So if we want to avoid string-matching <code>Intel</code>, we might as well look at PCI info... can we try using the <code>pci-info</code> crate?</p>
<p>(It appears that <code>pci-info</code> has two backends for Windows, &quot;SetupAPI&quot; and WMI. I don&#x27;t know what the benefits of each are, but WMI is an optional crate feature and not used by default so I assume it&#x27;s a heavier-weight dependency. In either case let&#x27;s keep an eye on performance and binary size.)</p>
<p>By the way, what&#x27;s a good way to test this? Intel Tiber AI Cloud? (AWS/Google/etc. don&#x27;t have Intel GPUs, do they?)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-03 10:15</div>
            <div class="timeline-body"><p>@geofft Thanks for your suggestion. I followed your code to change the Linux detection pass instead of using subprocess. And change Windows pass to match vendor id instead of string <code>Intel</code>, which is easy to mismatch with <code>intelligent</code>.</p>
<blockquote>
<blockquote>
<blockquote>
<p>Also - should we be trying to do something to detect whether we have an Intel GPU actually capable of interesting GPGPU work? I assume a lot of users will have a very boring Intel integrated GPU, and this change as written will switch them to the XPU download instead of the CPU one because we&#x27;ll see an Intel-brand display device. Will they actually benefit from the XPU download?</p>
</blockquote>
</blockquote>
</blockquote>
<p>Thatâ€™s a fair concern. Right now the detection is fairly coarse. Some old iGPU are widely available and easily accessible, which gives community users a great oppotunity to learn and experiment with the GPU-enabled version of PyTorch. In addition, some newer iGPU are actually quite powerful (e.g. LNL), which even outperform certain discrete GPUs, and users can benefit from real performance gains.
Also the XPU version of PyTorch runs perfectly fine on CPUs, so there&#x27;s no downside there. Since we only fall back to detecting Intel GPUs after checking NVDIA and ROCm, this logic shouldn&#x27;t interfere with users on those platforms. Perhaps, we could go with this apporach for now and adjust the detection logic later basd on real feedback.</p>
<blockquote>
<blockquote>
<blockquote>
<p>can we try using the pci-info crate</p>
</blockquote>
</blockquote>
</blockquote>
<p>TBH, I&#x27;m still a Rust beginner, so integrating <code>pci-info</code> might take some time. Also, if you insist on preferring <code>pci-info</code>, I will try it. Should we consider using it for both Linux and Windows to handle detection consistently?</p>
<blockquote>
<blockquote>
<blockquote>
<p>By the way, what&#x27;s a good way to test this? Intel Tiber AI Cloud? (AWS/Google/etc. don&#x27;t have Intel GPUs, do they?)</p>
</blockquote>
</blockquote>
</blockquote>
<p>Yes, the better way is test it in iGPU, which is more widely available.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-03 10:23</div>
            <div class="timeline-body"><p>BTW, the windows CI failure is irrelevant to this PR, right?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-04 03:40</div>
            <div class="timeline-body"><blockquote>
<p>BTW, the windows CI failure is irrelevant to this PR, right?</p>
</blockquote>
<p>I forgot to update <code>Cargo.lock</code>, which caused the Clippy check to fail on Windows. Rust is smart enough to catch it â€” I&#x27;m really starting to love this language!</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/geofft">@geofft</a> approved on 2025-07-04 14:14</div>
            <div class="timeline-body"><p>OK, this seems fine to me, at least on the Linux side - I&#x27;d appreciate if someone else can form an opinion on the Windows side on e.g. whether we can assume it&#x27;s safe to run <code>powershell</code>, but the logic seems plausible there too.</p>
<p>Thanks for the updates and for contributing this!</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-07-04 16:02</div>
            <div class="timeline-body"><p>Is it important that we support this on Windows for now? Iâ€™m hesitant to merge code that relies on a PowerShell invocation.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-06 06:21</div>
            <div class="timeline-body"><p>@charliermarsh @geofft I completely understand your concerns regarding Windows. You&#x27;re absolutely right â€” relying on <code>shellpower</code> doesnâ€™t seem like an elegant or robust solution, and it&#x27;s reasonable that we shouldn&#x27;t put that burden on <code>uv</code>. Respecting your perspective, I agree that for now we should limit Intel GPU auto-detection to Linux only, and revisit Windows support once we have a cleaner and more reliable approach. Do you think this is reasonable?</p>
<p>Removed the code for Windows support and highlighted the limitation on the docs.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/charliermarsh">@charliermarsh</a> reviewed on 2025-07-08 20:16</div>
            <div class="timeline-body"></div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on <code>crates/uv-torch/src/accelerator.rs</code>:176 on 2025-07-08 20:16</div>
            <div class="timeline-body"><p>Is this different than using <code>fs::read_dir</code>, since we&#x27;re only going one depth?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/charliermarsh">@charliermarsh</a> reviewed on 2025-07-08 20:37</div>
            <div class="timeline-body"></div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on <code>crates/uv-torch/src/accelerator.rs</code>:176 on 2025-07-08 20:37</div>
            <div class="timeline-body"><p>(Apart from this, the rest of the change looks good and I&#x27;m happy to merge once resolved.)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/guangyey">@guangyey</a> on <code>crates/uv-torch/src/accelerator.rs</code>:176 on 2025-07-09 01:21</div>
            <div class="timeline-body"><p>CI clippy told me <code>fs::read_dir</code> is disallowed in <code>uv</code>, please refer to https://github.com/astral-sh/uv/blob/5e2dc5a9aa18aaa942f26513ca3b8ae4704eb018/clippy.toml#L21-L29</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/guangyey">@guangyey</a> reviewed on 2025-07-09 01:21</div>
            <div class="timeline-body"></div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/charliermarsh">@charliermarsh</a> reviewed on 2025-07-09 01:54</div>
            <div class="timeline-body"></div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on <code>crates/uv-torch/src/accelerator.rs</code>:176 on 2025-07-09 01:54</div>
            <div class="timeline-body"><p>Ohh, it wants you to use <code>fs_err::read_dir</code>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/guangyey">@guangyey</a> reviewed on 2025-07-09 03:00</div>
            <div class="timeline-body"></div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/guangyey">@guangyey</a> on <code>crates/uv-torch/src/accelerator.rs</code>:176 on 2025-07-09 03:00</div>
            <div class="timeline-body"><p>@charliermarsh Thanks for the suggestion! Iâ€™ve switched to using <code>fs_err::read_dir</code> as recommended. Both the CI and local checks now pass.
<img src="https://github.com/user-attachments/assets/c5c05406-b2af-498d-ab88-028606a0a014" alt="image"></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/charliermarsh">@charliermarsh</a> approved on 2025-07-09 13:19</div>
            <div class="timeline-body"><p>Thank you!</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Merged by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-07-09 13:31</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2025-07-09 13:31</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/guangyey">@guangyey</a> on 2025-07-10 02:00</div>
            <div class="timeline-body"><p>Happy to help. Thanks for your review and feedback.</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 19:52:58 UTC
    </footer>
</body>
</html>
