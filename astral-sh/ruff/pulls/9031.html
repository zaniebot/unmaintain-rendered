<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>release: switch to Cargo's default - astral-sh/ruff #9031</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>release: switch to Cargo&#x27;s default</h1>

    <div class="meta">
        <span class="state-icon state-merged"></span>
        <a href="https://github.com/astral-sh/ruff/pull/9031">#9031</a>
        opened by <a href="https://github.com/BurntSushi">@BurntSushi</a>
        on 2023-12-07 00:37
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/BurntSushi">@BurntSushi</a></div>
            <div class="timeline-body"><p>This sets <code>lto = &quot;thin&quot;</code> instead of using &quot;fat&quot; LTO, and sets <code>codegen-units = 16</code>. These are the defaults for Cargo&#x27;s <code>release</code> profile, and I think it may give us faster iteration times, especially when benchmarking. The point of this PR is to see what kind of impact this has on benchmarks. It is expected that benchmarks may regress to some extent.</p>
<p>I did some quick ad hoc experiments to quantify this change in compile times. Namely, I ran:</p>
<pre><code>cargo build --profile release -p ruff_cli</code></pre>
<p>Then I ran</p>
<pre><code>touch crates/ruff_python_formatter/src/expression/string/docstring.rs</code></pre>
<p>(because that&#x27;s where i&#x27;ve been working lately) and re-ran</p>
<pre><code>cargo build --profile release -p ruff_cli</code></pre>
<p>This last command is what I timed, since it reflects how much time one has to wait between making a change and getting a compiled artifact.</p>
<p>Here are my results:</p>
<ul>
<li>With status quo <code>release</code> profile, build takes 77s</li>
<li>with <code>release</code> but <code>lto = &quot;thin&quot;</code>, build takes 41s</li>
<li>with <code>release</code>, but <code>lto = false</code>, build takes 19s</li>
<li>with <code>release</code>, but <code>lto = false</code> <strong>and</strong> <code>codegen-units = 16</code>, build takes 7s</li>
<li>with <code>release</code>, but <code>lto = &quot;thin&quot;</code> <strong>and</strong> <code>codegen-units = 16</code>, build takes 16s (i believe this is the default <code>release</code> configuration)</li>
</ul>
<p>This PR represents the last option. It&#x27;s not the fastest to compile, but it&#x27;s nearly a whole minute faster! The idea is that with <code>codegen-units = 16</code>, we still make use of parallelism, but keep <em>some</em> level of LTO on to try and re-gain what we lose by increasing the number of codegen units.</p>


Summary


Test Plan


</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from <a href="https://github.com/MichaReiser">@MichaReiser</a> by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-07 00:37</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from <a href="https://github.com/charliermarsh">@charliermarsh</a> by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-07 00:37</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from <a href="https://github.com/zanieb">@zanieb</a> by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-07 00:37</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from <a href="https://github.com/konstin">@konstin</a> by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-07 00:37</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-12-07 01:36</div>
            <div class="timeline-body"><p>Looks like the benchmarks didn&#x27;t run ü§î</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-12-07 01:55</div>
            <div class="timeline-body"><p>Yeah, seems like our determine changes is too aggressive.</p>
<p>Would you mind running our hyperfine benchmarks (linting the cpython code base) in addition to the micro benchmarks to get a better understanding of how the performance of the CLI etc is impacted?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">internal</span> added by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-12-07 01:55</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/zanieb">@zanieb</a> on 2023-12-07 02:40</div>
            <div class="timeline-body"><p>Hm looks like #8225 has a bug in it since this <em>should</em> have been detected as a code change @Cjkjvfnby</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/zanieb">@zanieb</a> on 2023-12-07 02:53</div>
            <div class="timeline-body"><p>Here&#x27;s the fix? ~<a href="https://github.com/astral-sh/ruff/pull/9035">astral-sh/ruff#9035</a>~ <a href="https://github.com/astral-sh/ruff/pull/9038">astral-sh/ruff#9038</a></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/konstin">@konstin</a> approved on 2023-12-07 13:07</div>
            <div class="timeline-body"><p>While we&#x27;re at it, could we rename <code>release-debug</code> to <code>profiling</code>? That makes it clearer why this profile exists.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/codspeed-hq[bot]">@codspeed-hq[bot]</a> on 2023-12-07 13:41</div>
            <div class="timeline-body"><a href="https://codspeed.io/astral-sh/ruff/branches/ag/default-release-experiment">CodSpeed Performance Report</a>
Merging #9031 will <strong>degrade performances by 5.92%</strong>
<p>Comparing <code>ag/default-release-experiment</code> (9bff2ec) with <code>main</code> (c014622)</p>
Summary
<p><code>‚ùå 7</code> regressions
<code>‚úÖ 23</code> untouched benchmarks</p>
<blockquote>
<p>:warning: <em>Please fix the performance issues or <a href="https://codspeed.io/astral-sh/ruff/branches/ag/default-release-experiment">acknowledge them on CodSpeed</a>.</em></p>
</blockquote>
Benchmarks breakdown
<p>|     | Benchmark | <code>main</code> | <code>ag/default-release-experiment</code> | Change |
| --- | --------- | ----------------------- | ------------------- | ------ |
| ‚ùå | <code>linter/all-rules[numpy/globals.py]</code> | 4 ms | 4.2 ms | -4.38% |
| ‚ùå | <code>linter/all-rules[pydantic/types.py]</code> | 73 ms | 77.6 ms | -5.92% |
| ‚ùå | <code>linter/all-rules[numpy/ctypeslib.py]</code> | 34.7 ms | 36.2 ms | -4.3% |
| ‚ùå | <code>linter/all-with-preview-rules[numpy/globals.py]</code> | 4.2 ms | 4.5 ms | -5.02% |
| ‚ùå | <code>linter/all-with-preview-rules[pydantic/types.py]</code> | 81.1 ms | 85 ms | -4.59% |
| ‚ùå | <code>linter/all-with-preview-rules[large/dataset.py]</code> | 187.2 ms | 198.7 ms | -5.77% |
| ‚ùå | <code>linter/all-with-preview-rules[numpy/ctypeslib.py]</code> | 37.3 ms | 39.6 ms | -5.79% |</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/github-actions[bot]">@github-actions[bot]</a> on 2023-12-07 13:48</div>
            <div class="timeline-body">

<code>ruff-ecosystem</code> results
Linter (stable)
<p>‚úÖ ecosystem check detected no linter changes.</p>
Linter (preview)
<p>‚úÖ ecosystem check detected no linter changes.</p>
Formatter (stable)
<p>‚úÖ ecosystem check detected no format changes.</p>
Formatter (preview)
<p>‚úÖ ecosystem check detected no format changes.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/konstin">@konstin</a> on 2023-12-07 14:17</div>
            <div class="timeline-body"><p>Given the regressions, fwiw i&#x27;m also fine with different settings for <code>profiling</code> and <code>release</code></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-07 15:00</div>
            <div class="timeline-body"><p>OK, so I followed @charliermarsh&#x27;s suggestion to run a <code>hyperfine</code> benchmark on CPython. To do that, I created profiles for each configuration we want to test (basically <code>{fatlto, thinlto, nolto} x {cg=1, cg=16}</code>):</p>
<pre><code>[profile.fatcg1]
inherits = &quot;release&quot;
lto = &quot;fat&quot;
codegen-units = 1

[profile.fatcg16]
inherits = &quot;release&quot;
lto = &quot;fat&quot;
codegen-units = 16

[profile.thincg1]
inherits = &quot;release&quot;
lto = &quot;thin&quot;
codegen-units = 1

[profile.thincg16]
inherits = &quot;release&quot;
lto = &quot;thin&quot;
codegen-units = 16

[profile.noltocg1]
inherits = &quot;release&quot;
lto = false
codegen-units = 1

[profile.noltocg16]
inherits = &quot;release&quot;
lto = false
codegen-units = 16
</code></pre>
<p>Then I compiled a binary for each profile:</p>
<pre><code>cargo clean
mkdir -p target/release
for p in fatcg1 fatcg16 thincg1 thincg16 noltocg1 noltocg16; do
  cargo build --profile $p -p ruff_cli
  cp target/$p/ruff target/release/ruff-$p
done
</code></pre>
<p>And baked them off:</p>
<pre><code>hyperfine \
    --warmup 10 \
    --runs 100 \
    &quot;ruff-fatcg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e&quot; \
    &quot;ruff-fatcg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e&quot; \
    &quot;ruff-thincg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e&quot; \
    &quot;ruff-thincg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e&quot; \
    &quot;ruff-noltocg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e&quot; \
    &quot;ruff-noltocg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e&quot;
Benchmark 1: ruff-fatcg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
  Time (mean ¬± œÉ):     125.8 ms ¬±   3.6 ms    [User: 2001.5 ms, System: 137.6 ms]
  Range (min ‚Ä¶ max):   120.0 ms ‚Ä¶ 143.9 ms    100 runs

Benchmark 2: ruff-fatcg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
  Time (mean ¬± œÉ):     126.9 ms ¬±   4.3 ms    [User: 1991.5 ms, System: 142.3 ms]
  Range (min ‚Ä¶ max):   119.1 ms ‚Ä¶ 140.6 ms    100 runs

Benchmark 3: ruff-thincg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
  Time (mean ¬± œÉ):     126.0 ms ¬±   4.1 ms    [User: 1991.0 ms, System: 138.9 ms]
  Range (min ‚Ä¶ max):   119.1 ms ‚Ä¶ 137.8 ms    100 runs

Benchmark 4: ruff-thincg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
  Time (mean ¬± œÉ):     127.9 ms ¬±   4.1 ms    [User: 2016.1 ms, System: 133.5 ms]
  Range (min ‚Ä¶ max):   120.5 ms ‚Ä¶ 139.8 ms    100 runs

Benchmark 5: ruff-noltocg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
  Time (mean ¬± œÉ):     132.8 ms ¬±   3.7 ms    [User: 2135.2 ms, System: 126.4 ms]
  Range (min ‚Ä¶ max):   126.7 ms ‚Ä¶ 141.6 ms    100 runs

Benchmark 6: ruff-noltocg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
  Time (mean ¬± œÉ):     132.7 ms ¬±   4.4 ms    [User: 2115.1 ms, System: 133.7 ms]
  Range (min ‚Ä¶ max):   125.3 ms ‚Ä¶ 148.1 ms    100 runs

Summary
  ruff-fatcg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e ran
    1.00 ¬± 0.04 times faster than ruff-thincg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
    1.01 ¬± 0.04 times faster than ruff-fatcg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
    1.02 ¬± 0.04 times faster than ruff-thincg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
    1.05 ¬± 0.05 times faster than ruff-noltocg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
    1.06 ¬± 0.04 times faster than ruff-noltocg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e
</code></pre>
<p>As expected, <code>ruff-fatcg1</code> is the fastest, but <code>ruff-thincg1</code>, <code>ruff-fatcg16</code> and <code>ruff-thincg16</code> are all extremely close. (This PR has the <code>thincg16</code> configuration.)</p>
<p>So the microbenchmark regressions here do indeed look a little scary, but the more holistic/realistic benchmark looks okay to me?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-07 15:10</div>
            <div class="timeline-body"><blockquote>
<p>fwiw i&#x27;m also fine with different settings for <code>profiling</code> and <code>release</code></p>
</blockquote>
<p>In theory I&#x27;m fine with it too, but I do feel like it can be pretty tricky. What I&#x27;m thinking about is something like this:</p>
<ol>
<li>You submit a PR.</li>
<li>codspeed benchmarks run and show a small but measurable regression.</li>
<li>You run benchmarks and profile things with the <code>profiling</code> profile.</li>
</ol>
<p>In this case, you might be seeing something very different than what was benchmarked in the PR, and tracking down the regression could prove quite annoying. LTO can greatly impact function inlining. My suspicion is that, in most cases, if a regression exists with LTO enabled, then it probably also exists with LTO disabled (or in a different mode). But not necessarily.</p>
<p>With that said, yeah, if we find we can&#x27;t relax the LTO configuration then given the difference in compile times here, I&#x27;d probably accept the above as a downside I&#x27;d be willing to pay I think?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-12-07 15:13</div>
            <div class="timeline-body"><p>@BurntSushi - Would it be easy to re-run that comparison with <code>--select ALL</code>?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/dhruvmanila">@dhruvmanila</a> on 2023-12-07 15:14</div>
            <div class="timeline-body"><p>Just a note that we&#x27;re moving ahead on <a href="https://github.com/astral-sh/ruff/pull/8835">astral-sh/ruff#8835</a> with the regression. I&#x27;m not sure if this would affect the code generation problem in that PR anyhow but we should take the combined regression into account.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/dhruvmanila">@dhruvmanila</a> reviewed on 2023-12-07 15:17</div>
            <div class="timeline-body"></div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/dhruvmanila">@dhruvmanila</a> on <code>Cargo.toml</code>:120 on 2023-12-07 15:17</div>
            <div class="timeline-body"><p>We would also want to update all references of <code>release-debug</code> in the profiling section of <a href="https://github.com/astral-sh/ruff/blob/main/CONTRIBUTING.md#benchmarking-and-profiling"><code>CONTRIBUTING.md</code></a>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-07 15:47</div>
            <div class="timeline-body"><blockquote>
<p>@BurntSushi - Would it be easy to re-run that comparison with <code>--select ALL</code>?</p>
</blockquote>
<p>@charliermarsh Yeah! Here you go:</p>
<pre><code>hyperfine \
    --warmup 10 \
    --runs 100 \
    &quot;ruff-fatcg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-fatcg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-thincg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-thincg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-noltocg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-noltocg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL&quot;
Benchmark 1: ruff-fatcg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     414.5 ms ¬±   8.0 ms    [User: 5297.2 ms, System: 329.0 ms]
  Range (min ‚Ä¶ max):   397.6 ms ‚Ä¶ 453.5 ms    100 runs

Benchmark 2: ruff-fatcg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     413.8 ms ¬±   7.2 ms    [User: 5257.4 ms, System: 329.6 ms]
  Range (min ‚Ä¶ max):   399.5 ms ‚Ä¶ 436.6 ms    100 runs

Benchmark 3: ruff-thincg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     433.8 ms ¬±   7.5 ms    [User: 5352.2 ms, System: 325.2 ms]
  Range (min ‚Ä¶ max):   416.0 ms ‚Ä¶ 453.6 ms    100 runs

Benchmark 4: ruff-thincg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     435.4 ms ¬±   9.1 ms    [User: 5491.0 ms, System: 329.1 ms]
  Range (min ‚Ä¶ max):   412.2 ms ‚Ä¶ 461.6 ms    100 runs

Benchmark 5: ruff-noltocg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     470.0 ms ¬±  11.1 ms    [User: 5718.5 ms, System: 319.6 ms]
  Range (min ‚Ä¶ max):   446.5 ms ‚Ä¶ 498.2 ms    100 runs

Benchmark 6: ruff-noltocg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     472.9 ms ¬±  10.5 ms    [User: 5772.5 ms, System: 324.8 ms]
  Range (min ‚Ä¶ max):   451.5 ms ‚Ä¶ 501.0 ms    100 runs

Summary
  ruff-fatcg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL ran
    1.00 ¬± 0.03 times faster than ruff-fatcg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
    1.05 ¬± 0.03 times faster than ruff-thincg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
    1.05 ¬± 0.03 times faster than ruff-thincg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
    1.14 ¬± 0.03 times faster than ruff-noltocg1 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
    1.14 ¬± 0.03 times faster than ruff-noltocg16 ./crates/ruff_linter/resources/test/cpython/ --no-cache --silent -e --select ALL
</code></pre>
<p>So in this case, we end up with a 1.05x regression for both <code>thincg1</code> and <code>thincg16</code>. Interestingly, <code>fatcg16</code> seems about on par with <code>fatcg1</code>. I thought maybe we could get a free win by switching to <code>fatcg16</code>, but compile times are actually worse in that configuration. Using my test outlined in the initial comment in this PR, the re-build time is 92s (versus 77s for <code>fatcg1</code>). I speculate that the compile times are worse because the <code>codegen-units = 16</code> ends up creating more work for fat LTO.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-12-08 00:15</div>
            <div class="timeline-body"><p>Sorry to ask for more benchmarks, but it would be good to have some numbers on the formatter too:</p>
<pre><code>hyperfine ./target/release/ruff format ./checkouts/zulip
</code></pre>
<p>And you may want to benchmark another project than CPython (one that actually uses ruff like homeassistant or <code>zulip</code>) for <code>--select ALL</code> because CPython has so many violations that you manly profile the diagnostic printing and caching of a vast amount if diagnostics (atypical workload)</p>
<p>I&#x27;m a bit surprised that the lexer microbenchmarks are affected that much... Could it be that Rust directly inlines too much of the lexer into the benchmark, removing multiple function calls? I otherwise wouldn&#x27;t expect much change because the lexer code is mostly self contained in one crate (and called by the parser from the same crate).</p>
<p>It may be worth to compare the profiles between LTO1 and THIN16 to see if there are some obvious candidates where it makes sense to add an <code>#[inline]</code> attribute to preserve the cross-crate inlining that the linker did automatically with fat but doesn&#x27;t with thin. For example, adding inline to <code>Cursor::eat_while</code> is a 4% perf improvement on my machine.</p>
<p>It would be nice if we could specify the code units per crate. E.g. <code>ruff_python_ast</code> and parser change infrequently but a fast parser is important. Having <code>code_units=1</code> might be worth it for the parser without reducing your iteration speed much.</p>
<blockquote>
<p>In theory I&#x27;m fine with it too, but I do feel like it can be pretty tricky. What I&#x27;m thinking about is something like this:</p>
</blockquote>
<p>I think I&#x27;m otherwise okay with a 3-5% regression, if we have sufficient proof that it boosts our productivity significantly (Note: This won&#x27;t improve our CI times other than for the benchmarks run)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-13 19:09</div>
            <div class="timeline-body"><p>All righty, I re-ran <code>ruff</code> on the <code>zulip</code> repo with <code>--select ALL</code>:</p>
<pre><code>$ hyperfine \
    --warmup 10 \
    --runs 100 \
    &quot;ruff-fatcg1 ./ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-fatcg16 ./ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-thincg1 ./ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-thincg16 ./ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-noltocg1 ./ --no-cache --silent -e --select ALL&quot; \
    &quot;ruff-noltocg16 ./ --no-cache --silent -e --select ALL&quot;
Benchmark 1: ruff-fatcg1 ./ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     127.2 ms ¬±   5.8 ms    [User: 1567.6 ms, System: 114.5 ms]
  Range (min ‚Ä¶ max):   114.0 ms ‚Ä¶ 141.1 ms    100 runs

Benchmark 2: ruff-fatcg16 ./ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     128.7 ms ¬±   5.6 ms    [User: 1571.7 ms, System: 111.6 ms]
  Range (min ‚Ä¶ max):   115.9 ms ‚Ä¶ 141.8 ms    100 runs

Benchmark 3: ruff-thincg1 ./ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     131.6 ms ¬±   5.2 ms    [User: 1605.3 ms, System: 111.5 ms]
  Range (min ‚Ä¶ max):   121.1 ms ‚Ä¶ 145.8 ms    100 runs

Benchmark 4: ruff-thincg16 ./ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     133.8 ms ¬±   6.3 ms    [User: 1646.6 ms, System: 118.6 ms]
  Range (min ‚Ä¶ max):   121.3 ms ‚Ä¶ 151.4 ms    100 runs

Benchmark 5: ruff-noltocg1 ./ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     142.3 ms ¬±   6.4 ms    [User: 1733.6 ms, System: 116.4 ms]
  Range (min ‚Ä¶ max):   128.9 ms ‚Ä¶ 160.0 ms    100 runs

Benchmark 6: ruff-noltocg16 ./ --no-cache --silent -e --select ALL
  Time (mean ¬± œÉ):     143.6 ms ¬±   5.8 ms    [User: 1774.2 ms, System: 114.8 ms]
  Range (min ‚Ä¶ max):   130.7 ms ‚Ä¶ 159.8 ms    100 runs

Summary
  ruff-fatcg1 ./ --no-cache --silent -e --select ALL ran
    1.01 ¬± 0.06 times faster than ruff-fatcg16 ./ --no-cache --silent -e --select ALL
    1.03 ¬± 0.06 times faster than ruff-thincg1 ./ --no-cache --silent -e --select ALL
    1.05 ¬± 0.07 times faster than ruff-thincg16 ./ --no-cache --silent -e --select ALL
    1.12 ¬± 0.07 times faster than ruff-noltocg1 ./ --no-cache --silent -e --select ALL
    1.13 ¬± 0.07 times faster than ruff-noltocg16 ./ --no-cache --silent -e --select ALL
</code></pre>
<p>And I also checked <code>ruff format</code> on the Zulip repo too (being careful to reset the changes between each run, so that we actually test the time it takes to format the code):</p>
<pre><code>$ hyperfine \
    --warmup 10 \
    --prepare &#x27;git reset --hard main&#x27; \
    --cleanup &#x27;git reset --hard main&#x27; \
    &#x27;ruff-fatcg1 format ./&#x27; \
    &#x27;ruff-fatcg16 format ./&#x27; \
    &#x27;ruff-thincg1 format ./&#x27; \
    &#x27;ruff-thincg16 format ./&#x27; \
    &#x27;ruff-noltocg1 format ./&#x27; \
    &#x27;ruff-noltocg16 format ./&#x27;
Benchmark 1: ruff-fatcg1 format ./
  Time (mean ¬± œÉ):      47.8 ms ¬±   2.4 ms    [User: 126.9 ms, System: 57.7 ms]
  Range (min ‚Ä¶ max):    41.9 ms ‚Ä¶  51.5 ms    41 runs

Benchmark 2: ruff-fatcg16 format ./
  Time (mean ¬± œÉ):      49.8 ms ¬±   2.0 ms    [User: 131.2 ms, System: 57.4 ms]
  Range (min ‚Ä¶ max):    45.1 ms ‚Ä¶  53.9 ms    43 runs

Benchmark 3: ruff-thincg1 format ./
  Time (mean ¬± œÉ):      49.2 ms ¬±   1.8 ms    [User: 132.1 ms, System: 58.9 ms]
  Range (min ‚Ä¶ max):    44.9 ms ‚Ä¶  53.5 ms    43 runs

Benchmark 4: ruff-thincg16 format ./
  Time (mean ¬± œÉ):      50.0 ms ¬±   2.1 ms    [User: 131.6 ms, System: 59.4 ms]
  Range (min ‚Ä¶ max):    45.6 ms ‚Ä¶  53.9 ms    43 runs

Benchmark 5: ruff-noltocg1 format ./
  Time (mean ¬± œÉ):      52.6 ms ¬±   2.0 ms    [User: 148.2 ms, System: 56.9 ms]
  Range (min ‚Ä¶ max):    47.2 ms ‚Ä¶  56.6 ms    42 runs

Benchmark 6: ruff-noltocg16 format ./
  Time (mean ¬± œÉ):      52.1 ms ¬±   2.0 ms    [User: 145.1 ms, System: 57.3 ms]
  Range (min ‚Ä¶ max):    47.3 ms ‚Ä¶  58.0 ms    40 runs

Summary
  ruff-fatcg1 format ./ ran
    1.03 ¬± 0.06 times faster than ruff-thincg1 format ./
    1.04 ¬± 0.07 times faster than ruff-fatcg16 format ./
    1.05 ¬± 0.07 times faster than ruff-thincg16 format ./
    1.09 ¬± 0.07 times faster than ruff-noltocg16 format ./
    1.10 ¬± 0.07 times faster than ruff-noltocg1 format ./
</code></pre>
<p>It looks like the relative difference here is about the same as with linting.</p>
<p>My target or hope here is to switch to <code>thincg16</code> (the default Cargo <code>release</code> profile), since I think it strikes a good balance and shaves an entire minute off our current 77s release build times. I do kind of feel like a 1.05x regression is acceptable given the enormous savings in build time. But let&#x27;s do some more digging first. I think @MichaReiser has a good idea here that we might actually be able to recuperate our losses with some well-placed <code>inline</code> annotations.</p>
<p>I was indeed able to add an <code>#[inline]</code> annotation to <code>Cursor::eat_while</code> and that seems to help the lexer microbenchmark a little bit.</p>
<p>I looked for other opportunities like that but came up short. I did see some functions inlined with <code>fatcg1</code> but not <code>thincg16</code>. For example, <code>alloc::raw_vec::RawVec&lt;T,A&gt;::allocate_in</code>. But I don&#x27;t know how to force a function from inside of <code>std</code> to get inlined unfortunately. And nothing else really jumps out at me.</p>
<blockquote>
<p>I think I&#x27;m otherwise okay with a 3-5% regression, if we have sufficient proof that it boosts our productivity significantly (Note: This won&#x27;t improve our CI times other than for the benchmarks run)</p>
</blockquote>
<p>Aye. It&#x27;s hard to provide proof, but I do actually feel somewhat strongly that this kind of improvement in iteration time will eventually pay for itself. Ideally, we could get it even faster than 20 seconds, but I think disabling thin LTO is probably a bridge too far. Getting better iteration times after this I think will involve figuring out how to make <code>ruff</code> compile faster in other ways.</p>
<blockquote>
<p>It would be nice if we could specify the code units per crate. E.g. ruff_python_ast and parser change infrequently but a fast parser is important. Having code_units=1 might be worth it for the parser without reducing your iteration speed much.</p>
</blockquote>
<p>Oh hey! Actually this can be done! I had assumed <code>codegen-units</code> was a global setting (like <code>lto</code> is), but it&#x27;s not. I added these lines to <code>Cargo.toml</code>:</p>
<pre><code># Some crates don&#x27;t change as much but benefit more from
# more expensive optimization passes, so we selectively
# decrease codegen-units in some cases.
[profile.release.package.ruff_python_parser]
codegen-units = 1
[profile.release.package.ruff_python_ast]
codegen-units = 1
</code></pre>
<p>And at least locally, this seems to shrink the regression for the lexer microbenchmark substantially. I just pushed up that change here, so let&#x27;s see how it does with codspeed.</p>
<p>I also re-ran the hyperfine benchmarks above with this new configuration, but it doesn&#x27;t detect any difference between it and <code>thincg16</code>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-13 19:22</div>
            <div class="timeline-body"><p>The micro-benchmark regressions are now quite a bit smaller: <a href="https://github.com/astral-sh/ruff/pull/9031">astral-sh/ruff#9031</a>/checks?check_run_id=19612967302</p>
<p>Summary: This PR reverts the release profile to the default configuration, but does set <code>codegen-units = 1</code> for <code>ruff_python_parser</code> and <code>ruff_python_ast</code>. Overall, this appears to result in about a 1.05x regression on real world workloads, in addition to several micro-benchmark regressions in the 4-6% range. In exchange, iterative builds drop from about 77s to 17s (on my machine), which is IMO very substantial. The key benefit I think this has is that it improves iteration times. I think that faster iteration times <em>encourage</em> more iteration and have less of a chance of disrupting flow states. So overall, I feel like this change is worth making despite the small regression in perf.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/MichaReiser">@MichaReiser</a> approved on 2023-12-14 03:04</div>
            <div class="timeline-body"><p>Thanks for pushing for this and the detailed profiling. This seems a worthwhile trade off to me which most users shouldn&#x27;t even notice. Long compile times have been a real concern for many contributors and can be very noticable if you work on a somewhat older computer.</p>
<p>I&#x27;m all in for merging as is. We can reconsider using lto=fat in the future if we managed to improve our crate structure, with future rust versions or when having better devtooling (earthly with incremental builds?)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/T-256">@T-256</a> on 2023-12-14 07:28</div>
            <div class="timeline-body"><p>Is this regression also effects on final published releases?
If yes, then imo we need consider use separated release profile. (e.g. optimized or publish)
As final user, the speed important for us, as far as publishing happens on few weeks so I don&#x27;t think compile-time matters.</p>
<p>I mean <code>profile.release</code> be compile fast but slower runtime, and <code>profile.publish</code> be most optimized on runtime but compile slower.
btw this PR seems fine to me, we can track new profile on new PR :)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-14 13:49</div>
            <div class="timeline-body"><p>@T-256 I discussed that point <a href="https://github.com/astral-sh/ruff/pull/9031#issuecomment-1845520046">here</a>. The core issue is that if the thing you&#x27;re benchmarking and the thing you release are different, then you risk measuring and tuning the wrong thing. I don&#x27;t mean to say that this is an iron-clad argument against something like having a <code>publish</code> profile that is only used to cut a release, but rather, it feels like a cost to me. I&#x27;m not strongly opposed to a setup like that. It&#x27;s also very hard to say how big of a cost it is.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/charliermarsh">@charliermarsh</a> reviewed on 2023-12-14 18:39</div>
            <div class="timeline-body"><p>This is very good and thorough work. I&#x27;m <em>slightly</em> less convinced that this change is worth the tradeoffs (as compared to @MichaReiser) since it only helps with release builds, and we do release builds locally so infrequently compare to how often Ruff is run in the wild by users. But the size of the regression is not so great that I would block it if you two are in favor.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-12-15 01:02</div>
            <div class="timeline-body"><blockquote>
<p>This is very good and thorough work. I&#x27;m <em>slightly</em> less convinced that this change is worth the tradeoffs (as compared to @MichaReiser) since it only helps with release builds, and we do release builds locally so infrequently compare to how often Ruff is run in the wild by users. But the size of the regression is not so great that I would block it if you two are in favor.</p>
</blockquote>
<p>I agree that must of us don&#x27;t to frequent release builds (at least for publishing), but they&#x27;re ferquently needed when doing performance work where building the benchmarks and running our benchmarks takes a significant amount of time (to be fair, running them probably takes longer than building). Having a faster feedback-cycle there would certainly help.</p>
<p>An alternative is to use different profiles between releasing and profiling but it comes with the downside that what we see in profiles might not match what we see in production. But maybe that&#x27;s something we have to accept anyway if we e.g. consider using data driven optimiations (run ruff with a typical workload and then feed that information into the optimizer, similar to what Rust does).</p>
<p>Yet another alternative is to have a light and &quot;full&quot; profiling profile where you can do both a full and light profiling, compare if you see the same outliers, and then use the &quot;light&quot; profilie to optimize the code until the outlier is gone.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/T-256">@T-256</a> on 2023-12-15 01:37</div>
            <div class="timeline-body"><blockquote>
<p>An alternative is to use different profiles between releasing and profiling but it comes with the downside that what we see in profiles might not match what we see in production</p>
</blockquote>
<p>Production builds intended to be always faster at run-time (?) We can document build profiles: &quot;For final production builds, we are using optimized compilation flags to have most optimized run-time.&quot;</p>
<p>For measuring, we can always rely on default release profile, since always comparing to same builds then I think we&#x27;d have correct comparation for new changes.</p>
<p>When need to compare against foreign tools (e.g. promoting Ruff against tools/competitors) we can use production build measured numbers.</p>
<p>For now, I think we can merge current PR. production builds needs separated discussion.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-15 13:19</div>
            <div class="timeline-body"><p>I do find myself more sympathetic to the idea of having different profile settings for typical benchmarking and the final release build. But, let&#x27;s see how far we can get with having them use the same configuration. We can always revisit this and at least change the <code>release</code> profile back to fat LTO and <code>codegen-units = 1</code> while simultaneously changing the <code>profiling</code> profile to thin LTO and <code>codegen-units = 16</code>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Merged by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-15 13:19</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2023-12-15 13:19</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Branch deleted on 2023-12-15 13:19</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 18:59:51 UTC
    </footer>
</body>
</html>
