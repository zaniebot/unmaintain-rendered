```yaml
number: 5878
title: "Fix `SimpleTokenizer`'s backward lexing of `# `"
type: pull_request
state: merged
author: MichaReiser
labels: []
assignees: []
merged: true
base: main
head: fix-simple-tokenizer-hash-in-string
created_at: 2023-07-19T09:53:10Z
updated_at: 2023-07-20T09:54:19Z
url: https://github.com/astral-sh/ruff/pull/5878
synced_at: 2026-01-12T03:30:22Z
```

# Fix `SimpleTokenizer`'s backward lexing of `# `

---

_Pull request opened by @MichaReiser on 2023-07-19 09:53_

<!--
Thank you for contributing to Ruff! To help us out with reviewing, please consider the following:

- Does this pull request include a summary of the change? (See below.)
- Does this pull request include a descriptive title?
- Does this pull request include references to any relevant issues?
-->

## Summary

This PR fixes the `SimpleTokenizer`s backward lexing to handle `#` tokens inside multiline strings correctly. 

```python
'a multiline\
string containing a # token'

'''a multiline
string containing a # token'''
```

For both these cases, `#` is not the start of a comment. Instead, it's just a regular hash inside of the multiline string. 

However, the following is a comment

```python
'a string' # comment 'with quotes'
```

Unfortunately, there's no easy way to detect whether `# token'` is a comment or part of a string when lexing backward because it's impossible to tell if two quotes form a pair when looking backward:

```python
# It looks like the ''' in the comment match the opening ''' on the first line
'''
a # comment '''

# But let's zoom out... are they matching pairs?

a + '''
some multiline text
'''
a # comment '''
```

The way this is solved in this PR is that we lex out all strings and comments if we encounter any `#` that is followed by a quote. This should be rare because:

* Comments are rare; comments containing quotes are even rarer. 
* Strings are common, but they rarely contain hashes

Also, backward lexing is strongly discouraged, and you should prefer `up_to_no_back_comment`, which skips over the expensive comments handling until it reaches a new line (and the whole purpose of the lexer is only to lex very few tokens). 

For most cases, lexing the "whole" source should be fast because we only lex over a very limited range. This is not true for `up_to`, which we used in `lines_before`. That's why I went ahead and rewrote `lines_before` and `lines_after` to **not** use the `SimpleTokenizer` anymore. They don't care about trivia, meaning we can get away with a trivial `Cursor` based implementation.



## Test Plan

I added new snapshot tests
<!-- How was it tested? -->


---

_Comment by @MichaReiser on 2023-07-19 09:53_

Current dependencies on/for this PR:
* main
  * **PR #5878** <a href="https://app.graphite.dev/github/pr/astral-sh/ruff/5878" target="_blank"><img src="https://static.graphite.dev/graphite-32x32.png" alt="Graphite" width="10px" height="10px"/></a>  ðŸ‘ˆ

This comment was auto-generated by [Graphite](https://app.graphite.dev/github/pr/astral-sh/ruff/5878?utm_source=stack-comment).

---

_Review comment by @MichaReiser on `crates/ruff_python_whitespace/src/tokenizer.rs`:27 on 2023-07-19 10:07_

I removed this method because I intentionally want to make it hard to do backward lexing. 

---

_@MichaReiser reviewed on 2023-07-19 10:08_

---

_Review requested from @charliermarsh by @MichaReiser on 2023-07-19 10:12_

---

_Review requested from @konstin by @MichaReiser on 2023-07-19 10:12_

---

_@MichaReiser reviewed on 2023-07-19 10:16_

---

_Review comment by @MichaReiser on `crates/ruff_python_whitespace/src/tokenizer.rs`:539 on 2023-07-19 10:16_

The case where we have multiple hashes could probably be sped up to avoid finding the unterminated string repeatedly because I think this is extremely rare. 

---

_Comment by @github-actions[bot] on 2023-07-19 10:50_

## PR Check Results
### Benchmark
#### Linux
```
group                                      main                                    pr
-----                                      ----                                    --
formatter/large/dataset.py                 1.01     10.0Â±0.44ms     4.1 MB/sec     1.00      9.8Â±0.58ms     4.1 MB/sec
formatter/numpy/ctypeslib.py               1.04      2.0Â±0.12ms     8.1 MB/sec     1.00  1959.8Â±106.02Âµs     8.5 MB/sec
formatter/numpy/globals.py                 1.04   231.7Â±16.28Âµs    12.7 MB/sec     1.00   221.9Â±13.97Âµs    13.3 MB/sec
formatter/pydantic/types.py                1.03      4.4Â±0.28ms     5.9 MB/sec     1.00      4.2Â±0.22ms     6.0 MB/sec
linter/all-rules/large/dataset.py          1.01     14.8Â±0.46ms     2.8 MB/sec     1.00     14.7Â±0.57ms     2.8 MB/sec
linter/all-rules/numpy/ctypeslib.py        1.00      3.7Â±0.15ms     4.6 MB/sec     1.06      3.9Â±0.22ms     4.3 MB/sec
linter/all-rules/numpy/globals.py          1.00   484.1Â±24.98Âµs     6.1 MB/sec     1.00   486.4Â±27.61Âµs     6.1 MB/sec
linter/all-rules/pydantic/types.py         1.00      6.6Â±0.33ms     3.8 MB/sec     1.03      6.8Â±0.29ms     3.7 MB/sec
linter/default-rules/large/dataset.py      1.01      7.4Â±0.31ms     5.5 MB/sec     1.00      7.3Â±0.30ms     5.6 MB/sec
linter/default-rules/numpy/ctypeslib.py    1.00  1580.4Â±110.05Âµs    10.5 MB/sec    1.00  1577.0Â±96.50Âµs    10.6 MB/sec
linter/default-rules/numpy/globals.py      1.07   198.0Â±12.38Âµs    14.9 MB/sec     1.00   184.7Â±11.22Âµs    16.0 MB/sec
linter/default-rules/pydantic/types.py     1.03      3.5Â±0.17ms     7.4 MB/sec     1.00      3.4Â±0.21ms     7.6 MB/sec
```

#### Windows
```
group                                      main                                   pr
-----                                      ----                                   --
formatter/large/dataset.py                 1.00     11.0Â±0.18ms     3.7 MB/sec    1.01     11.1Â±0.14ms     3.7 MB/sec
formatter/numpy/ctypeslib.py               1.00      2.2Â±0.03ms     7.7 MB/sec    1.00      2.2Â±0.03ms     7.7 MB/sec
formatter/numpy/globals.py                 1.00    244.8Â±5.15Âµs    12.1 MB/sec    1.00    244.3Â±8.24Âµs    12.1 MB/sec
formatter/pydantic/types.py                1.00      4.8Â±0.07ms     5.3 MB/sec    1.00      4.8Â±0.07ms     5.3 MB/sec
linter/all-rules/large/dataset.py          1.01     15.3Â±0.12ms     2.7 MB/sec    1.00     15.2Â±0.18ms     2.7 MB/sec
linter/all-rules/numpy/ctypeslib.py        1.00      4.0Â±0.06ms     4.1 MB/sec    1.00      4.1Â±0.07ms     4.1 MB/sec
linter/all-rules/numpy/globals.py          1.03    499.3Â±8.69Âµs     5.9 MB/sec    1.00   485.9Â±13.32Âµs     6.1 MB/sec
linter/all-rules/pydantic/types.py         1.00      7.0Â±0.16ms     3.6 MB/sec    1.00      7.0Â±0.26ms     3.6 MB/sec
linter/default-rules/large/dataset.py      1.01      8.1Â±0.12ms     5.0 MB/sec    1.00      8.0Â±0.14ms     5.1 MB/sec
linter/default-rules/numpy/ctypeslib.py    1.00  1671.6Â±15.32Âµs    10.0 MB/sec    1.00  1665.6Â±25.19Âµs    10.0 MB/sec
linter/default-rules/numpy/globals.py      1.00    189.6Â±3.33Âµs    15.6 MB/sec    1.01    191.7Â±3.73Âµs    15.4 MB/sec
linter/default-rules/pydantic/types.py     1.00      3.6Â±0.04ms     7.1 MB/sec    1.00      3.6Â±0.03ms     7.1 MB/sec
```
<!-- thollander/actions-comment-pull-request "PR Check Results" -->

---

_@konstin approved on 2023-07-19 12:47_

Can this handle escaped quotes?

This feels like a lot of effort given that we should already know comment positions and the caller should know expression ranges that would avoid most of the effort

---

_Renamed from "Fix SimpleTokenizer hash in string handling" to "Fix SimpleTokenizer backwards lexing of # " by @MichaReiser on 2023-07-19 12:49_

---

_Renamed from "Fix SimpleTokenizer backwards lexing of # " to "Fix `SimpleTokenizer`'s backward lexing of `# `" by @MichaReiser on 2023-07-19 14:45_

---

_Comment by @MichaReiser on 2023-07-19 14:49_

> Can this handle escaped quotes?

Let me add a test, just to be sure 


> This feels like a lot of effort given that we should already know comment positions and the caller should know expression ranges that would avoid most of the effort

That's fair. I could see us split out the `BackwardTokenizer` and require you to pass in the `CommentRanges`. 


---

_Merged by @MichaReiser on 2023-07-20 09:54_

---

_Closed by @MichaReiser on 2023-07-20 09:54_

---

_Branch deleted on 2023-07-20 09:54_

---
