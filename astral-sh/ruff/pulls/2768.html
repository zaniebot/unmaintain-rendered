<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>perf: Use custom allocator - astral-sh/ruff #2768</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>perf: Use custom allocator</h1>

    <div class="meta">
        <span class="state-icon state-merged"></span>
        <a href="https://github.com/astral-sh/ruff/pull/2768">#2768</a>
        opened by <a href="https://github.com/MichaReiser">@MichaReiser</a>
        on 2023-02-11 16:21
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/MichaReiser">@MichaReiser</a></div>
            <div class="timeline-body"><p>This PR replaces the system allocator with a custom allocator to improve performance:</p>
<ul>
<li>Windows: mimalloc</li>
<li>Unix: tikv-jemallocator</li>
</ul>
Performance:
<ul>
<li>Linux<ul>
<li><code>cpython --no-cache</code>: 208.8ms -&gt; 190.5ms</li>
<li><code>cpython</code>: 32.8ms -&gt; 31ms</li>
</ul>
</li>
<li>Mac:<ul>
<li><code>cpython --no-cache</code>: 436.3ms -&gt; 380ms</li>
<li><code>cpython</code>: 40.9ms -&gt; 39.6ms</li>
</ul>
</li>
<li>Windows:<ul>
<li><code>cpython --no-cache</code>: 367ms -&gt; 268ms</li>
<li><code>cpython</code>: 92.5ms -&gt; 92.3ms</li>
</ul>
</li>
</ul>
Size
<ul>
<li>Linux: +5MB from 13MB -&gt; 18MB (I need to double check this)</li>
<li>Mac: +0.7MB from 8.3MB-&gt; 9MB</li>
<li>Windows: -0.16MB from 8.29MB -&gt; 8.13MB (that&#x27;s unexpected)</li>
</ul>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-02-11 17:07</div>
            <div class="timeline-body"><p>Nice! Are there any costs or risks associated with this apart from the increased binary size? Have you used custom allocators in previous projects? I&#x27;ve seen them mentioned in <a href="https://nnethercote.github.io/perf-book/heap-allocations.html#using-an-alternative-allocator">Nethercote&#x27;s book</a> but never tried them :)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-02-11 17:26</div>
            <div class="timeline-body"><p>We use the same setup at Rome and haven&#x27;t had any issues (but it greatly improved performance) [<a href="https://github.com/rome/tools/pull/3237">PR1</a>, <a href="https://github.com/rome/tools/pull/2900">PR2</a>]</p>
<p>The book you mention summarises it well</p>
<blockquote>
<p>The exact effect will depend on the individual program and the alternative allocator chosen, but large improvements in speed and very large reductions in memory usage have been seen in practice. The effect will also vary across platforms, because each platform’s system allocator has its own strengths and weaknesses. The use of an alternative allocator can also affect binary size.</p>
</blockquote>
<p>We could try to measure peak memory usage and compare it between allocators.</p>
<p>The main risk I see is that it introduces an additional surface that needs maintaining. We need to update the allocator when it has a known security issue and it may be more likely that a custom allocator has bugs which I would expect less from the system allocators.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-02-11 17:40</div>
            <div class="timeline-body"><p>(Thumbs-up from me.)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-02-11 17:45</div>
            <div class="timeline-body"><p>I&#x27;m undecided whether the performance improvement on Linux warrants such a significant binary size increase.</p>
<p>But, using jemalloc on all unix platforms has a harmonizing effect, reducing the differences between the two platforms.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Marked ready for review by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-02-11 17:45</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-02-11 17:53</div>
            <div class="timeline-body"><p>The build failure looks unrelated to these changes.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Merged by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-02-11 18:26</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-02-11 18:26</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/andersk">@andersk</a> on 2023-02-11 20:30</div>
            <div class="timeline-body"><p>Much of the size difference on Linux is due to debug symbols. (NixOS 23.05 on Ryzen 7 1800X, averages of 100 runs.)</p>
<p>allocator | size | stripped | cpython time | cpython RSS memory
--- | --- | --- | --- | ---
system | 13.01 MiB | 9.32 MiB | 407 ms | 131 MiB
jemalloc | 18.25 MiB | 9.92 MiB | 378 ms | 175 MiB
mimalloc | 13.04 MiB | 9.37 MiB | 454 ms | 202 MiB
mimalloc (no <code>secure</code>) | 13.03 MiB | 9.36 MiB | 401 ms | 197 MiB</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/andersk">@andersk</a> on 2023-02-11 20:38</div>
            <div class="timeline-body"><p>The memory use change is a bit concerning though—with Ruff already being as blindingly fast as it is, is an 8% speed increase really worth a 34% memory use increase?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-02-11 20:51</div>
            <div class="timeline-body"><p>Ahhh that&#x27;s really interesting, thank you for running + tabulating. And indeed that&#x27;s a bit concerning. I was going to say that perhaps it makes sense to ship on Windows but not elsewhere given that the speed increase in the PR summary is much more significant, but presumedly (?) that would result in a similar memory use increase as the <code>mimalloc</code> benchmarks above.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2023-02-11 22:39</div>
            <div class="timeline-body"><p>@MichaReiser - Should we consider putting this behind a flag while we continue to discuss, based on the above?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-02-12 07:40</div>
            <div class="timeline-body"><p>@andersk, what command did you use to get the peak memory usage?</p>
<p>I used mac&#x27;s instruments to get some numbers before and after.</p>
<table>
<tr>
	<td>system allocator
	<td>jemalloc
<tr>
	<td><img alt="Memory usage with system allocator" src="https://user-images.githubusercontent.com/1203881/218298010-0ed86279-b2d4-42a3-9876-2d8bb36a84b8.png"> 
	<td><img alt="Memory usage with jemalloc allocator" src="https://user-images.githubusercontent.com/1203881/218298026-244b4f99-ae8f-4107-97d3-32f804bb66db.png">
<tr>
	<td>Peak: 42MB
	<td>Peak: 145MB
</table>

<p>This looks very concerning but these measurements aren&#x27;t necessarily fair. My understanding of the numbers shown by Instruments is that it shows <code>total(malloc) - total(free)</code> over time. This is useful for understanding the allocations in your program but only has limited meaning in assessing how much physical memory your application uses because it doesn&#x27;t account for heap fragmentation and the fact that the OS allocates the memory in pages (~4Kib). Meaning, you can have a situation where the OS isn&#x27;t able to free a single page because your application has freed all allocations except one on every page.</p>
<p>Using <code>jemalloc</code> gives you an entirely different graph because <code>jemalloc</code> handles the memory pages manually. It is an abstraction on top of <code>malloc</code> and <code>free</code>. That&#x27;s why the graph only shows a few but larger allocations. Now, why does <code>jemalloc</code> not return memory? One possibility is that the heap is too fragmented to release a single page or <code>ruff</code> terminates before <code>jemalloc</code> runs its memory release routine.</p>
<blockquote>
<p>This was a new notion to me. Surely a good piece of code, which behaves itself and puts away its toys when it&#x27;s done, will keep its resource usage to a minimum? As it turns out, that isn&#x27;t exactly the case. It really comes down to the behavior of the allocator, and most allocators won&#x27;t necessarily return memory right away to the OS. Jemalloc has a few ways this can be tuned, but by default it uses a time-based strategy for returning memory back to the OS. Memory allocations are serviced first by memory already held by the process that is free for reuse, and then failing that, new memory is acquired from the OS. Given enough time, should a chunk of memory not have anything assigned to it, it will be returned (and if returning resources quickly is important to you, you can configure jemalloc to return the memory immediately).
<a href="https://engineering.linkedin.com/blog/2021/taming-memory-fragmentation-in-venice-with-jemalloc">source</a></p>
</blockquote>
<p>I recommend investing time to find a long-running workload and tooling that measures the reserved OS memory if we&#x27;re concerned about peak memory consumption. I myself aren&#x27;t too concerned, considering that 150MB for lining the whole cpython code seems reasonable.</p>
<blockquote>
<p>@MichaReiser - Should we consider putting this behind a flag while we continue to discuss, based on the above?</p>
</blockquote>
<p>I personally would probably just revert the PR and reopen it. It&#x27;s not a lot of code.</p>
<p>For JS tooling it is very common to use custom allocators:</p>
<ul>
<li><a href="https://github.com/swc-project/swc/pull/4791">SWC</a></li>
<li><a href="https://github.com/parcel-bundler/lightningcss/commit/18adb3be5215c74b5dfa9a495c0a11e959e0d361">lightning_css</a></li>
<li><a href="https://github.com/rome/tools/pull/2900">Rome</a></li>
<li><a href="https://github.com/Boshen/oxc/pull/12">Oxc</a></li>
</ul>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/andersk">@andersk</a> on 2023-02-12 08:04</div>
            <div class="timeline-body"><p>The <code>time(1)</code> command on Linux gives you memory information, but you have to make sure to ask for the command and not the shell builtin:</p>
<pre><code>$ time ruff -n ../cpython
…
real    0m0.710s
user    0m5.472s
sys     0m0.146s

$ command time ruff -n ../cpython
…
5.39user 0.15system 0:00.79elapsed 695%CPU (0avgtext+0avgdata 112264maxresident)k
0inputs+0outputs (0major+28331minor)pagefaults 0swaps

$ \time ruff -n ../cpython
…
5.47user 0.15system 0:00.71elapsed 786%CPU (0avgtext+0avgdata 111580maxresident)k
0inputs+0outputs (0major+28573minor)pagefaults 0swaps
</code></pre>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-02-15 11:36</div>
            <div class="timeline-body"><p>I spent some more time collecting metrics around memory usage but are having troubles interpreting the numbers.</p>
System Allocator (commit 6d1adc85fcbce5c9f87c04e59251a78c5eeda557)
<code>/usr/bin/time</code>
<p><code>Maximum resident set size (kbytes): 178368 -&gt; ~174MB</code></p>
Details

<pre><code>Command exited with non-zero status 1
	Command being timed: &quot;./target/release/ruff ./crates/ruff/resources/test/cpython/ --no-cache&quot;
	User time (seconds): 3.36
	System time (seconds): 0.23
	Percent of CPU this job got: 1179%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.30
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 178368
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 8
	Minor (reclaiming a frame) page faults: 44799
	Voluntary context switches: 13397
	Involuntary context switches: 3917
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
</code></pre>


Valgrind
<p>Total heap memory (mapped pages): 1,681,956,864 -&gt; 1604MB <a href="https://gist.github.com/MichaReiser/a6e51e0f1f17791bcf7984c364d24271">Full output</a></p>
Jemalloc (ca49b00e55973a1daa577c4ccb34dd5c8a5311fb)
<code>/usr/bin/time</code>
<p><code>Maximum resident set size (kbytes): 346904 -&gt; 338MB</code></p>

	Details

<pre><code>Command exited with non-zero status 1
	Command being timed: &quot;./target/release/ruff ./crates/ruff/resources/test/cpython/ --no-cache&quot;
	User time (seconds): 3.40
	System time (seconds): 0.12
	Percent of CPU this job got: 1161%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.30
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 346904
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 17
	Minor (reclaiming a frame) page faults: 22861
	Voluntary context switches: 835
	Involuntary context switches: 4940
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
</code></pre>


Valgrind
<p>Total mapped pages: 1917MB  <a href="https://gist.github.com/MichaReiser/2339206d6e44d06c420a8a349b8dba9b">Full output</a></p>
Summary
<p><code>/usr/bin/time</code> suggests that memory usage almost doubled, whereas valgridn only shows a 20% increase. One thing I haven&#x27;t tried yet is to force trigger a jemalloc release of unused memory.</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 18:52:11 UTC
    </footer>
</body>
</html>
