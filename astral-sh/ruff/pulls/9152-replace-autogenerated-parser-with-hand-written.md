```yaml
number: 9152
title: Replace autogenerated parser with hand-written parser
type: pull_request
state: closed
author: LaBatata101
labels:
  - performance
  - parser
assignees: []
base: main
head: new-parser
created_at: 2023-12-15T21:59:56Z
updated_at: 2024-02-19T04:14:46Z
url: https://github.com/astral-sh/ruff/pull/9152
synced_at: 2026-01-10T22:57:09Z
```

# Replace autogenerated parser with hand-written parser

---

_Pull request opened by @LaBatata101 on 2023-12-15 21:59_

This PR replaces the autogenerated lalrpop parser in favor of a handwritten parser. Beyond the performance improvements, one of the main differences between the handwritten parser and the autogenerated parser is, the new parser is error persistent. That is, the parser can create an AST even if the source code contains invalid syntax. This also means that, the parser now collects all syntax errors found in the source code, while the old parser would stop parsing after finding the first invalid syntax. 

The new parser also enables us to create more helpful error messages, for example, in the old parser the following code would generate this error message:
```python
yield from *a
```
Resulting error message "invalid syntax. Got unexpected token '*' ", in the new parser the error message is `starred expression "*a` is not allowed in a `yield from` statement"

The new parser still preserves the old behavior of returning only the first syntax error it encounters. The Python syntax supported by the parser is of the version 3.12.

## Summary of Changes

### Changes in the AST
- Change `Identifier` node to use `SmolStr` instead of `String`
- Add `Invalid` node to `Expr`, `Pattern` and `FStringElement`

### Changes to error types
 - Refactor `ParseError` and `LexicalError` to use `TextRange` for the error location instead of `TextSize`.
 - Add `FStringError` to `ParseError`, the lexer used to check if the `{` in the f-string was unclosed, now this check happens in the parser.
 - Add new error in the lexer to check if a Unicode escape sequence has a `{` and `}`
- The following `LexicalErrorType` were moved to `ParseErrorType`:
  - `PositionalArgumentError`
  - `UnpackedArgumentError`
  - `DuplicateArgumentError`
  - `DuplicateKeywordArgumentError`
  - `AssignmentError`

A couple of errors checks that are found during the parse phase were being checked in the lexing phase, now these error checks are in the parser code. For example, the lexer used to check if the `(`, `[` or `{` had its closing counterpart.

### Linter
~~The changes made in the linter code are mostly due to the type of `Identifier` changing  from `String` to `SmolStr`.~~

### Formatter
The changes made in the formatter code are to handle the new `Invalid` nodes, the way I chose to handle these nodes in the fromatter was by just displaying the text as it was written in the source code.

### Misc
The `StringType` is a helper type used when parsing string/f-string literals. Since the parser now handles invalid syntax, we need a way to represent an invalid string/f-string when parsing string/f-string literals. So we add the `Invalid` type to deal with that.

Here's how the parser handles invalid strings:

For a single invalid string literal, e.g., a string with invalid escape sequence `"a \xxx"`, the resulting AST node for that will be an `Expr::Invalid`. The current implementation for string literal parsing has one limitation when it comes to implicit  concatenated strings. We can't know yet which specific string within the implicit concatenated strings are invalid. That is, the resulting AST node won't have an `Invalid` node for the invalid string literal.

Fixes #8914

## Test Plan
Tested with `cargo test --lib`. 
The previous parser tests were moved to the `ruff_python_parser/src/parser/tests`. 
I also added more tests in `ruff_python_parser/src/parser/tests/parser.rs`

---

_Comment by @codspeed-hq[bot] on 2023-12-15 22:07_

## [CodSpeed Performance Report](https://codspeed.io/astral-sh/ruff/branches/LaBatata101:new-parser)

### Merging #9152 will **degrade performances by 6.04%**

<sub>Comparing <code>LaBatata101:new-parser</code> (f8e577d) with <code>main</code> (fe79798)</sub>



### Summary

`âŒ 3` regressions
`âœ… 27` untouched benchmarks



> :warning: _Please fix the performance issues or [acknowledge them on CodSpeed](https://codspeed.io/astral-sh/ruff/branches/LaBatata101:new-parser)._

### Benchmarks breakdown

|     | Benchmark | `main` | `LaBatata101:new-parser` | Change |
| --- | --------- | ----------------------- | ------------------- | ------ |
| âŒ | `parser[large/dataset.py]` | 63.6 ms | 67.5 ms | -5.81% |
| âŒ | `parser[numpy/ctypeslib.py]` | 10.8 ms | 11.5 ms | -6.04% |
| âŒ | `parser[unicode/pypinyin.py]` | 3.8 ms | 3.9 ms | -4.7% |


---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/functions.rs`:1 on 2023-12-15 22:49_

This file needs a better name

---

_@LaBatata101 reviewed on 2023-12-15 22:49_

---

_@charliermarsh reviewed on 2023-12-16 00:35_

I'm really excited to see this. Thanks for all the work that went into it!

I haven't reviewed yet, but one clarifying question:

> The new parser still preserves the old behavior of returning only the first syntax error it encounters. The Python syntax supported by the parser is of the version 3.12.

Do we attempt to lint after encountering a syntax error? Or do we bail (as on `main`)?

---

_@charliermarsh reviewed on 2023-12-16 00:42_

---

_Review comment by @charliermarsh on `crates/ruff_linter/src/rules/flake8_gettext/mod.rs`:10 on 2023-12-16 00:42_

We should either change our settings types to use `SmolStr` or (less work) use iterations to avoid these allocations:

```rust
functions_names
    .iter()
    .any(|function_name| function_name == id)
```

---

_Label `parser` added by @dhruvmanila on 2023-12-16 01:09_

---

_Label `performance` added by @dhruvmanila on 2023-12-16 01:09_

---

_Comment by @LaBatata101 on 2023-12-16 01:32_

> I'm really excited to see this. Thanks for all the work that went into it!
> 
> I haven't reviewed yet, but one clarifying question:
> 
> > The new parser still preserves the old behavior of returning only the first syntax error it encounters. The Python syntax supported by the parser is of the version 3.12.
> 
> Do we attempt to lint after encountering a syntax error? Or do we bail (as on `main`)?

I think we could lint after encountering syntax errors but I have no idea how to do that at the moment.

---

_@charliermarsh reviewed on 2023-12-16 15:49_

---

_Review comment by @charliermarsh on `crates/ruff_python_parser/src/lexer.rs`:245 on 2023-12-16 15:49_

How much work would it be, in your opinion, to separate the `SmolStr` changes out to a separate PR, with this PR as the upstream? It feels like they're slightly independent from the changes to the parser and error recovery etc., and so separating them out would make it easier to merge this PR and the `SmolStr` PR (since they can be reviewed and considered independently), and easier to understand the impact of the `SmolStr` change.


---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/lexer.rs`:245 on 2023-12-16 16:51_

Probably won't be much work, most of the changes related to `SmolStr` are in one commit.

I just need to learn how to do that, any help would appreciated.

---

_@LaBatata101 reviewed on 2023-12-16 16:51_

---

_@zanieb reviewed on 2023-12-16 17:46_

---

_Review comment by @zanieb on `crates/ruff_python_parser/src/lexer.rs`:245 on 2023-12-16 17:46_

You can `git rebase -i main` then just "drop" that commit, noting its hash. Then you can create a new branch from your parser branch, and `git cherry-pick <commit>` the commit onto it. You can then open a pull request from that branch with your parser branch as the "base".

---

_Review comment by @charliermarsh on `crates/ruff_python_parser/src/lexer.rs`:245 on 2023-12-16 18:01_

@zanieb - the problem with this suggestion (not sure if you have ideasâ€¦) is that I donâ€™t believe @LaBatata101 will be able to create a PR on Ruff that uses this current PR as a branch, since this branch isnâ€™t in Ruff, itâ€™s in a fork.

---

_@charliermarsh reviewed on 2023-12-16 18:01_

---

_@zanieb reviewed on 2023-12-16 19:28_

---

_Review comment by @zanieb on `crates/ruff_python_parser/src/lexer.rs`:245 on 2023-12-16 19:28_

Ah true. You can create a second pull request with `main` as the base and it'll just duplicate all of the changes from here until this is merged (but we can review the single commit), or you can just wait until this is merged to open the second pull request, or you can open it over on your fork and link to it if you want feedback sooner ðŸ¤·â€â™€ï¸ 

---

_Comment by @MichaReiser on 2023-12-18 02:38_

This PR is fantastic! So many users will be surprised that Ruff will get even faster for them, and not just a little! And I can't wait to remove the complexity of the generated parser by some simple Rust code (where changing a single grammar rule doesn't change 10k lines ðŸ¤£ )

> Here's the benchmark results, runned on an Intel Core i5-8265U.

I'm confused by the results because they indicate that the new parser is slower, but codspeed suggests otherwise. Could you rerun them?

It will take me a while to get through reviewing 60k changes ðŸ˜† and I have to finish some work this week to avoid losing context before my Christmas/New Year break. I plan to do an in-depth review in the week of the 8th of January. I hope that's okay with you. 

An important decision we have to make is how we plan to roll out this change because changing the parser changes the foundation of Ruff. Bugs can have far-reaching consequences, from Ruff panicking during formatting / linting to silently changing the program semantics and making Ruff useless in the worst case (because it consistently crashes). 

Our ecosystem checks and tests give us good coverage for valid programs. But we have little coverage for invalid programs, to which Ruff is exposed when used inside an Editor (and error recovery in parsers is hard). That's why it's worth considering how we can increase our test coverage before enabling this parser in production (not the same as merging PRs). Here are some options that I've been thinking about:

1. Enable the new parser when running Ruff with `--preview` before switching the default
1. Use fuzzing and compare the parser/linter/ formatter output between Ruff using the generated and hand-written parser. @addisoncrump or @qarmin might be able to help us with setting up fuzzing. They have done excellent fuzzing work in the past.
1. Run Ruff over a more extensive set of repositories (by extending the ecosystem check and running it locally). However, this will only help a little with invalid syntax because users rarely commit invalid files.

Implementing one or two requires we find a way to switch the parser at runtime. Changing the parser at runtime requires that both parsers are API compatible: they use the same AST, the same or at least similar error recovery, etc. We can either refact our current parser to `SmolStr` OR change your parser to emit `String`s for now and later change it to `SmolStr` everywhere. Implementing error recovery in our existing parser doesn't make sense. But, we can pretend that your new parser doesn't support error recovery by returning an `Err` if the parser emits any diagnostic. 

We can figure out how to best test the parser together and how we can come up with a compatible API, but I'm happy to defer to you because you probably know it the best. 

## Invalid syntax representation

I haven't studied it in detail, but it seems that your AST design uses `Invalid` nodes when encountering invalid syntax, similar to what Biome / Swift, and, to some extent, Roslyn do. I would like to learn more about what alternatives you considered and why you decided on this design (and the granularity of invalid nodes) vs. creating missing placeholder nodes or other designs. 

## Linting / Formatting programs containing invalid syntax

For now, I think it might be best to opt out of linting and formatting invalid ASTs to reduce the scope of this PR and get time to think through the implications thoroughly. 

For example, returning the unformatted code for an invalid expression may remove the parentheses of an expression or lead to invalid syntax if the parent formatting rule assumes how the child gets formatted. What worked well for Biome is to return an `Err(FormatError::InvalidSyntax)` for invalid nodes that aren't statements.

## Thanks

Thanks again for working on this. I'm thrilled by the changes and want to see this happening. Let's figure out together how to roll this change out and then enjoy how our users will be stunned that Ruff just got significantely faster!

---

_Assigned to @MichaReiser by @MichaReiser on 2023-12-18 03:03_

---

_Comment by @addisoncrump on 2023-12-18 13:49_

Yup, setting up a differential fuzzer here should be straightforward. @LaBatata101, should I PR this to your fork?

---

_Comment by @qarmin on 2023-12-18 16:08_

[623.py.zip](https://github.com/astral-sh/ruff/files/13705891/623.py.zip) - eats all available memory with `ruff . --select ALL --no-cache`

https://github.com/qarmin/Automated-Fuzzer/releases/download/test/FILES_999.zip - contains "small" python files set, that crashes in multiple ways with same command as above
```
panicked at crates/ruff_python_parser/src/parser/mod.rs:3167:20:
called `Option::unwrap()` on a `None` value
Backtrace:    0: ruff_cli::panic::catch_unwind::{{closure}}
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/alloc/src/boxed.rs:2021:9
   2: std::panicking::rust_panic_with_hook
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/panicking.rs:735:13
   3: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/panicking.rs:601:13
   4: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/sys_common/backtrace.rs:170:18
   5: rust_begin_unwind
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/panicking.rs:597:5
   6: core::panicking::panic_fmt
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/core/src/panicking.rs:72:14
   7: core::panicking::panic
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/core/src/panicking.rs:127:5
   8: ruff_python_parser::parser::Parser<I>::parse_slice
   9: ruff_python_parser::parser::Parser<I>::parse_lhs
  10: ruff_python_parser::parser::Parser<I>::expr_bp
```
```
panicked at crates/ruff_python_parser/src/parser/mod.rs:564:9:
assertion failed: self.eat(TokenKind::Dedent)
Backtrace:    0: ruff_cli::panic::catch_unwind::{{closure}}
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/alloc/src/boxed.rs:2021:9
   2: std::panicking::rust_panic_with_hook
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/panicking.rs:735:13
   3: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/panicking.rs:601:13
   4: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/sys_common/backtrace.rs:170:18
   5: rust_begin_unwind
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/std/src/panicking.rs:597:5
   6: core::panicking::panic_fmt
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/core/src/panicking.rs:72:14
   7: core::panicking::panic
             at /rustc/a28077b28a02b92985b3a3faecf92813155f1ea1/library/core/src/panicking.rs:127:5
   8: ruff_python_parser::parser::Parser<I>::handle_unexpected_indentation
   9: ruff_python_parser::parser::functions::parse_tokens
  10: ruff_python_parser::parser::functions::parse_program
  11: ruff_linter::rules::eradicate::detection::comment_contains_code
  12: ruff_linter::rules::eradicate::rules::commented_out_code::commented_out_code
```




---

_Comment by @LaBatata101 on 2023-12-18 16:35_

> Yup, setting up a differential fuzzer here should be straightforward. @LaBatata101, should I PR this to your fork?

Yes, that would be nice!

---

_Comment by @addisoncrump on 2023-12-18 16:38_

Great. I'll set this up sometime in the coming days, it's quite late to start today. Definitely use the existing fuzzers, both mine and @qarmin's, as they find different kinds of problems. lmk if you need any guidance here, but the README should tell you what you need to know.

---

_Comment by @zanieb on 2023-12-18 17:06_

I'm a big fan of toggling use of the new parser with `--preview` if feasible. It seems like it may be challenging? Although easier if the `SmolString` changes are split out.

---

_Comment by @LaBatata101 on 2023-12-18 23:54_

> I'm confused by the results because they indicate that the new parser is slower, but codspeed suggests otherwise. Could you rerun them?

I realized that my computer it's not very reliable for benchmarks, every time I run I get a 2~4% difference in the results. I guess my old HDD is the culprit, sometimes my computer gets randomly slower. So it's for one of you guys to run the benchmark and see if the result matches.

> It will take me a while to get through reviewing 60k changes ðŸ˜† and I have to finish some work this week to avoid losing context before my Christmas/New Year break. 

Most of those 60k changes are from snapshot files.

> I plan to do an in-depth review in the week of the 8th of January. I hope that's okay with you.

No problem.

> We can either refact our current parser to SmolStr OR change your parser to emit Strings for now and later change it to SmolStr everywhere. 
I'm currently working on a PR to change the current parser to use `SmolStr`

> I haven't studied it in detail, but it seems that your AST design uses Invalid nodes when encountering invalid syntax, similar to what Biome / Swift, and, to some extent, Roslyn do. I would like to learn more about what alternatives you considered and why you decided on this design (and the granularity of invalid nodes) vs. creating missing placeholder nodes or other designs.

I didn't research this very deeply, but from what I found, there were two ways to do that, use an `Invalid` node and making every field of the AST node an `Option`. Using `Option` for every field of the node, creates two problems. The first problem is, we can't distinguish when a field of the node is actually optional or its an invalid syntax. The second one is, I would have to change basically the entire code base to support this ðŸ˜†. So I went with the `Invalid` node strategy, which felt more natural and ergonomic.
The only exception to this is the `Identifier`, where I consider an identifier as invalid if it has an empty string in the `id` field. So if I have some code like this:
```python
def 1(): ...
```
where we have a `Number` token instead of an `Id` token. The AST will look like this:
```
Module(
    ModModule {
        range: 0..13,
        body: [
            FunctionDef(
                StmtFunctionDef {
                    range: 1..13,
                    is_async: false,
                    decorator_list: [],
                    name: Identifier {
                        id: "",
                        range: 5..6,
                    },
                    type_params: None,
                    parameters: Parameters {
                        ...
                    },
                    returns: None,
                    body: [
                        ...
                    ],
                },
            ),
        ],
    },
)
```
In the `Identifier` struct, there's a function called `is_valid` that checks if `id` is empty. Probably, not the best way to represent invalid syntax in this situation.

Another limitation, in the current implementation, is in implicit concatenated f-strings with strings, where an `Invalid` node isn't created when one of the strings is invalid. For example, `f"hello" "\xxx" f"world"`, the second string has an invalid escape sequence, resulting in the following AST
```
Module(
        ModModule {
            range: 0..26,
            body: [
                Expr(
                    StmtExpr {
                        range: 1..25,
                        value: FString(
                            ExprFString {
                                range: 1..25,
                                value: FStringValue {
                                    inner: Concatenated(
                                        [
                                            FString(
                                                FString {
                                                    range: 1..9,
                                                    elements: [
                                                        Literal(
                                                            FStringLiteralElement {
                                                                range: 3..8,
                                                                value: "hello",
                                                            },
                                                        ),
                                                    ],
                                                },
                                            ),
                                            FString(
                                                FString {
                                                    range: 17..25,
                                                    elements: [
                                                        Literal(
                                                            FStringLiteralElement {
                                                                range: 19..24,
                                                                value: "world",
                                                            },
                                                        ),
                                                    ],
                                                },
                                            ),
                                        ],
                                    ),
                                },
                            },
                        ),
                    },
                ),
            ],
        },
    ),
```

What do you mean by "granularity of invalid nodes"?

---

_Comment by @MichaReiser on 2023-12-19 08:35_

> I realized that my computer it's not very reliable for benchmarks, every time I run I get a 2~4% difference in the results. I guess my old HDD is the culprit, sometimes my computer gets randomly slower. So it's for one of you guys to run the benchmark and see if the result matches.

That's okay. The codspeed benchmarks are probably good enough. I recommend removing the results from the PR summary to avoid confusion.

> Most of those 60k changes are from snapshot files.

That's good to know. And most of the deleted 80k is the generated parser :)

> I didn't research this very deeply, but from what I found, there were two ways to do that, use an Invalid node and making every field of the AST node an Option....

That sounds reasonable and we don't need to finalize the representation of invalid programs as part of this PR. This is something we can iterate on later. I just find it useful to understand the motiviation. I believe TypeScript uses a third option that I'm somewhat interested (in the Future) that avoids Invalid nodes. TypeScript's recovery (from what I understand) synthesizes nodes where necessary: For example, it creates an empty identifier node if the right side of a binary expression is missing `a + ` gets parsed as `ident(a)` `+` `ident("")`. I haven't looked into how it handles invalid tokens but I believe it simply terminates the previous statement, fills missing places with synthesized nodes, and starts a new statement that contains as much as it understands. But again, that's something we can explore later. Is this something you would be interested in?

> What do you mean by "granularity of invalid nodes"?

There are multiple invalid nodes. I think I saw invalid nodes for statements and expressions. Maybe there are more. How did you decide on which "invalid" nodes to add?

@LaBatata101 how much work do you think it would be to make the parser switchable at runtime and is this something you're interested in working on? It would help us with shipping the new parser incrementally (local Debug builds first, --preview only, switch over production) for a smooth release. 

---

_Comment by @LaBatata101 on 2023-12-20 22:16_

> That sounds reasonable and we don't need to finalize the representation of invalid programs as part of this PR. This is something we can iterate on later. I just find it useful to understand the motiviation... But again, that's something we can explore later. Is this something you would be interested in?

Yes!

> There are multiple invalid nodes. I think I saw invalid nodes for statements and expressions. Maybe there are more. How did you decide on which "invalid" nodes to add?

The current invalid nodes the AST has are for `Expression`, `Pattern` and `FStringElement`. The reason for having `Invalid` nodes in those types are similar, because in the current implementation of the parser, the functions used to parse them always needs to return the one of those types. So when encountering an unexpected token or when the lexer returns an `Err` (which gets transformed into a `TokenKind::Invalid` or `Tok::Invalid`) during the parsing, I use the `Invalid` node to handle these situations. 

So the current presence of `Invalid` nodes is due to the way I implemented the parser, which requires that parsing functions for AST nodes always return the corresponding node type, regardless of what happens.

> how much work do you think it would be to make the parser switchable at runtime and is this something you're interested in working on? It would help us with shipping the new parser incrementally (local Debug builds first, --preview only, switch over production) for a smooth release.

Honestly, I don't know how much work that would be, but I can try it anyway.

---

_Comment by @zanieb on 2023-12-20 22:37_

@LaBatata101 

> Honestly, I don't know how much work that would be, but I can try it anyway.

You may want to hold off on that for a minute we're discussing internally if we want to support two parsers or just switch over with lots of manual testing.

---

_Comment by @LaBatata101 on 2023-12-20 22:55_

> You may want to hold off on that for a minute we're discussing internally if we want to support two parsers or just switch over with lots of manual testing.

Ok, I was only going to try that after fixing the bugs found by the fuzzer which should take a while.

---

_Comment by @addisoncrump on 2023-12-31 00:42_

I've just implemented the fuzzer for equivalency between the new and the old parsers. To a degree, it might be a good thing to keep both implementations around and maintained after this is merged since the fuzzer will effectively instantly find discrepancies in future optimisations or updates.

---

_Comment by @github-actions[bot] on 2024-01-03 21:16_

<!-- generated-comment ecosystem -->
## `ruff-ecosystem` results
### Linter (stable)
âœ… ecosystem check detected no linter changes.

### Linter (preview)
âœ… ecosystem check detected no linter changes.

### Formatter (stable)
âœ… ecosystem check detected no format changes.

### Formatter (preview)
âœ… ecosystem check detected no format changes.




---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:233 on 2024-01-08 09:47_

What's the reason for returning an empty range in this case. It seems inconsistent to me that the non empty case includes the range of comments but the "empty" module does not.

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:250 on 2024-01-08 09:48_

Nit. I prefer to use `assert_eq!(&self.ctx, &[])` and `assert_eq!(&self.ctx_stack, &[])` because it generates more helpful error messages in case the context isn't empty (it shows its content)

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:270 on 2024-01-08 09:50_

What happens if `self.ctx_stack` is empty. Aren't we risking that `self.ctx` than points to a stale value? 

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:309 on 2024-01-08 09:52_

Nit: 

```suggestion
                    self.source.text_len(),
```

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:314 on 2024-01-08 09:53_

How much lookahead does the parser require? It might be worth to limit the allowed lookahead to only a few tokens or even having explicit lookahead methods. It can sometimes be tempting to use a larger lookahead even when it isn't strictly necessary. Limiting the lookahead can push us to re-consider if we can implement the same without requiring as much lookahead (e.g by adding an explicit assertion that the lookahead is never larger than `n`)

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:421 on 2024-01-08 09:56_

Does this `TODO` still need to be addressed?

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:429 on 2024-01-08 09:57_


```suggestion
                self.source.text_len() - TextSize::new(1));
```

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:416 on 2024-01-08 09:58_

Would it be an option to instead "patch" the range in `parse_expression_starts_at` or what's the reason that this function is called with out of bound offsets?

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:459 on 2024-01-08 10:00_

This feels a bit suspicious (something that `parse_separated` doesn't know). Would it be worth changing `func` (the parsing function?) to return the `TextRange` of the last parsed item instead?

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:527 on 2024-01-08 10:02_

What happens if the `Dedent` token is missing. Will this result in an infinite loop?

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/functions.rs`:1 on 2024-01-08 10:04_

I think we should move the functions in here into the `lib.rs`, considering that they are the public interface of the parser crate.

I would move the `parse_ok_tokens_lalrpop` and `parse_ok_tokens_new` into their corresponding modules `parser/mod.rs` and `lalrpop/mod.rs`

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/functions.rs`:344 on 2024-01-08 10:06_

I recommend moving `ParenthesizedExpr` into the `lalrpop` module and making it private (it's not a public type)

---

_Comment by @addisoncrump on 2024-01-08 11:10_

@MichaReiser, I'm quite confident the differential fuzzer depends on `parse_ok_tokens_{lalrpop,new}` being public. Can we somehow conditionally make these publically available?

---

_Comment by @MichaReiser on 2024-01-08 12:41_

> @MichaReiser, I'm quite confident the differential fuzzer depends on `parse_ok_tokens_{lalrpop,new}` being public. Can we somehow conditionally make these publically available?

Oh I see. Do you know if the fuzzer runs concurrently? We could otherwise simply use the environment variable to toggle between the two implementations. 

Another idea for fuzzing (I recommend implementing this as a separate PR) would be to compare the linter and formatter results (mainly for files that have no syntax errors). This ensures that the new parser works well with the tool based on it (may help to identify potential issues due to the parser generating different ranges for the same nodes)

---

_Comment by @addisoncrump on 2024-01-08 13:40_

Oh, yikes, do you not intend to use the same parsers for formatters and linters?

---

_Comment by @addisoncrump on 2024-01-08 14:02_

Okay, I reread your message :joy: I understand now.

Regarding setting the environmental variable: I'll admit, this rubs me the wrong way. Why does an internal function check the environment? This is a code smell; this should be checked earlier and passed as a configuration value, no? It is doable for now to implement it like so.

A fuzzer comparing the linter and formatter results would find a strict subset of the issues of the current differential fuzzer. If your concern is purely functional, we can replace the existing differential fuzzer with the one you are describing.


---

_Comment by @MichaReiser on 2024-01-08 14:35_

> Regarding setting the environmental variable: I'll admit, this rubs me the wrong way. Why does an internal function check the environment? This is a code smell; this should be checked earlier and passed as a configuration value, no? It is doable for now to implement it like so.

I share that sentiment that it should be an option if we want to support two different parsers in the long term. I think doing it here is fine because this isn't our goal. We only allow switching to evaluate the new parser. The old parser will be removed as soon as we do the switch over. In that sense, adding a public option to define the parser would prevent us from removing the old parser later without a breaking API (not that we care, since the parser crate is internal). 

> A fuzzer comparing the linter and formatter results would find a strict subset of the issues of the current differential fuzzer. If your concern is purely functional, we can replace the existing differential fuzzer with the one you are describing.

I think having both fuzzers is useful:

* Parser: Compares both valid and invalid programs. Easier to narrow down the issue because it is scoped to the parser. 
* Formatter / Linter: Main focus on valid programs. Identifies compatibility issues with the formatter/linter code.

---

_Review comment by @MichaReiser on `crates/ruff_python_codegen/src/generator.rs`:1196 on 2024-01-08 14:59_

I wonder if it is possible that simply printing a program that contains syntax errors produces new syntax errors. For example, let's say we had

```python
a = (4 + 
    4ident ===
    bcd
)
```

The `4identifier` is not a valid identifier and neither is the `===` operator. Now, what I fear is that we'll reprint this as

```python
a = 4 + 4ident ===
    bcd
```

which now parses as two statements because the source code generator (as far as I remember) doesn't retain parentheses. 

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/tests/function.rs`:17 on 2024-01-08 15:27_

I try to avoid macros for test generation (okay, not true, I try to avoid macros in general, it's a last resort option in my view) because they make tests harder to read and they don't work well with IDEs and other dev tools (e.g. jumping to the failing test doesn't work). 

Not using macros will require some more boilerblate but that's a worth trade in my view, considering that it makes the tests more familiar to readers

```rust

fn parse_program(code: &str) -> Result<Suite, ParseError> {
    crate::parser::parse_suite(code)
}

#[test]
fn test_function_no_args_with_ranges() {
    let parsed = parse_program("def f(): pass")
    assert_debug_snapshot!(parsed)
}
```

The `parse_program` helper function is probably overkill. But a utility function can be helpful in case we need to perform more setup logic (to avoid code duplication)

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/tests/function.rs`:47 on 2024-01-08 15:28_

Same here. I don't think the added complexity of using a macro justifies the briefer syntax. 

---

_@MichaReiser reviewed on 2024-01-08 15:41_

This is some preliminary feedback. I haven't read through all the code yet. 

Overall I'm very impressed by the changes and this goes into the direction I want our parser to go. 

My main concern right now is the representation of invalid syntax and how downstream dependencies handle it. For example, I'm not sure if parsing a program and printing it with our code generator results in the same code (and doesn't introduce new or different syntax errors). I don't know yet what the solution for this is. 

I've a few notes but these are things we can address after merging.

---

_@LaBatata101 reviewed on 2024-01-08 16:24_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/tests/function.rs`:17 on 2024-01-08 16:24_

Good point, those macros in the tests were already there, I will remove them.

---

_@LaBatata101 reviewed on 2024-01-08 16:40_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:314 on 2024-01-08 16:40_

At the moment, there's no way of knowing how much lookahead we need because of [this](https://github.com/astral-sh/ruff/blob/d4a4d056bc4db660500a89a3486ac5f7bf98dd50/crates/ruff_python_parser/src/parser/mod.rs#L1709) problem when parsing `WithItem`, but ideally it should be 1. I'm going to look how pyright handles this case.

---

_@LaBatata101 reviewed on 2024-01-08 16:47_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:233 on 2024-01-08 16:47_

I don't really know the reason why it is an empty range, the autogenerated parser does that so I just copied the behavior. Originally, the parser included the range of the comments and empty lines in the module.

---

_@LaBatata101 reviewed on 2024-01-08 21:45_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:527 on 2024-01-08 21:45_

IIRC the lexer always creates a `Dedent` token for an `Indent` token, unless there's a bug in the lexer, so an infinite loop is impossible. But I can add check for the `Eof` token just to make sure.

---

_@LaBatata101 reviewed on 2024-01-08 22:10_

---

_Review comment by @LaBatata101 on `crates/ruff_python_codegen/src/generator.rs`:1196 on 2024-01-08 22:10_

For this case, the resulting AST will be
```
Module(
        ModModule {
            range: 0..43,
            body: [
                Assign(
                    StmtAssign {
                        range: 1..20,
                        targets: [
                            Name(
                                ExprName {
                                    range: 1..2,
                                    id: "a",
                                    ctx: Store,
                                },
                            ),
                        ],
                        value: BinOp(
                            ExprBinOp {
                                range: 6..15,
                                left: NumberLiteral(
                                    ExprNumberLiteral {
                                        range: 6..7,
                                        value: Int(
                                            4,
                                        ),
                                    },
                                ),
                                op: Add,
                                right: NumberLiteral(
                                    ExprNumberLiteral {
                                        range: 14..15,
                                        value: Int(
                                            4,
                                        ),
                                    },
                                ),
                            },
                        ),
                    },
                ),
            ],
        },
    ),
```
Notice that only the binary operation appears, this is due to when parsing a parenthesized expression (or an expression inside `[]`, `{}`)  with unexpected tokens, in this case it would be `ident` (`4ident` is lexed as two different tokens), `===` and `bcd`, the parser will skip these tokens in an attempt to find the `)`. So the reprint it's probably even worse ðŸ¤£, because it'll generate this  `a = 4 + 4`. 

The error message needs improvement for this case, because it doesn't create `unexpected token` error messages, it only creates the `expecting ")", found "ident"` error message.

Shouldn't we use a CST for the code generator? An AST doesn't seem very reliable to generate code as it was written.

---

_Comment by @LaBatata101 on 2024-01-08 22:12_

Why the performance regressed ðŸ¤” 

---

_@LaBatata101 reviewed on 2024-01-08 22:38_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:270 on 2024-01-08 22:38_

`self.ctx_stack` will only be empty at the end of the parsing, if it's empty during the parsing, `self.set_ctx` or `self.clear_ctx` got misused somehow. To avoid this, I think we need to add an `assert!(!self.ctx_stack.is_empty())` in `self.clear_ctx`.

---

_@LaBatata101 reviewed on 2024-01-08 22:50_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:459 on 2024-01-08 22:50_

Do you mean the empty range that is returned? 

It's not worth changing `func` to return a `TextRange` because, we don't need the range created by `parse_separated` since we only care about the range of the delimiters, e.g., `()`, `[]` etc.

---

_@LaBatata101 reviewed on 2024-01-08 23:01_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:421 on 2024-01-08 23:01_

Yes, there's some test (I don't remember right now) where the function return type annotation is parsed separately with `parse_type_annotation`, and there is an unexpected token there. The code is something like this:
```python
def f() -> "int /":
   pass
```
With the current code, the `value` field of the `Invalid` node for the unexpected token `/` will contain the entire source, in this case `int /`, instead of just `/`. I'm not sure how to solve this yet.

---

_@LaBatata101 reviewed on 2024-01-08 23:11_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:416 on 2024-01-08 23:11_

The main problem is that `parse_expression_starts_at` passes to the parser a subset of the source code while maintaining the range of the whole source code. If we could pass the entire source code without breaking the behavior of `parse_expression_starts_at`, it would solve this problem.

---

_Comment by @charliermarsh on 2024-01-08 23:20_

@LaBatata101 - Have you rebased / merged in main? We upgraded Rust and it had a big impact on lexer / parser perf. So you might just be comparing against the wrong baseline now.

---

_Comment by @LaBatata101 on 2024-01-09 00:43_

> @LaBatata101 - Have you rebased / merged in main? We upgraded Rust and it had a big impact on lexer / parser perf. So you might just be comparing against the wrong baseline now.

I haven't merged main yet.

---

_@MichaReiser reviewed on 2024-01-09 11:20_

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:314 on 2024-01-09 11:20_

Ah interesting. Could we try parsing it as a parenthesized expression if the parser is at a `(` and and "fix up" the range if there's no following `as` keyword? That would avoid the need for the lookahead. It may require a special `parse_parenthesized_expression` method that returns the unparenthesized range as well.

---

_Comment by @MichaReiser on 2024-01-09 11:24_

I ran some hyperfine benchmarks (CPython and airflow) and the new parser is between 20 and 30% faster. Which is pretty impressive. 

---

_@MichaReiser reviewed on 2024-01-09 11:26_

---

_Review comment by @MichaReiser on `crates/ruff_python_codegen/src/generator.rs`:1196 on 2024-01-09 11:26_

Hmm that's a serious problem. It's okay for the parser to skip tokens but these tokens must be attached to the AST. I assumed it would do so by creating an `InvalidExpr` for the invalid nodes. Let me spend some time today to take a closer look at the parser's error recovery.

---

_@LaBatata101 reviewed on 2024-01-09 12:54_

---

_Review comment by @LaBatata101 on `crates/ruff_python_codegen/src/generator.rs`:1196 on 2024-01-09 12:54_

Probably won't be much work to create an `InvalidExpr` for the skipped tokens.

---

_@MichaReiser reviewed on 2024-01-09 12:56_

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:71 on 2024-01-09 12:56_

I think it should be possible to remove most usages of `ctx` by instead pass the context to the specific parsing function:

* `parse_body`: Create a `Clause` enum with variants for each possible clause that can have a body. Add an explicit `parent_clause` argument to `parse_body`. 
* `parse_paremters`: Create a dedicated `FunctionKind` enum with a variant for `Lambda` and `FunctionDefinition`. Explicilty pass the `function_kind` to `parse_parameters`.

Removing the need for `ctx_stack` is probably the trickiest but we can do it by using the call-stack by:

* Copying the existing `ctx` flags before changing them
* Changing the context flags
* Call into the sub-parsing
* Restoring the context flags afterwards

I don't fully understand `last_ctx` yet, so there might still be a need for the structure as it is today. But passing contexts explicitly where possible improves readability and helps to remove some variants from the parsing context.

This won't give you access to the parent context while parsing, but I wonder if that's even necessary. All we need to know is if we're inside of a for target. We can keep that context flag set for the entire target expression (and we can explicilty set it back to `false` if there's an expression where it needs to be restored to false).

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:71 on 2024-01-09 13:29_

If I'm not mistaken, `last_ctx`is mainly being used to check if the parsed expression was parenthesized.

---

_@LaBatata101 reviewed on 2024-01-09 13:29_

---

_@LaBatata101 reviewed on 2024-01-09 21:57_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:314 on 2024-01-09 21:57_

> Could we try parsing it as a parenthesized expression if the parser is at a ( and and "fix up" the range if there's no following as keyword?

Wouldn't work because if we have something like `(a as A, b)` and try to parse as a parenthesized expression, this example will be parsed as a tuple, resulting in an invalid syntax because of the `as` keyword.

---

_@MichaReiser reviewed on 2024-01-11 08:39_

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:71 on 2024-01-11 08:39_

I wonder if we could remove `last_ctx` by changing `parse_expr` to return a `ParsedExpression` struct that has two fields:

* The parsed expression
* A boolean field whether the expression was parenthesized



---

_@LaBatata101 reviewed on 2024-01-11 12:55_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:71 on 2024-01-11 12:55_

That might work.

---

_@LaBatata101 reviewed on 2024-01-11 23:25_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:71 on 2024-01-11 23:25_

> I wonder if we could remove `last_ctx` by changing `parse_expr` to return a `ParsedExpression` struct that has two fields:
> 
>     * The parsed expression
> 
>     * A boolean field whether the expression was parenthesized

To be able to remove `last_ctx` we need to create another solution for that problem in `parse_with_items`.

---

_@LaBatata101 reviewed on 2024-01-12 20:58_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:76 on 2024-01-12 20:58_

Is it possible to treat `ParsedExpr` as an `Expr` so that I don't have to type `.expr` every time?

---

_@MichaReiser reviewed on 2024-01-13 11:05_

---

_Review comment by @MichaReiser on `crates/ruff_python_parser/src/parser/mod.rs`:76 on 2024-01-13 11:05_

You can implement Deref on `ParsedExpr`

---

_@LaBatata101 reviewed on 2024-01-13 13:40_

---

_Review comment by @LaBatata101 on `crates/ruff_python_parser/src/parser/mod.rs`:76 on 2024-01-13 13:40_

Oh right, forgot about Deref.

---

_Assigned to @dhruvmanila by @dhruvmanila on 2024-02-15 07:20_

---

_Unassigned @MichaReiser by @dhruvmanila on 2024-02-15 07:20_

---

_Comment by @dhruvmanila on 2024-02-19 04:14_

Closing in favor of #10036. Big thanks to @LaBatata101 for their work on the new parser.

---

_Closed by @dhruvmanila on 2024-02-19 04:14_

---
