<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shrink `TypeInference` struct by 6 words - astral-sh/ruff #16185</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>Shrink <code>TypeInference</code> struct by 6 words</h1>

    <div class="meta">
        <span class="state-icon state-closed"></span>
        <a href="https://github.com/astral-sh/ruff/pull/16185">#16185</a>
        opened by <a href="https://github.com/MichaReiser">@MichaReiser</a>
        on 2025-02-16 14:32
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header">Pull request opened by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2025-02-16 14:32</div>
            <div class="timeline-body"><h2>Summary</h2>
<p>Red Knot creates a <code>TypeInference</code> result for every standalone expression, definition, and each scope.
For tomllib, that's around 10k instances. This PR reduces the size of <code>TypeInference</code> by 6 words (from 24 to 18)
by <code>Option</code> boxing <code>TypeCheckDiagnostics</code> (which gives us a 60KB memory reduction on tomllib).
The idea behind this is that most files don't have diagnostics and saving memory should outweight the cost of dereferencing and one extra allocation in the few cases
where a file/scope/expression/definition has diagnostics.</p>
<p>We may want to look into other ways on how to shrink <code>TypeInferenceResult</code> further but I'll leave at this for now.</p>
<h2>Test Plan</h2>
<p><code>cargo test</code></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">red-knot</span> added by @MichaReiser on 2025-02-16 14:32</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Marked ready for review by @MichaReiser on 2025-02-16 14:51</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from @carljm by @MichaReiser on 2025-02-16 14:51</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from @AlexWaygood by @MichaReiser on 2025-02-16 14:51</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from @sharkdp by @MichaReiser on 2025-02-16 14:51</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/AlexWaygood">@AlexWaygood</a> reviewed on 2025-02-16 15:06</div>
            <div class="timeline-body"><p>Hmm... it makes sense to me to shrink <code>TypeInference</code>, but this is actually showing a tiny regression on the cold benchmark on codspeed: https://codspeed.io/astral-sh/ruff/branches/micha%2Fshrinkg-type-inference.</p>
<p>Does this show up as an improvement when running on black, or one of the other codebases we can run with the <code>knot_benchmark</code> script? If not, I wonder if it's worth the added complexity right now</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2025-02-16 21:10</div>
            <div class="timeline-body"><p>The first run showed an improvement on the incremental benchmark. I then fixed a lint and now it's neutral.</p>
<p>I also think the regression does make sense because there are diagnostics. So it doesn't represent the use case we're optimising for.</p>
<p>I consider the complexity minimal. It's all encapsulated in a single type</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/sharkdp">@sharkdp</a> on 2025-02-17 08:21</div>
            <div class="timeline-body"><p>I saw this and thought it would be a neat testing environment for my (unfinished) peak-memory-usage feature in hyperfine. The results for checking <code>tomllib</code> are shown below. We use ~34 MiB of memory in both cases. There is no statistically significant difference between this branch and <code>main</code> (p = 0.81). But it is true that the <em>minimum</em> peak memory usage on this feature branch is 96 KiB smaller than on main (96 KiB / 34 MiB = 0.3%), which seems to be in the same order of magnitude than your estimate/measurement of 60 kB.</p>
<p><img src="https://github.com/user-attachments/assets/c2e01400-890d-4063-8f1b-9d7d793d4388" alt="1" /></p>
<p>and zoomed in:</p>
<p><img src="https://github.com/user-attachments/assets/9f0ad1e8-ac93-47e7-9e24-b56e0e612f51" alt="2" /></p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by @MichaReiser on 2025-02-17 08:31</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2025-02-17 08:32</div>
            <div class="timeline-body"><p>I still think this is worth it. While true that it's not load bearing (because of ASTs and source text), it's still a worthwhile trade off to trade close to 0 perf implication with less memory usages.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/sharkdp">@sharkdp</a> on 2025-02-17 08:51</div>
            <div class="timeline-body"><blockquote>
<p>I still think this is worth it.</p>
</blockquote>
<p>I did not want to imply that you need to close this. I just wanted to add some measurements for discussion.</p>
<blockquote>
<p>I also think the regression does make sense because there are diagnostics.</p>
</blockquote>
<p>I am not sure I'm following this argument. We currently report 5 diagnostic messages for tomllib. Even if all of them are from different scopes, it would still just be a tiny 0.05% fraction of all scopes. If there <em>were</em> a significant benefit to not allocating for the other 99.95% of scopes, shouldn't it show up in the tomllib benchmark?</p>
<blockquote>
<p>So it doesn't represent the use case we're optimising for.</p>
</blockquote>
<p>What are we optimizing for? The zero-diagnostics case? I thought I read elsewhere that we mostly care about incremental/LSP performance, where the default state is that there <em>are</em> diagnostics?</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2025-02-17 08:58</div>
            <div class="timeline-body"><blockquote>
<p>I did not want to imply that you need to close this. I just wanted to add some measurements for discussion.</p>
</blockquote>
<p>I don't care enough about this change that it's worth debating right now. I considered it a nice and small improvement but maybe it isn't as impactful as I thought (We currently crate 1k TypeInfernce scopes for tomllib for 20 files. This gives us ~50 instances and we save 6 bytes for each (minus the overhead for tracking the allocation if there is one). It comes down to maybe a 30MB memory improvement if you have 100k files</p>
<blockquote>
<p>If there were a significant benefit to not allocating for the other 99.95% of scopes, shouldn't it show up in the tomllib benchmark?</p>
</blockquote>
<p>The improvement isn't about reducing allocation. In fact, the current implementation doesn't allocate because it only stores empty <code>Vec</code>s and the new implementation has to allocate if there are diagnostics (it allocates more often). The improvement is exclusively about avoiding an unnecessarily large <code>TypeInference</code> strcut</p>
<blockquote>
<p>What are we optimizing for? The zero-diagnostics case? I thought I read elsewhere that we mostly care about incremental/LSP performance, where the default state is that there are diagnostics?</p>
</blockquote>
<p>It's correct that errors are the default in the LSP case, but there are only few scopes that have diagnostics. It's not the majority. That still means it's worth optimizing for the &quot;no&quot; or &quot;very few&quot; diagnostic cases.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/carljm">@carljm</a> on 2025-02-19 18:21</div>
            <div class="timeline-body"><blockquote>
<p>most files don't have diagnostics</p>
</blockquote>
<p>I do think it will be interesting to collect real-world data on this from some real codebases; I don't have a strong intuition about how true this is. I certainly expect it's a solid majority, but I wouldn't be surprised if it turns out to be closer to, say, 85% rather than like 99% or something. Mostly I would expect this to be due to suppressions.</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-10 19:49:30 UTC
    </footer>
</body>
</html>
