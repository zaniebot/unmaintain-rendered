<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Benchmark all rules - astral-sh/ruff #3570</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>Benchmark all rules</h1>

    <div class="meta">
        <span class="state-icon state-merged"></span>
        <a href="https://github.com/astral-sh/ruff/pull/3570">#3570</a>
        opened by <a href="https://github.com/MichaReiser">@MichaReiser</a>
        on 2023-03-17 07:07
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/MichaReiser">@MichaReiser</a></div>
            <div class="timeline-body">Summary
<p>This PR adds the new benchmark group <code>linter/all-rules</code> (and renames the existing group to <code>linter/default-rules</code>).</p>
<p>The motivation of benchmarking all rules is new rules are not part of the default-set and are, thus, not benchmarked. This can result in us missing a new rule that regresses the performance for all users enabling it.</p>
Considerations
<p>Why not change the existing benchmark to run all rules: The default rules benchmark allows us to track the performance of Ruff&#x27;s infrastructure better. The cost of our infrastructure (scope analysis, traversing the tree) is neglectable when running all rules but is more significant when only running some rules. At least now, the cost of running some more benchmarks is &quot;cheap&quot; because the CI job spends most time building the benchmark.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Marked ready for review by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-03-17 07:12</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from &quot;benchmarks: Benchmark all rules&quot; to &quot;Benchmark all rules&quot; by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-03-17 07:19</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/github-actions[bot]">@github-actions[bot]</a> on 2023-03-17 07:27</div>
            <div class="timeline-body">PR Check Results
Ecosystem
<p>âœ… ecosystem check detected no changes.</p>
Benchmark
Linux
<pre><code>group                                      main                                   pr
-----                                      ----                                   --
linter/all-rules/large/dataset.py                                                 1.00     14.5Â±0.06ms     2.8 MB/sec
linter/all-rules/numpy/ctypeslib.py                                               1.00      3.8Â±0.02ms     4.3 MB/sec
linter/all-rules/numpy/globals.py                                                 1.00    436.3Â±1.57Âµs     6.8 MB/sec
linter/all-rules/pydantic/types.py                                                1.00      6.4Â±0.01ms     4.0 MB/sec
linter/default-rules/large/dataset.py                                             1.00      8.2Â±0.01ms     5.0 MB/sec
linter/default-rules/numpy/ctypeslib.py                                           1.00   1781.3Â±3.16Âµs     9.3 MB/sec
linter/default-rules/numpy/globals.py                                             1.00    186.3Â±0.56Âµs    15.8 MB/sec
linter/default-rules/pydantic/types.py                                            1.00      3.8Â±0.01ms     6.7 MB/sec
linter/large/dataset.py                    1.00      8.3Â±0.01ms     4.9 MB/sec  
linter/numpy/ctypeslib.py                  1.00      2.2Â±0.01ms   156.8 MB/sec  
linter/numpy/globals.py                    1.00  1146.0Â±10.33Âµs   155.5 MB/sec  
linter/pydantic/types.py                   1.00      3.9Â±0.02ms     6.6 MB/sec  
</code></pre>
Windows
<pre><code>group                                      main                                    pr
-----                                      ----                                    --
linter/all-rules/large/dataset.py                                                  1.00     20.1Â±1.17ms     2.0 MB/sec
linter/all-rules/numpy/ctypeslib.py                                                1.00      5.3Â±0.21ms     3.1 MB/sec
linter/all-rules/numpy/globals.py                                                  1.00   687.8Â±45.59Âµs     4.3 MB/sec
linter/all-rules/pydantic/types.py                                                 1.00      9.0Â±0.41ms     2.8 MB/sec
linter/default-rules/large/dataset.py                                              1.00     11.5Â±0.41ms     3.5 MB/sec
linter/default-rules/numpy/ctypeslib.py                                            1.00      2.4Â±0.10ms     7.0 MB/sec
linter/default-rules/numpy/globals.py                                              1.00   295.0Â±19.57Âµs    10.0 MB/sec
linter/default-rules/pydantic/types.py                                             1.00      5.2Â±0.31ms     4.9 MB/sec
linter/large/dataset.py                    1.00     12.4Â±0.68ms     3.3 MB/sec   
linter/numpy/ctypeslib.py                  1.00      2.7Â±0.13ms   126.4 MB/sec   
linter/numpy/globals.py                    1.00  1441.7Â±116.37Âµs   123.6 MB/sec  
linter/pydantic/types.py                   1.00      5.4Â±0.27ms     4.7 MB/sec   
</code></pre>


</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Review requested from <a href="https://github.com/charliermarsh">@charliermarsh</a> by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-03-17 07:51</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/charliermarsh">@charliermarsh</a> approved on 2023-03-17 17:17</div>
            <div class="timeline-body"><p>Since we&#x27;re doubling the number of output rows, should we consider reducing the number of files that are included in the benchmark, just to keep it information-dense? (In other words, how much marginal benefit is there right now for each of the four files we&#x27;re analyzing? Are any of them redundant?)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-03-17 17:30</div>
            <div class="timeline-body"><p>The output will have 8 rows after merging (the last 4 are only because the benchmark names between main and this branch don&#x27;t match).</p>
<blockquote>
<p>Since we&#x27;re doubling the number of output rows, should we consider reducing the number of files that are included in the benchmark, just to keep it information-dense? (In other words, how much marginal benefit is there right now for each of the four files we&#x27;re analyzing? Are any of them redundant?)</p>
</blockquote>
<p>We could. I didn&#x27;t spend much time picking the files but my thinking was:</p>
<ul>
<li>a small file -&gt; sensitive to changes that increase the infrastructure overhead: <code>numpy/globals</code></li>
<li>two medium files -&gt; to represent the average case: [<code>pydantic/types</code>, <code>nmpy/ctypeslib.py</code>]</li>
<li>a large file -&gt; sensitive to rules with <code>O(n^2)</code> or worse complexity: [<code>large/dataset.py</code>]</li>
<li>a file with many type annotations <code>pydantic/types</code></li>
</ul>
<p>We could potentially remove <code>numpy/ctypeslib.py</code> because it is a medium file but I think it&#x27;s good worth keeping it because the large file has barely any comments.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-03-17 17:48</div>
            <div class="timeline-body"><p>Current dependencies on/for this PR:</p>
<ul>
<li>main<ul>
<li><strong>PR #3570</strong> <a href="https://app.graphite.dev/github/pr/charliermarsh/ruff/3570"><img src="https://static.graphite.dev/graphite-32x32.png" alt="Graphite"></a>  ðŸ‘ˆ</li>
</ul>
</li>
</ul>
<p>This comment was auto-generated by <a href="https://app.graphite.dev/github/pr/charliermarsh/ruff/3570?utm_source=stack-comment">Graphite</a>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on <code>crates/ruff_benchmark/benches/linter.rs</code>:34 on 2023-03-17 17:49</div>
            <div class="timeline-body"><p>whoops... I never verified if it downloads the correct files. The non-raw endpoints return HTML and not python :hand_over_mouth:</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Review comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on <code>crates/ruff_benchmark/src/lib.rs</code>:72 on 2023-03-17 17:50</div>
            <div class="timeline-body"><p>This fixes an issue where the benchmarks created a target folder in the <code>ruff_benchmark</code> directory instead of re-using Cargo&#x27;s <code>target</code> directory (copied from criterion)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/MichaReiser">@MichaReiser</a> reviewed on 2023-03-17 17:52</div>
            <div class="timeline-body"></div>
        </div>
        <div class="timeline-entry">
            <div class="event">Merged by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-03-17 18:29</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2023-03-17 18:29</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Branch deleted on 2023-03-17 18:29</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 18:52:49 UTC
    </footer>
</body>
</html>
