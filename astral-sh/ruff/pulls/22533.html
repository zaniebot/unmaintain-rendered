<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Do not split subscripts for literals or `Name`s - astral-sh/ruff #22533</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>Do not split subscripts for literals or <code>Name</code>s</h1>

    <div class="meta">
        <span class="state-icon state-open"></span>
        <a href="https://github.com/astral-sh/ruff/pull/22533">#22533</a>
        opened by <a href="https://github.com/dylwil3">@dylwil3</a>
        on 2026-01-12 15:03
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/dylwil3">@dylwil3</a></div>
            <div class="timeline-body"><p>WIP experiment</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">formatter</span> added by <a href="https://github.com/dylwil3">@dylwil3</a> on 2026-01-12 15:03</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">preview</span> added by <a href="https://github.com/dylwil3">@dylwil3</a> on 2026-01-12 15:03</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/astral-sh-bot[bot]">@astral-sh-bot[bot]</a> on 2026-01-12 15:16</div>
            <div class="timeline-body">

<code>ruff-ecosystem</code> results
Formatter (stable)
<p>✅ ecosystem check detected no format changes.</p>
Formatter (preview)
<p>ℹ️ ecosystem check <strong>detected format changes</strong>. (+1226 -1500 lines in 201 files in 29 projects; 26 projects unchanged)</p>
<a href="https://github.com/DisnakeDev/disnake">DisnakeDev/disnake</a> (+6 -6 lines across 1 file)
<p>
<pre>ruff format --preview</pre>
</p>
<p>

<p><a href="https://github.com/DisnakeDev/disnake/blob/394227ce57cc5245784c2125f5689cdc68a675c1/disnake/ext/commands/flag_converter.py#L288">disnake/ext/commands/flag_converter.py~L288</a></p>
<pre><code>                 flags.update(base.__dict__[&quot;__commands_flags__&quot;])
                 aliases.update(base.__dict__[&quot;__commands_flag_aliases__&quot;])
                 if case_insensitive is MISSING:
-                    attrs[&quot;__commands_flag_case_insensitive__&quot;] = base.__dict__[
-                        &quot;__commands_flag_case_insensitive__&quot;
-                    ]
+                    attrs[&quot;__commands_flag_case_insensitive__&quot;] = (
+                        base.__dict__[&quot;__commands_flag_case_insensitive__&quot;]
+                    )
                 if delimiter is MISSING:
-                    attrs[&quot;__commands_flag_delimiter__&quot;] = base.__dict__[
-                        &quot;__commands_flag_delimiter__&quot;
-                    ]
+                    attrs[&quot;__commands_flag_delimiter__&quot;] = (
+                        base.__dict__[&quot;__commands_flag_delimiter__&quot;]
+                    )
                 if prefix is MISSING:
                     attrs[&quot;__commands_flag_prefix__&quot;] = base.__dict__[&quot;__commands_flag_prefix__&quot;]
 
</code></pre>
</p>

<a href="https://github.com/RasaHQ/rasa">RasaHQ/rasa</a> (+150 -172 lines across 14 files)
<p>
<pre>ruff format --preview</pre>
</p>
<p>

<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/actions/action.py#L422">rasa/core/actions/action.py~L422</a></p>
<pre><code>         if RESPONSE_SELECTOR_PROPERTY_NAME not in latest_message.parse_data:
             return None
 
-        response_selector_properties = latest_message.parse_data[
-            RESPONSE_SELECTOR_PROPERTY_NAME  # type: ignore[literal-required]
-        ]
+        response_selector_properties = (
+            latest_message.parse_data[RESPONSE_SELECTOR_PROPERTY_NAME]  # type: ignore[literal-required]
+        )
 
         if (
             self.intent_name_from_action(self.action_name)
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/actions/action.py#L437">rasa/core/actions/action.py~L437</a></p>
<pre><code>             return None
 
         selected = response_selector_properties[query_key]
-        full_retrieval_utter_action = selected[RESPONSE_SELECTOR_PREDICTION_KEY][
-            RESPONSE_SELECTOR_UTTER_ACTION_KEY
-        ]
+        full_retrieval_utter_action = selected[RESPONSE_SELECTOR_PREDICTION_KEY][RESPONSE_SELECTOR_UTTER_ACTION_KEY]
         return full_retrieval_utter_action
 
     async def run(
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/actions/action.py#L456">rasa/core/actions/action.py~L456</a></p>
<pre><code>         if latest_message is None:
             return []
 
-        response_selector_properties = latest_message.parse_data[
-            RESPONSE_SELECTOR_PROPERTY_NAME  # type: ignore[literal-required]
-        ]
+        response_selector_properties = (
+            latest_message.parse_data[RESPONSE_SELECTOR_PROPERTY_NAME]  # type: ignore[literal-required]
+        )
 
         if (
             self.intent_name_from_action(self.action_name)
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/actions/action.py#L482">rasa/core/actions/action.py~L482</a></p>
<pre><code>         # Override utter action of ActionBotResponse
         # with the complete utter action retrieved from
         # the output of response selector.
-        self.utter_action = selected[RESPONSE_SELECTOR_PREDICTION_KEY][
-            RESPONSE_SELECTOR_UTTER_ACTION_KEY
-        ]
+        self.utter_action = selected[RESPONSE_SELECTOR_PREDICTION_KEY][RESPONSE_SELECTOR_UTTER_ACTION_KEY]
 
         return await super().run(output_channel, nlg, tracker, domain)
 
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/policies/rule_policy.py#L1233">rasa/core/policies/rule_policy.py~L1233</a></p>
<pre><code>         result = super()._default_predictions(domain)
 
         if self._enable_fallback_prediction:
-            result[domain.index_for_action(self._fallback_action_name)] = self.config[
-                &quot;core_fallback_threshold&quot;
-            ]
+            result[domain.index_for_action(self._fallback_action_name)] = (
+                self.config[&quot;core_fallback_threshold&quot;]
+            )
         return result
 
     def persist(self) -&gt; None:
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/policies/ted_policy.py#L1431">rasa/core/policies/ted_policy.py~L1431</a></p>
<pre><code>             # exactly the same positional encoding
             dialogue_in = tf.reverse_sequence(dialogue_in, dialogue_lengths, seq_axis=1)
 
-        dialogue_transformed, attention_weights = self._tf_layers[
-            f&quot;transformer.{DIALOGUE}&quot;
-        ](dialogue_in, 1 - mask, self._training)
+        dialogue_transformed, attention_weights = (
+            self._tf_layers[f&quot;transformer.{DIALOGUE}&quot;](
+                dialogue_in, 1 - mask, self._training
+            )
+        )
         dialogue_transformed = tf.nn.gelu(dialogue_transformed)
 
         if self.max_history_featurizer_is_used:
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/policies/ted_policy.py#L1515">rasa/core/policies/ted_policy.py~L1515</a></p>
<pre><code>             # state-level attributes don&#x27;t use an encoding layer, hence their size is
             # just the output size of the corresponding sparse+dense feature combining
             # layer
-            units = self._tf_layers[
-                f&quot;sparse_dense_concat_layer.{attribute}&quot;
-            ].output_units
+            units = (
+                self._tf_layers[f&quot;sparse_dense_concat_layer.{attribute}&quot;].output_units
+            )
 
         attribute_features = tf.zeros(
             (batch_dim, dialogue_dim, units), dtype=tf.float32
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/policies/ted_policy.py#L1628">rasa/core/policies/ted_policy.py~L1628</a></p>
<pre><code>                 sequence_feature_lengths, sequence_feature_lengths
             )
 
-            attribute_features, _, _, _, _, _ = self._tf_layers[
-                f&quot;sequence_layer.{attribute}&quot;
-            ](
-                (
-                    tf_batch_data[attribute][SEQUENCE],
-                    tf_batch_data[attribute][SENTENCE],
-                    sequence_feature_lengths,
-                ),
-                training=self._training,
+            attribute_features, _, _, _, _, _ = (
+                self._tf_layers[f&quot;sequence_layer.{attribute}&quot;](
+                    (
+                        tf_batch_data[attribute][SEQUENCE],
+                        tf_batch_data[attribute][SENTENCE],
+                        sequence_feature_lengths,
+                    ),
+                    training=self._training,
+                )
             )
 
             combined_sentence_sequence_feature_lengths = sequence_feature_lengths + 1
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/policies/ted_policy.py#L1672">rasa/core/policies/ted_policy.py~L1672</a></p>
<pre><code>         else:
             # resulting attribute features will have shape
             # combined batch dimension and dialogue length x 1 x units
-            attribute_features = self._tf_layers[
-                f&quot;sparse_dense_concat_layer.{attribute}&quot;
-            ]((tf_batch_data[attribute][SENTENCE],), training=self._training)
+            attribute_features = (
+                self._tf_layers[f&quot;sparse_dense_concat_layer.{attribute}&quot;](
+                    (tf_batch_data[attribute][SENTENCE],), training=self._training
+                )
+            )
 
         if attribute in SENTENCE_FEATURES_TO_ENCODE + LABEL_FEATURES_TO_ENCODE:
             attribute_features = self._tf_layers[f&quot;encoding_layer.{attribute}&quot;](
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/policies/ted_policy.py#L2062">rasa/core/policies/ted_policy.py~L2062</a></p>
<pre><code>         ) = self._embed_dialogue(dialogue_in, tf_batch_data)
         dialogue_mask = tf.squeeze(dialogue_mask, axis=-1)
 
-        sim_all, scores = self._tf_layers[
-            f&quot;loss.{LABEL}&quot;
-        ].get_similarities_and_confidences_from_embeddings(
-            dialogue_embed[:, :, tf.newaxis, :],
-            self.all_labels_embed[tf.newaxis, tf.newaxis, :, :],
-            dialogue_mask,
+        sim_all, scores = (
+            self._tf_layers[f&quot;loss.{LABEL}&quot;].get_similarities_and_confidences_from_embeddings(
+                dialogue_embed[:, :, tf.newaxis, :],
+                self.all_labels_embed[tf.newaxis, tf.newaxis, :, :],
+                dialogue_mask,
+            )
         )
 
         predictions = {
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/core/test.py#L564">rasa/core/test.py~L564</a></p>
<pre><code>     else:
         response_selector_info = (
             {
-                RESPONSE_SELECTOR_PROPERTY_NAME: predicted[
-                    RESPONSE_SELECTOR_PROPERTY_NAME
-                ]
+                RESPONSE_SELECTOR_PROPERTY_NAME: predicted[RESPONSE_SELECTOR_PROPERTY_NAME]
             }
             if RESPONSE_SELECTOR_PROPERTY_NAME in predicted
             else None
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/classifiers/diet_classifier.py#L356">rasa/nlu/classifiers/diet_classifier.py~L356</a></p>
<pre><code>             # check that all hidden layer sizes are the same
             identical_hidden_layer_sizes = all(
                 current_hidden_layer_sizes == first_hidden_layer_sizes
-                for current_hidden_layer_sizes in self.component_config[
-                    HIDDEN_LAYERS_SIZES
-                ].values()
+                for current_hidden_layer_sizes in self.component_config[HIDDEN_LAYERS_SIZES].values()
             )
             if not identical_hidden_layer_sizes:
                 raise ValueError(
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/classifiers/diet_classifier.py#L1774">rasa/nlu/classifiers/diet_classifier.py~L1774</a></p>
<pre><code>             tf_batch_data, TEXT
         )
 
-        text_transformed, _, _, _, _, attention_weights = self._tf_layers[
-            f&quot;sequence_layer.{self.text_name}&quot;
-        ](
-            (
-                tf_batch_data[TEXT][SEQUENCE],
-                tf_batch_data[TEXT][SENTENCE],
-                sequence_feature_lengths,
-            ),
-            training=self._training,
+        text_transformed, _, _, _, _, attention_weights = (
+            self._tf_layers[f&quot;sequence_layer.{self.text_name}&quot;](
+                (
+                    tf_batch_data[TEXT][SEQUENCE],
+                    tf_batch_data[TEXT][SENTENCE],
+                    sequence_feature_lengths,
+                ),
+                training=self._training,
+            )
         )
         predictions = {
             DIAGNOSTIC_DATA: {
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/classifiers/diet_classifier.py#L1860">rasa/nlu/classifiers/diet_classifier.py~L1860</a></p>
<pre><code>         )
         sentence_vector_embed = self._tf_layers[f&quot;embed.{TEXT}&quot;](sentence_vector)
 
-        _, scores = self._tf_layers[
-            f&quot;loss.{LABEL}&quot;
-        ].get_similarities_and_confidences_from_embeddings(
-            sentence_vector_embed[:, tf.newaxis, :],
-            self.all_labels_embed[tf.newaxis, :, :],
+        _, scores = (
+            self._tf_layers[f&quot;loss.{LABEL}&quot;].get_similarities_and_confidences_from_embeddings(
+                sentence_vector_embed[:, tf.newaxis, :],
+                self.all_labels_embed[tf.newaxis, :, :],
+            )
         )
 
         return {&quot;i_scores&quot;: scores}
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/extractors/extractor.py#L402">rasa/nlu/extractors/extractor.py~L402</a></p>
<pre><code>         }
 
         if confidences is not None:
-            entity[ENTITY_ATTRIBUTE_CONFIDENCE_TYPE] = confidences[
-                ENTITY_ATTRIBUTE_TYPE
-            ][idx]
+            entity[ENTITY_ATTRIBUTE_CONFIDENCE_TYPE] = (
+                confidences[ENTITY_ATTRIBUTE_TYPE][idx]
+            )
 
         if ENTITY_ATTRIBUTE_ROLE in tag_names and role_tag != NO_ENTITY_TAG:
             entity[ENTITY_ATTRIBUTE_ROLE] = role_tag
             if confidences is not None:
-                entity[ENTITY_ATTRIBUTE_CONFIDENCE_ROLE] = confidences[
-                    ENTITY_ATTRIBUTE_ROLE
-                ][idx]
+                entity[ENTITY_ATTRIBUTE_CONFIDENCE_ROLE] = (
+                    confidences[ENTITY_ATTRIBUTE_ROLE][idx]
+                )
         if ENTITY_ATTRIBUTE_GROUP in tag_names and group_tag != NO_ENTITY_TAG:
             entity[ENTITY_ATTRIBUTE_GROUP] = group_tag
             if confidences is not None:
-                entity[ENTITY_ATTRIBUTE_CONFIDENCE_GROUP] = confidences[
-                    ENTITY_ATTRIBUTE_GROUP
-                ][idx]
+                entity[ENTITY_ATTRIBUTE_CONFIDENCE_GROUP] = (
+                    confidences[ENTITY_ATTRIBUTE_GROUP][idx]
+                )
 
         return entity
 
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/featurizers/dense_featurizer/convert_featurizer.py#L324">rasa/nlu/featurizers/dense_featurizer/convert_featurizer.py~L324</a></p>
<pre><code> 
     def _sentence_encoding_of_text(self, batch: List[Text]) -&gt; np.ndarray:
 
-        return self.sentence_encoding_signature(tf.convert_to_tensor(batch))[
-            &quot;default&quot;
-        ].numpy()
+        return self.sentence_encoding_signature(
+            tf.convert_to_tensor(batch)
+        )[&quot;default&quot;].numpy()
 
     def _sequence_encoding_of_text(self, batch: List[Text]) -&gt; np.ndarray:
 
-        return self.sequence_encoding_signature(tf.convert_to_tensor(batch))[
-            &quot;sequence_encoding&quot;
-        ].numpy()
+        return self.sequence_encoding_signature(
+            tf.convert_to_tensor(batch)
+        )[&quot;sequence_encoding&quot;].numpy()
 
     def process_training_data(self, training_data: TrainingData) -&gt; TrainingData:
         &quot;&quot;&quot;Featurize all message attributes in the training data with the ConveRT model.
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/featurizers/dense_featurizer/convert_featurizer.py#L410">rasa/nlu/featurizers/dense_featurizer/convert_featurizer.py~L410</a></p>
<pre><code> 
     def _tokenize(self, sentence: Text) -&gt; Any:
 
-        return self.tokenize_signature(tf.convert_to_tensor([sentence]))[
-            &quot;default&quot;
-        ].numpy()
+        return self.tokenize_signature(
+            tf.convert_to_tensor([sentence])
+        )[&quot;default&quot;].numpy()
 
     def tokenize(self, message: Message, attribute: Text) -&gt; List[Token]:
         &quot;&quot;&quot;Tokenize the text using the ConveRT model.
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/selectors/response_selector.py#L814">rasa/nlu/selectors/response_selector.py~L814</a></p>
<pre><code>         )
 
         # Combine all feature types into one and embed using a transformer.
-        label_transformed, _, _, _, _, _ = self._tf_layers[
-            f&quot;sequence_layer.{self.label_name}&quot;
-        ](
-            (
-                self.tf_label_data[LABEL][SEQUENCE],
-                self.tf_label_data[LABEL][SENTENCE],
-                sequence_feature_lengths,
-            ),
-            training=self._training,
+        label_transformed, _, _, _, _, _ = (
+            self._tf_layers[f&quot;sequence_layer.{self.label_name}&quot;](
+                (
+                    self.tf_label_data[LABEL][SEQUENCE],
+                    self.tf_label_data[LABEL][SENTENCE],
+                    sequence_feature_lengths,
+                ),
+                training=self._training,
+            )
         )
 
         # Last token is taken from the last position with real features, determined
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/selectors/response_selector.py#L879">rasa/nlu/selectors/response_selector.py~L879</a></p>
<pre><code>         sequence_feature_lengths_label = self._get_sequence_feature_lengths(
             tf_batch_data, LABEL
         )
-        label_transformed, _, _, _, _, _ = self._tf_layers[
-            f&quot;sequence_layer.{self.label_name}&quot;
-        ](
-            (
-                tf_batch_data[LABEL][SEQUENCE],
-                tf_batch_data[LABEL][SENTENCE],
-                sequence_feature_lengths_label,
-            ),
-            training=self._training,
+        label_transformed, _, _, _, _, _ = (
+            self._tf_layers[f&quot;sequence_layer.{self.label_name}&quot;](
+                (
+                    tf_batch_data[LABEL][SEQUENCE],
+                    tf_batch_data[LABEL][SENTENCE],
+                    sequence_feature_lengths_label,
+                ),
+                training=self._training,
+            )
         )
 
         losses = []
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/selectors/response_selector.py#L953">rasa/nlu/selectors/response_selector.py~L953</a></p>
<pre><code>         sequence_feature_lengths = self._get_sequence_feature_lengths(
             tf_batch_data, TEXT
         )
-        text_transformed, _, _, _, _, attention_weights = self._tf_layers[
-            f&quot;sequence_layer.{self.text_name}&quot;
-        ](
-            (
-                tf_batch_data[TEXT][SEQUENCE],
-                tf_batch_data[TEXT][SENTENCE],
-                sequence_feature_lengths,
-            ),
-            training=self._training,
+        text_transformed, _, _, _, _, attention_weights = (
+            self._tf_layers[f&quot;sequence_layer.{self.text_name}&quot;](
+                (
+                    tf_batch_data[TEXT][SEQUENCE],
+                    tf_batch_data[TEXT][SENTENCE],
+                    sequence_feature_lengths,
+                ),
+                training=self._training,
+            )
         )
 
         predictions = {
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/nlu/selectors/response_selector.py#L978">rasa/nlu/selectors/response_selector.py~L978</a></p>
<pre><code>         sentence_vector = self._last_token(text_transformed, sequence_feature_lengths)
         sentence_vector_embed = self._tf_layers[f&quot;embed.{TEXT}&quot;](sentence_vector)
 
-        _, scores = self._tf_layers[
-            f&quot;loss.{LABEL}&quot;
-        ].get_similarities_and_confidences_from_embeddings(
-            sentence_vector_embed[:, tf.newaxis, :],
-            self.all_labels_embed[tf.newaxis, :, :],
+        _, scores = (
+            self._tf_layers[f&quot;loss.{LABEL}&quot;].get_similarities_and_confidences_from_embeddings(
+                sentence_vector_embed[:, tf.newaxis, :],
+                self.all_labels_embed[tf.newaxis, :, :],
+            )
         )
         predictions[&quot;i_scores&quot;] = scores
 
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/shared/core/domain.py#L616">rasa/shared/core/domain.py~L616</a></p>
<pre><code>                     entity_properties.entities.append(_entity)
                     if sub_labels:
                         if ENTITY_ROLES_KEY in sub_labels:
-                            entity_properties.roles[_entity] = sub_labels[
-                                ENTITY_ROLES_KEY
-                            ]
+                            entity_properties.roles[_entity] = (
+                                sub_labels[ENTITY_ROLES_KEY]
+                            )
                         if ENTITY_GROUPS_KEY in sub_labels:
-                            entity_properties.groups[_entity] = sub_labels[
-                                ENTITY_GROUPS_KEY
-                            ]
+                            entity_properties.groups[_entity] = (
+                                sub_labels[ENTITY_GROUPS_KEY]
+                            )
                         if (
                             ENTITY_FEATURIZATION_KEY in sub_labels
                             and sub_labels[ENTITY_FEATURIZATION_KEY] is False
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/utils/tensorflow/rasa_layers.py#L82">rasa/utils/tensorflow/rasa_layers.py~L82</a></p>
<pre><code>                     attribute in new_sparse_feature_sizes
                     and feature_type in new_sparse_feature_sizes[attribute]
                 ):
-                    new_feature_sizes = new_sparse_feature_sizes[attribute][
-                        feature_type
-                    ]
-                    old_feature_sizes = old_sparse_feature_sizes[attribute][
-                        feature_type
-                    ]
+                    new_feature_sizes = (
+                        new_sparse_feature_sizes[attribute][feature_type]
+                    )
+                    old_feature_sizes = (
+                        old_sparse_feature_sizes[attribute][feature_type]
+                    )
                     if sum(new_feature_sizes) &gt; sum(old_feature_sizes):
                         self._tf_layers[name] = self._replace_dense_for_sparse_layer(
                             layer_to_replace=layer,
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/utils/tensorflow/rasa_layers.py#L474">rasa/utils/tensorflow/rasa_layers.py~L474</a></p>
<pre><code>             # featurizers.
             if sequence_units != sentence_units:
                 for feature_type in [SEQUENCE, SENTENCE]:
-                    self._tf_layers[
-                        f&quot;unify_dims_before_seq_sent_concat.{feature_type}&quot;
-                    ] = layers.Ffnn(
+                    self._tf_layers[f&quot;unify_dims_before_seq_sent_concat.{feature_type}&quot;] = layers.Ffnn(
                         layer_name_suffix=f&quot;unify_dims.{attribute}_{feature_type}&quot;,
                         layer_sizes=[config[CONCAT_DIMENSION][attribute]],
                         dropout_rate=config[DROP_RATE],
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/rasa/utils/tensorflow/rasa_layers.py#L513">rasa/utils/tensorflow/rasa_layers.py~L513</a></p>
<pre><code>         # If needed, pass both feature types through a dense layer to bring them to the
         # same shape.
         if f&quot;unify_dims_before_seq_sent_concat.{SEQUENCE}&quot; in self._tf_layers:
-            sequence_tensor = self._tf_layers[
-                f&quot;unify_dims_before_seq_sent_concat.{SEQUENCE}&quot;
-            ](sequence_tensor)
+            sequence_tensor = (
+                self._tf_layers[f&quot;unify_dims_before_seq_sent_concat.{SEQUENCE}&quot;](
+                    sequence_tensor
+                )
+            )
         if f&quot;unify_dims_before_seq_sent_concat.{SENTENCE}&quot; in self._tf_layers:
-            sentence_tensor = self._tf_layers[
-                f&quot;unify_dims_before_seq_sent_concat.{SENTENCE}&quot;
-            ](sentence_tensor)
+            sentence_tensor = (
+                self._tf_layers[f&quot;unify_dims_before_seq_sent_concat.{SENTENCE}&quot;](
+                    sentence_tensor
+                )
+            )
 
         # mask_combined_sequence_sentence has for each input example a sequence of 1s of
         # the length seq_length+1, where seq_length is the number of real tokens. The
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/tests/core/evaluation/test_marker_stats.py#L284">tests/core/evaluation/test_marker_stats.py~L284</a></p>
<pre><code>         rows = [row for row in reader]
 
     actual_information = {
-        (row[&quot;sender_id&quot;], row[&quot;session_idx&quot;], row[&quot;marker&quot;], row[&quot;statistic&quot;]): row[
-            &quot;value&quot;
-        ]
+        (
+            row[&quot;sender_id&quot;],
+            row[&quot;session_idx&quot;],
+            row[&quot;marker&quot;],
+            row[&quot;statistic&quot;],
+        ): row[&quot;value&quot;]
         for row in rows
     }
     num_digits = 3
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/tests/core/policies/test_unexpected_intent_policy.py#L122">tests/core/policies/test_unexpected_intent_policy.py~L122</a></p>
<pre><code>             SENTENCE,
         ]
         assert list(assembled_label_data_signature[LABEL].keys()) == [IDS]
-        assert assembled_label_data_signature[f&quot;{LABEL}_{INTENT}&quot;][SENTENCE][
-            0
-        ].units == len(default_domain.intents)
+        assert (
+            assembled_label_data_signature[f&quot;{LABEL}_{INTENT}&quot;][SENTENCE][0].units
+            == len(default_domain.intents)
+        )
 
     def test_training_with_no_intent(
         self,
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/tests/nlu/classifiers/test_diet_classifier.py#L788">tests/nlu/classifiers/test_diet_classifier.py~L788</a></p>
<pre><code>     old_sparse_feature_sizes = processed_message.get_sparse_feature_sizes(
         attribute=TEXT
     )
-    initial_diet_layers = classifier.model._tf_layers[&quot;sequence_layer.text&quot;]._tf_layers[
-        &quot;feature_combining&quot;
-    ]
-    initial_diet_sequence_layer = initial_diet_layers._tf_layers[
-        &quot;sparse_dense.sequence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
-    initial_diet_sentence_layer = initial_diet_layers._tf_layers[
-        &quot;sparse_dense.sentence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
+    initial_diet_layers = classifier.model._tf_layers[&quot;sequence_layer.text&quot;]._tf_layers[&quot;feature_combining&quot;]
+    initial_diet_sequence_layer = initial_diet_layers._tf_layers[&quot;sparse_dense.sequence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
+    initial_diet_sentence_layer = initial_diet_layers._tf_layers[&quot;sparse_dense.sentence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
 
     initial_diet_sequence_size = initial_diet_sequence_layer.get_kernel().shape[0]
     initial_diet_sentence_size = initial_diet_sentence_layer.get_kernel().shape[0]
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/tests/nlu/classifiers/test_diet_classifier.py#L819">tests/nlu/classifiers/test_diet_classifier.py~L819</a></p>
<pre><code>         attribute=TEXT
     )
 
-    final_diet_layers = finetune_classifier.model._tf_layers[
-        &quot;sequence_layer.text&quot;
-    ]._tf_layers[&quot;feature_combining&quot;]
-    final_diet_sequence_layer = final_diet_layers._tf_layers[
-        &quot;sparse_dense.sequence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
-    final_diet_sentence_layer = final_diet_layers._tf_layers[
-        &quot;sparse_dense.sentence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
+    final_diet_layers = finetune_classifier.model._tf_layers[&quot;sequence_layer.text&quot;]._tf_layers[&quot;feature_combining&quot;]
+    final_diet_sequence_layer = final_diet_layers._tf_layers[&quot;sparse_dense.sequence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
+    final_diet_sentence_layer = final_diet_layers._tf_layers[&quot;sparse_dense.sentence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
 
     final_diet_sequence_size = final_diet_sequence_layer.get_kernel().shape[0]
     final_diet_sentence_size = final_diet_sentence_layer.get_kernel().shape[0]
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/tests/nlu/selectors/test_selectors.py#L700">tests/nlu/selectors/test_selectors.py~L700</a></p>
<pre><code>         attribute=TEXT
     )
 
-    initial_rs_layers = response_selector.model._tf_layers[
-        &quot;sequence_layer.text&quot;
-    ]._tf_layers[&quot;feature_combining&quot;]
-    initial_rs_sequence_layer = initial_rs_layers._tf_layers[
-        &quot;sparse_dense.sequence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
-    initial_rs_sentence_layer = initial_rs_layers._tf_layers[
-        &quot;sparse_dense.sentence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
+    initial_rs_layers = response_selector.model._tf_layers[&quot;sequence_layer.text&quot;]._tf_layers[&quot;feature_combining&quot;]
+    initial_rs_sequence_layer = initial_rs_layers._tf_layers[&quot;sparse_dense.sequence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
+    initial_rs_sentence_layer = initial_rs_layers._tf_layers[&quot;sparse_dense.sentence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
 
     initial_rs_sequence_size = initial_rs_sequence_layer.get_kernel().shape[0]
     initial_rs_sentence_size = initial_rs_sentence_layer.get_kernel().shape[0]
</code></pre>
<p><a href="https://github.com/RasaHQ/rasa/blob/c4069568b4fe2adb5d5a1e55d17ce8cb9dda27fc/tests/nlu/selectors/test_selectors.py#L737">tests/nlu/selectors/test_selectors.py~L737</a></p>
<pre><code>         attribute=TEXT
     )
 
-    final_rs_layers = response_selector.model._tf_layers[
-        &quot;sequence_layer.text&quot;
-    ]._tf_layers[&quot;feature_combining&quot;]
-    final_rs_sequence_layer = final_rs_layers._tf_layers[
-        &quot;sparse_dense.sequence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
-    final_rs_sentence_layer = final_rs_layers._tf_layers[
-        &quot;sparse_dense.sentence&quot;
-    ]._tf_layers[&quot;sparse_to_dense&quot;]
+    final_rs_layers = response_selector.model._tf_layers[&quot;sequence_layer.text&quot;]._tf_layers[&quot;feature_combining&quot;]
+    final_rs_sequence_layer = final_rs_layers._tf_layers[&quot;sparse_dense.sequence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
+    final_rs_sentence_layer = final_rs_layers._tf_layers[&quot;sparse_dense.sentence&quot;]._tf_layers[&quot;sparse_to_dense&quot;]
 
     final_rs_sequence_size = final_rs_sequence_layer.get_kernel().shape[0]
     final_rs_sentence_size = final_rs_sentence_layer.get_kernel().shape[0]
</code></pre>
</p>

<a href="https://github.com/Snowflake-Labs/snowcli">Snowflake-Labs/snowcli</a> (+12 -12 lines across 4 files)
<p>
<pre>ruff format --preview</pre>
</p>
<p>

<p><a href="https://github.com/Snowflake-Labs/snowcli/blob/d175ae1dc28451de72bdfd9ff33212a9f1920932/src/snowflake/cli/api/commands/overrideable_parameter.py#L107">src/snowflake/cli/api/commands/overrideable_parameter.py~L107</a></p>
<pre><code>                         value and ctx.params.get(name, False)
                     ):  # if the current parameter is set to True and a previous parameter is also Truthy
                         curr_opt = param.opts[0]
-                        other_opt = [x for x in ctx.command.params if x.name == name][
-                            0
-                        ].opts[0]
+                        other_opt = [
+                            x for x in ctx.command.params if x.name == name
+                        ][0].opts[0]
                         raise IncompatibleParametersError([curr_opt, other_opt])
 
             # pass args to existing callback based on its signature (this is how Typer infers callback args)
</code></pre>
<p><a href="https://github.com/Snowflake-Labs/snowcli/blob/d175ae1dc28451de72bdfd9ff33212a9f1920932/test_external_plugins/snowpark_hello_single_command/src/snowflakecli/test_plugins/snowpark_hello/manager.py#L18">test_external_plugins/snowpark_hello_single_command/src/snowflakecli/test_plugins/snowpark_hello/manager.py~L18</a></p>
<pre><code> 
 class SnowparkHelloManager(SqlExecutionMixin):
     _plugin_config_manager = PluginConfigProvider()
-    _greeting = _plugin_config_manager.get_config(&quot;snowpark-hello&quot;).internal_config[
-        &quot;greeting&quot;
-    ]
+    _greeting = _plugin_config_manager.get_config(
+        &quot;snowpark-hello&quot;
+    ).internal_config[&quot;greeting&quot;]
 
     def say_hello(self, name: str) -&gt; SnowflakeCursor:
         return self.execute_query(
</code></pre>
<p><a href="https://github.com/Snowflake-Labs/snowcli/blob/d175ae1dc28451de72bdfd9ff33212a9f1920932/tests/app/test_telemetry.py#L60">tests/app/test_telemetry.py~L60</a></p>
<pre><code>         .to_dict()
     )
 
-    del usage_command_event[&quot;message&quot;][
-        &quot;command_ci_environment&quot;
-    ]  # to avoid side effect from CI
+    del (
+        usage_command_event[&quot;message&quot;][&quot;command_ci_environment&quot;]
+    )  # to avoid side effect from CI
     assert usage_command_event == {
         &quot;message&quot;: {
             &quot;driver_type&quot;: &quot;PythonConnector&quot;,
</code></pre>
<p><a href="https://github.com/Snowflake-Labs/snowcli/blob/d175ae1dc28451de72bdfd9ff33212a9f1920932/tests/nativeapp/test_manager.py#L1729">tests/nativeapp/test_manager.py~L1729</a></p>
<pre><code> 
     def get_events():
         dm = _get_dm()
-        pkg_model: ApplicationPackageEntityModel = dm.project_definition.entities[
-            &quot;app_pkg&quot;
-        ]
+        pkg_model: ApplicationPackageEntityModel = (
+            dm.project_definition.entities[&quot;app_pkg&quot;]
+        )
         app_model: ApplicationEntityModel = dm.project_definition.entities[&quot;myapp&quot;]
         app = ApplicationEntity(app_model, workspace_context)
         return app.get_events(
</code></pre>
</p>

<a href="https://github.com/alteryx/featuretools">alteryx/featuretools</a> (+16 -18 lines across 2 files)
<p>
<pre>ruff format --preview</pre>
</p>
<p>

<p><a href="https://github.com/alteryx/featuretools/blob/938a0f6ccb98eaf21eca89830a25be2358a75db7/featuretools/entityset/entityset.py#L917">featuretools/entityset/entityset.py~L917</a></p>
<pre><code>         )
 
         new_dataframe = new_dataframe.dropna(subset=[index])
-        new_dataframe2 = new_dataframe.drop_duplicates(index, keep=&quot;first&quot;)[
-            selected_columns
-        ]
+        new_dataframe2 = new_dataframe.drop_duplicates(
+            index, keep=&quot;first&quot;
+        )[selected_columns]
 
         if make_time_index:
             new_dataframe2 = new_dataframe2.rename(
</code></pre>
<p><a href="https://github.com/alteryx/featuretools/blob/938a0f6ccb98eaf21eca89830a25be2358a75db7/featuretools/entityset/entityset.py#L934">featuretools/entityset/entityset.py~L934</a></p>
<pre><code>             secondary_columns = [index, secondary_time_index] + list(
                 make_secondary_time_index.values(),
             )[0]
-            secondary_df = new_dataframe.drop_duplicates(index, keep=&quot;last&quot;)[
-                secondary_columns
-            ]
+            secondary_df = new_dataframe.drop_duplicates(
+                index, keep=&quot;last&quot;
+            )[secondary_columns]
             if new_dataframe_secondary_time_index:
                 secondary_df = secondary_df.rename(
                     columns={secondary_time_index: new_dataframe_secondary_time_index},
</code></pre>
<p><a href="https://github.com/alteryx/featuretools/blob/938a0f6ccb98eaf21eca89830a25be2358a75db7/featuretools/entityset/entityset.py#L984">featuretools/entityset/entityset.py~L984</a></p>
<pre><code>             secondary_time_index=make_secondary_time_index,
         )
 
-        self.dataframe_dict[base_dataframe_name] = self.dataframe_dict[
-            base_dataframe_name
-        ].ww.drop(additional_columns)
+        self.dataframe_dict[base_dataframe_name] = (
+            self.dataframe_dict[base_dataframe_name].ww.drop(additional_columns)
+        )
 
         self.dataframe_dict[base_dataframe_name].ww.add_semantic_tags(
             {base_dataframe_index: &quot;foreign_key&quot;},
</code></pre>
<p><a href="https://github.com/alteryx/featuretools/blob/938a0f6ccb98eaf21eca89830a25be2358a75db7/featuretools/entityset/entityset.py#L1293">featuretools/entityset/entityset.py~L1293</a></p>
<pre><code> 
         if dataframe_name is not None and values is not None:
             for column, vals in values.items():
-                self[dataframe_name].ww.columns[column].metadata[
-                    &quot;interesting_values&quot;
-                ] = vals
+                self[dataframe_name].ww.columns[column].metadata[&quot;interesting_values&quot;] = vals
             return
 
         if dataframe_name:
</code></pre>
<p><a href="https://github.com/alteryx/featuretools/blob/938a0f6ccb98eaf21eca89830a25be2358a75db7/featuretools/primitives/options_utils.py#L96">featuretools/primitives/options_utils.py~L96</a></p>
<pre><code>                 # don&#x27;t globally ignore a column if it&#x27;s included for a primitive
                 if &quot;include_columns&quot; in option:
                     for dataframe, include_cols in option[&quot;include_columns&quot;].items():
-                        global_ignore_columns[dataframe] = global_ignore_columns[
-                            dataframe
-                        ].difference(include_cols)
+                        global_ignore_columns[dataframe] = (
+                            global_ignore_columns[dataframe].difference(include_cols)
+                        )
                 option[&quot;ignore_dataframes&quot;] = option[&quot;ignore_dataframes&quot;].union(
                     ignore_dataframes.difference(included_dataframes),
                 )
</code></pre>
<p><a href="https://github.com/alteryx/featuretools/blob/938a0f6ccb98eaf21eca89830a25be2358a75db7/featuretools/primitives/options_utils.py#L106">featuretools/primitives/options_utils.py~L106</a></p>
<pre><code>                 # if already ignoring columns for this dataframe, add globals
                 for option in options:
                     if dataframe in option[&quot;ignore_columns&quot;]:
-                        option[&quot;ignore_columns&quot;][dataframe] = option[&quot;ignore_columns&quot;][
-                            dataframe
-                        ].union(ignore_cols)
+                        option[&quot;ignore_columns&quot;][dataframe] = (
+                            option[&quot;ignore_columns&quot;][dataframe].union(ignore_cols)
+                        )
                     # if no ignore_columns and dataframe is explicitly included, don&#x27;t ignore the column
                     elif dataframe in included_dataframes:
                         continue
</code></pre>
</p>

<a href="https://github.com/PlasmaPy/PlasmaPy">PlasmaPy/PlasmaPy</a> (+3 -3 lines across 1 file)
<p>
<pre>ruff format --preview</pre>
</p>
<p>

<p><a href="https://github.com/PlasmaPy/PlasmaPy/blob/5625ea9cd01bf599932a5e4bea01169f99f48fbc/src/plasmapy/formulary/braginskii.py#L2024">src/plasmapy/formulary/braginskii.py~L2024</a></p>
<pre><code>     eta_0 = eta_0_e[Z_idx]
 
     def f_eta_2(Z_idx, r):
-        numerator = (6 / 5 * Z + 3 / 5 * np.sqrt(2)) * r + hprime_0[Z_idx] * eta_0_e[
-            Z_idx
-        ]
+        numerator = (
+            6 / 5 * Z + 3 / 5 * np.sqrt(2)
+        ) * r + hprime_0[Z_idx] * eta_0_e[Z_idx]
         denominator = (
             r**3
             + hprime_4[Z_idx] * r ** (7 / 3)
</code></pre>
</p>

<a href="https://github.com/apache/airflow">apache/airflow</a> (+36 -39 lines across 8 files)
<p>
<pre>ruff format --preview</pre>
</p>
<p>

<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_task_instances.py#L4506">airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_task_instances.py~L4506</a></p>
<pre><code>         )
         response_data = response.json()
         if expected_status_code == 200:
-            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;created_at&quot;] = response_data[&quot;task_instances&quot;][
-                0
-            ][&quot;dag_version&quot;][&quot;created_at&quot;]
-            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;id&quot;] = response_data[&quot;task_instances&quot;][0][
-                &quot;dag_version&quot;
-            ][&quot;id&quot;]
+            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;created_at&quot;] = (
+                response_data[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;created_at&quot;]
+            )
+            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;id&quot;] = (
+                response_data[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;id&quot;]
+            )
             expected_json[&quot;task_instances&quot;][0][&quot;id&quot;] = response_data[&quot;task_instances&quot;][0][&quot;id&quot;]
         assert response.status_code == expected_status_code
         assert response_data == expected_json
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_task_instances.py#L5265">airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_task_instances.py~L5265</a></p>
<pre><code>         )
         response_data = response.json()
         if expected_status_code == 200:
-            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;created_at&quot;] = response_data[&quot;task_instances&quot;][
-                0
-            ][&quot;dag_version&quot;][&quot;created_at&quot;]
-            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;id&quot;] = response_data[&quot;task_instances&quot;][0][
-                &quot;dag_version&quot;
-            ][&quot;id&quot;]
+            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;created_at&quot;] = (
+                response_data[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;created_at&quot;]
+            )
+            expected_json[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;id&quot;] = (
+                response_data[&quot;task_instances&quot;][0][&quot;dag_version&quot;][&quot;id&quot;]
+            )
             expected_json[&quot;task_instances&quot;][0][&quot;id&quot;] = response_data[&quot;task_instances&quot;][0][&quot;id&quot;]
         assert response.status_code == expected_status_code
         assert response_data == expected_json
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/airflow-core/tests/unit/jobs/test_scheduler_job.py#L2584">airflow-core/tests/unit/jobs/test_scheduler_job.py~L2584</a></p>
<pre><code>             &quot;stuck in queued tries exceeded&quot;,
         ]
 
-        mock_executors[
-            0
-        ].send_callback.assert_called_once()  # this should only be called for the task that has a callback
+        mock_executors[0].send_callback.assert_called_once()  # this should only be called for the task that has a callback
         states = [x.state for x in dr.get_task_instances(session=session)]
         assert states == [&quot;failed&quot;, &quot;failed&quot;]
         mock_executors[0].fail.assert_called()
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/airflow-core/tests/unit/jobs/test_scheduler_job.py#L2686">airflow-core/tests/unit/jobs/test_scheduler_job.py~L2686</a></p>
<pre><code>             &quot;stuck in queued tries exceeded&quot;,
         ]
 
-        mock_executors[
-            0
-        ].send_callback.assert_called_once()  # this should only be called for the task that has a callback
+        mock_executors[0].send_callback.assert_called_once()  # this should only be called for the task that has a callback
         states = [x.state for x in dr.get_task_instances(session=session)]
         assert states == [&quot;failed&quot;, &quot;failed&quot;]
         mock_executors[0].fail.assert_called()
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/airflow-ctl/src/airflowctl/ctl/cli_config.py#L603">airflow-ctl/src/airflowctl/ctl/cli_config.py~L603</a></p>
<pre><code>             for parameter in api_operation[&quot;parameters&quot;]:
                 for parameter_key, parameter_type in parameter.items():
                     if self._is_primitive_type(type_name=parameter_type):
-                        method_params[self._sanitize_method_param_key(parameter_key)] = args_dict[
-                            parameter_key
-                        ]
+                        method_params[self._sanitize_method_param_key(parameter_key)] = (
+                            args_dict[parameter_key]
+                        )
                     else:
                         datamodel = getattr(generated_datamodels, parameter_type)
                         for expanded_parameter in self.datamodels_extended_map[parameter_type]:
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/providers/amazon/src/airflow/providers/amazon/aws/hooks/sagemaker.py#L1205">providers/amazon/src/airflow/providers/amazon/aws/hooks/sagemaker.py~L1205</a></p>
<pre><code>         except ClientError as e:
             # ValidationException can also happen if the package group name contains invalid char,
             # so we have to look at the error message too
-            if e.response[&quot;Error&quot;][&quot;Code&quot;] == &quot;ValidationException&quot; and e.response[&quot;Error&quot;][
-                &quot;Message&quot;
-            ].startswith(&quot;Model Package Group already exists&quot;):
+            if (
+                e.response[&quot;Error&quot;][&quot;Code&quot;] == &quot;ValidationException&quot;
+                and e.response[&quot;Error&quot;][&quot;Message&quot;].startswith(&quot;Model Package Group already exists&quot;)
+            ):
                 # log msg only so it doesn&#x27;t look like an error
                 self.log.info(&quot;%s&quot;, e.response[&quot;Error&quot;][&quot;Message&quot;])
                 return False
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/providers/amazon/tests/system/amazon/aws/example_bedrock_retrieve_and_generate.py#L216">providers/amazon/tests/system/amazon/aws/example_bedrock_retrieve_and_generate.py~L216</a></p>
<pre><code>     :param collection_name: The name of the Collection to create.
     &quot;&quot;&quot;
     log.info(&quot;\nCreating collection: %s.&quot;, collection_name)
-    return aoss_client.conn.create_collection(name=collection_name, type=&quot;VECTORSEARCH&quot;)[
-        &quot;createCollectionDetail&quot;
-    ][&quot;id&quot;]
+    return aoss_client.conn.create_collection(
+        name=collection_name, type=&quot;VECTORSEARCH&quot;
+    )[&quot;createCollectionDetail&quot;][&quot;id&quot;]
 
 
 @task
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/providers/amazon/tests/unit/amazon/aws/executors/aws_lambda/test_lambda_executor.py#L909">providers/amazon/tests/unit/amazon/aws/executors/aws_lambda/test_lambda_executor.py~L909</a></p>
<pre><code>         ]
         orphaned_tasks[0].external_executor_id = ser_airflow_key_1
         orphaned_tasks[1].external_executor_id = ser_airflow_key_2
-        orphaned_tasks[
-            2
-        ].external_executor_id = None  # One orphaned task has no external_executor_id, not adopted
+        orphaned_tasks[2].external_executor_id = (
+            None  # One orphaned task has no external_executor_id, not adopted
+        )
 
         for task in orphaned_tasks:
             task.try_number = 1
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/providers/amazon/tests/unit/amazon/aws/hooks/test_emr.py#L291">providers/amazon/tests/unit/amazon/aws/hooks/test_emr.py~L291</a></p>
<pre><code> 
         # Fetch a cluster from the second page using the boto API
         client = boto3.client(&quot;emr&quot;, region_name=&quot;us-east-1&quot;)
-        response_marker = client.list_clusters(ClusterStates=[&quot;RUNNING&quot;, &quot;WAITING&quot;, &quot;BOOTSTRAPPING&quot;])[
-            &quot;Marker&quot;
-        ]
+        response_marker = client.list_clusters(
+            ClusterStates=[&quot;RUNNING&quot;, &quot;WAITING&quot;, &quot;BOOTSTRAPPING&quot;]
+        )[&quot;Marker&quot;]
         second_page_cluster = client.list_clusters(
             ClusterStates=[&quot;RUNNING&quot;, &quot;WAITING&quot;, &quot;BOOTSTRAPPING&quot;], Marker=response_marker
         )[&quot;Clusters&quot;][0]
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/providers/apache/hive/tests/unit/apache/hive/hooks/test_hive.py#L140">providers/apache/hive/tests/unit/apache/hive/hooks/test_hive.py~L140</a></p>
<pre><code>             if AIRFLOW_V_3_0_PLUS
             else AIRFLOW_VAR_NAME_FORMAT_MAPPING[&quot;AIRFLOW_CONTEXT_EXECUTION_DATE&quot;][&quot;env_var_format&quot;]
         )
-        dag_run_id_ctx_var_name = AIRFLOW_VAR_NAME_FORMAT_MAPPING[&quot;AIRFLOW_CONTEXT_DAG_RUN_ID&quot;][
-            &quot;env_var_format&quot;
-        ]
+        dag_run_id_ctx_var_name = (
+            AIRFLOW_VAR_NAME_FORMAT_MAPPING[&quot;AIRFLOW_CONTEXT_DAG_RUN_ID&quot;][&quot;env_var_format&quot;]
+        )
         date_key = &quot;logical_date&quot; if AIRFLOW_V_3_0_PLUS else &quot;execution_date&quot;
         mock_output = [
             &quot;Connecting to jdbc:hive2://localhost:10000/default&quot;,
</code></pre>
<p><a href="https://github.com/apache/airflow/blob/cdecb7efbb3092e6c15ae5476436aa12add90fb8/providers/apache/hive/tests/unit/apache/hive/hooks/test_hive.py#L847">providers/apache/hive/tests/unit/apache/hive/hooks/test_hive.py~L847</a></p>
<pre><code>             if AIRFLOW_V_3_0_PLUS
             else AIRFLOW_VAR_NAME_FORMAT_MAPPING[&quot;AIRFLOW_CONTEXT_EXECUTION_DATE&quot;][&quot;env_var_format&quot;]
         )
-        dag_run_id_ctx_var_name = AIRFLOW_VAR_NAME_FORMAT_MAPPING[&quot;AIRFLOW_CONTEXT_DAG_RUN_ID&quot;][
-            &quot;env_var_format&quot;
-        ]
+        dag_run_id_ctx_var_name = (
+            AIRFLOW_VAR_NAME_FORMAT_MAPPING[&quot;AIRFLOW_CONTEXT_DAG_RUN_ID&quot;][&quot;env_var_format&quot;]
+        )
 
         with mock.patch.dict(
             &quot;os.environ&quot;,
</code></pre>
</p>

<a href="https://github.com/apache/superset">apache/superset</a> (+54 -54 lines across 13 files)
<p>
<pre>ruff format --preview</pre>
</p>
<p>

<p><a href="https://github.com/apache/superset/blob/169d27c9e974d045c36767ef30bfbc0af30648ac/superset/commands/dashboard/importers/v0.py#L199">superset/commands/dashboard/importers/v0.py~L199</a></p>
<pre><code>             &quot;expanded_slices&quot; in i_params_dict
             and old_slc_id_str in i_params_dict[&quot;expanded_slices&quot;]
         ):
-            new_expanded_slices[new_slc_id_str] = i_params_dict[&quot;expanded_slices&quot;][
-                old_slc_id_str
-            ]
+            new_expanded_slices[new_slc_id_str] = (
+                i_params_dict[&quot;expanded_slices&quot;][old_slc_id_str]
+            )
 
     # since PR #9109, filter_immune_slices and filter_immune_slice_fields
     # are converted to filter_scopes
</code></pre>
<p><a href="https://github.com/apache/superset/blob/169d27c9e974d045c36767ef30bfbc0af30648ac/superset/commands/importers/v1/utils.py#L166">superset/commands/importers/v1/utils.py~L166</a></p>
<pre><code> 
                 # populate ssh_tunnel_private_keys from the request or from existing DBs
                 if file_name in ssh_tunnel_private_keys:


... (truncated 5498 lines) ...

</code></pre>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 19:22:57 UTC
    </footer>
</body>
</html>
