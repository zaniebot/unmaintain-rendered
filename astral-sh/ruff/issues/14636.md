```yaml
number: 14636
title: New lint rules for Strength Reduction optimizations
type: issue
state: open
author: tdulcet
labels:
  - rule
  - needs-decision
assignees: []
created_at: 2024-11-27T16:01:25Z
updated_at: 2025-07-24T15:05:25Z
url: https://github.com/astral-sh/ruff/issues/14636
synced_at: 2026-01-10T01:56:54Z
```

# New lint rules for Strength Reduction optimizations

---

_Issue opened by @tdulcet on 2024-11-27 16:01_

Please consider adding new lint rules for [strength reduction](https://en.wikipedia.org/wiki/Strength_reduction) optimizations. All of the types of optimizations included in the below examples would improve the performance of Python code, some significantly, especially when the numbers are large. Ruff is fast, so lets help make Python fast as well! A few of the examples would also improve the accuracy/precision of the expression. They should hopefully not decrease the readability of the code and in a few cases might even improve it by simplifying the expressions. Most of them are language agnostic, but they are particularly important in Python, as there is of course typically no compiler to perform the optimizations automatically (at least in CPython). The indented ones are just additional examples of what the proposed new lint rules could fix.

### Power of 2 integer optimizations
```py
x * 2                        # â‡’ x + x or x << 1
    x * 8                    # â‡’ x << 3
x // 2                       # â‡’ x >> 1
    x // 8                   # â‡’ x >> 3
x % 2                        # â‡’ x & 1
    x % 8                    # â‡’ x & 7
2 ** x                       # â‡’ 1 << x
    pow(2, x)
    y * 2 ** x               # â‡’ y << x
    8 ** 3                   # â‡’ 8 << 6 or 1 << 9
```
### Exponentiation and Modular optimizations
```py
(x * x) % y                  # â‡’ pow(x, 2, y)
    (x ** 2) % y
    (x * x * x) % y          # â‡’ pow(x, 3, y)
    (x ** 3) % y
    pow(x, 3) % y
x ** 2                       # â‡’ x * x
    pow(x, 2)
pow(x, y)                    # â‡’ x ** y
q, r = x // y, x % y         # â‡’ q, r = divmod(x, y)
    q = x // y; r = x % y
len(bin(x)[2:])              # â‡’ x.bit_length() # Python 3.1+
    len(bin(x).lstrip("-0b"))
```
Also see the `FURB161` rule.
### Math library related, improves accuracy
```py
math.ceil(x / y)             # â‡’ -(x // -y) # int only
int(math.sqrt(x))            # â‡’ math.isqrt(x) # Python 3.8+
x ** 0.5                     # â‡’ math.sqrt(x)
    x ** (1 / 2)
    pow(x, 0.5)
    math.pow(x, 0.5)
x ** (1 / 3)                 # â‡’ math.cbrt(x) # Python 3.11+
    pow(x, 1 / 3)
    math.pow(x, 1 / 3)
math.e ** x                  # â‡’ math.exp(x)
    pow(math.e, x)
    math.pow(math.e, x)
2.0 ** x                     # â‡’ math.exp2(x) # float only # Python 3.11+
    pow(2.0, x)
    math.pow(2.0, x)
math.exp(x) - 1              # â‡’ math.expm1(x) # Python 3.2+
    (math.e ** x) - 1
    pow(math.e, x) - 1
    math.pow(math.e, x) - 1
x % y                        # â‡’ math.fmod(x, y) # float only
(x * y) + z                  # â‡’ math.fma(x, y, z) # float only # Python 3.13+
math.pi * 2                  # â‡’ math.tau # Python 3.6+
functools.reduce(operator.mul, (x, y, z), 1) # â‡’ math.prod((x, y, z)) # Python 3.8+
```
These are similar to the existing rule `FURB163`.
### Mathematical identities

- [ ] #19518

May be useful in combination with other autofixes to simplify expressions or when refactoring
```py
x * -1               # â‡’ -x
x / -1.0             # â‡’ -x # float only
x // -1              # â‡’ -x # int only
x * 0                # â‡’ 0
x * 1                # â‡’ x
x + 0                # â‡’ x
x - 0                # â‡’ x
x - -y               # â‡’ x + y
x + -y               # â‡’ x - y
+x                   # â‡’ x
x / 1.0              # â‡’ x # float only
x // 1               # â‡’ x # int only
x ** 0               # â‡’ 1
x ** 1               # â‡’ x
x / y / z            # â‡’ x * z / y # float only 
x / 0.0              # â‡’ ERROR!
x // 0               # â‡’ ERROR!
x % 0                # â‡’ ERROR!
```
Using [DeMorganâ€™s theorem](https://en.wikipedia.org/wiki/De_Morgan%27s_laws) to further simplify boolean expressions, similar to the [simplify-boolean-expr](https://clang.llvm.org/extra/clang-tidy/checks/readability/simplify-boolean-expr.html) rule in Clang Tidy, would be very helpful here. Also see #15447 and https://github.com/tonybaloney/perflint/issues/50.
### Opinionated, but improves performance
```py
# Replace division with multiplication of the reciprocal
x / 2.0                      # â‡’ x * 0.5 # float only
# Prefer separate lines for assignments, unless tuple unpacking or the expressions depend on each other
x, y = x + z, a * b          # â‡’ x += z; y = a * b
# Unnecessary trivial comprehensions
(y for y in x)               # â‡’ iter(x)
(func(y) for y in x)         # â‡’ map(func, x)
(func(*z) for z in zip(x, y))# â‡’ map(func, x, y)
(y for y in x if y)          # â‡’ filter(None, x)
(y for y in x if not y)      # â‡’ itertools.filterfalse(None, x)
(y for y in x if func(y))    # â‡’ filter(func, x)
(y for y in x if not func(y))# â‡’ itertools.filterfalse(func, x)
# Precompute small constant expressions
60 * 60 * 24                 # â‡’ 86400
(1 << 32) - 1                # â‡’ 0xFFFFFFFF
"0" * 8                      # â‡’ "00000000"
# Use float literals in exponential notation
10.0 ** 3.0                  # â‡’ 1e3 # float only
2.0 * 10.0 ** 3.0            # â‡’ 2e3
```
Also see the `FURB140` rule and https://github.com/adamchainz/flake8-comprehensions/issues/503.

---
All of these should also work with `PLR6104` (#8877), so for example `x * 2` above should also catch `x *= 2` and offer to replace it with `x <<= 1`. There are of course more examples, but these are some of the ones I see most frequently in code reviews. As noted above, some of these may require type inference, but I believe an unsafe autofix would still be acceptable in many cases where the chance of a false positive would be low.

I could see some of these proposed rules being added to the existing `UP`, `PL`, `PERF`, `FURB` and `RUF` categories, among others. Please let me know if you think I should file this request with one of those upstream linters instead.

For a real world example, see this Python function to calculate the Jacobi symbol [before](https://rosettacode.org/wiki/Jacobi_symbol#Python) and [after](https://github.com/tdulcet/AutoPrimeNet/blob/d92313f882ec6dc11e9d79bd55f99ecee2645258/gimps_status.py#L611-L630) manually applying several of these strength reduction optimizations, which approximately doubled the resulting performance of the code.

---

_Comment by @dhruvmanila on 2024-11-28 06:05_

Interesting! Thanks for writing this up with all the examples provided. This would surely bring Ruff closer to being a ["compiler"](https://docs.astral.sh/ruff/contributing/#compilation-pipeline) ;) 

---

_Label `rule` added by @dhruvmanila on 2024-11-28 06:06_

---

_Label `needs-decision` added by @dhruvmanila on 2024-11-28 06:06_

---

_Referenced in [astral-sh/ruff#14835](../../astral-sh/ruff/issues/14835.md) on 2024-12-08 13:25_

---

_Referenced in [astral-sh/ruff#15447](../../astral-sh/ruff/issues/15447.md) on 2025-01-13 00:59_

---

_Comment by @Avasam on 2025-03-04 07:44_

"Power of 2 integer optimizations" I find a *lot* less readable. Bitshifts and masks are not intuitive unless you actively use them regularly.

I'd expect those to be their own rule for when performance really matters. I'm all for "free speed optimizations" even (especially) for slow languages like Python. But these hurt readability a lot imo.

Some require type information that Ruff may not have atm (int vs float).

Everything else I'd love to see anytime!

---

_Comment by @tdulcet on 2025-03-04 12:01_

> I'd expect those to be their own rule for when performance really matters. I'm all for "free speed optimizations" even (especially) for slow languages like Python.

Yes, I imagine each of these categories would include at least one rule. Note that the resulting performance improvement from those power of 2 optimizations can be significant (this is with Python 3.12):
```
$ python3 -m timeit -s 'x = 136279841' '2 ** x'
1 loop, best of 5: 551 msec per loop
$ python3 -m timeit -s 'x = 136279841' '1 << x'
500 loops, best of 5: 741 usec per loop
```
That is **744Ã—** faster (ms to us).
```
$ python3 -m timeit -s 'x = (1 << 136279841) - 1' 'x % 2'
5 loops, best of 5: 44.3 msec per loop
$ python3 -m timeit -s 'x = (1 << 136279841) - 1' 'x & 1'
10000000 loops, best of 5: 33.8 nsec per loop
```
That is **762,980Ã—** faster (ms to ns)! ðŸ¤¯

---

_Comment by @kaddkaka on 2025-06-18 17:55_

> > I'd expect those to be their own rule for when performance really matters. I'm all for "free speed optimizations" even (especially) for slow languages like Python.
> 
> Yes, I imagine each of these categories would include at least one rule. Note that the resulting performance improvement from those power of 2 optimizations can be significant (this is with Python 3.12):
> 
> ```
> $ python3 -m timeit -s 'x = 136279841' '2 ** x'
> 1 loop, best of 5: 551 msec per loop
> $ python3 -m timeit -s 'x = 136279841' '1 << x'
> 500 loops, best of 5: 741 usec per loop
> ```
> 
> That is **744Ã—** faster (ms to us).
> 
> ```
> $ python3 -m timeit -s 'x = (1 << 136279841) - 1' 'x % 2'
> 5 loops, best of 5: 44.3 msec per loop
> $ python3 -m timeit -s 'x = (1 << 136279841) - 1' 'x & 1'
> 10000000 loops, best of 5: 33.8 nsec per loop
> ```
> 
> That is **762,980Ã—** faster (ms to ns)! ðŸ¤¯

I'm not sure how fair this comparison is. Such large numbers must be very rare? 

---

_Comment by @tdulcet on 2025-06-18 19:27_

> I'm not sure how fair this comparison is. Such large numbers must be very rare?

I used this large number to make it less noisy and easier to benchmark. Specifically, it is the [largest known prime number](https://en.wikipedia.org/wiki/Largest_known_prime_number). However, it is still much faster even with smaller numbers:
```
$ python3 -m timeit -s 'x = 127' '2 ** x'
1000000 loops, best of 5: 226 nsec per loop
$ python3 -m timeit -s 'x = 127' '1 << x'
10000000 loops, best of 5: 34.2 nsec per loop
```
That is 6.8Ã— faster.

---
