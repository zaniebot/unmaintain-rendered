<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[Feature Request] [NPY] Detect accidental cubic runtime cost due to `numpy.trace` - astral-sh/ruff #9664</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>[Feature Request] [NPY] Detect accidental cubic runtime cost due to <code>numpy.trace</code></h1>

    <div class="meta">
        <span class="state-icon state-open"></span>
        <a href="https://github.com/astral-sh/ruff/issues/9664">#9664</a>
        opened by <a href="https://github.com/randolf-scholz">@randolf-scholz</a>
        on 2024-01-29 00:56
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/randolf-scholz">@randolf-scholz</a></div>
            <div class="timeline-body"><p>There is a very common mistake people make in numerical code that incurs an accidental cubic runtime cost: They implement <code>np.trace(A @ B)</code>, which costs $O(n^3)$ instead of the equivalent <code>np.tensordot(A.T, B)</code> which costs $O(n^2)$, or <code>np.einsum</code> if batch dimensions are involved. (see <a href="https://en.wikipedia.org/wiki/Frobenius_inner_product">Frobenius Inner Product</a>)</p>
<p><a href="https://github.com/search?q=np.trace+%40&amp;type=code">GitHub is full of this issue</a>, but I don&#x27;t blame people, since book authors should take more care to point this out. This not just affects <code>numpy</code>, but basically all related numerical libraries (<code>jax</code>, <code>torch</code>, <code>tensorflow</code>, etc.)</p>
<p>Basically, one needs to look out for <code>trace</code> being used with a matrix multiplication inside. A simple check would look something like:</p>
<pre><code>match node:
    case Call(
        func=Attribute(value=Name(id=&quot;np&quot;), attr=&quot;trace&quot;),
        args=[BinOp(op=MatMult()) | Call(func=Attribute(attr=&quot;dot&quot;))],
    ):
        # use np.tensordot instead.
</code></pre>
<p>One can probably make a more clever version that also detect cases like <code>np.trace(2*(A@B))</code>.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">rule</span> added by <a href="https://github.com/charliermarsh">@charliermarsh</a> on 2024-01-29 00:59</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/randolf-scholz">@randolf-scholz</a> on 2024-01-29 01:52</div>
            <div class="timeline-body"><p>Probably one should be a bit conservative here, since transposing can also incur a surprisingly large cost, and only flag cases where one doesn&#x27;t need to transpose. A few benchmarks:</p>
<p>|                        | 4       | 16      | 64      | 256     | 512    | 1024   | 2048   |
|------------------------|---------|---------|---------|---------|--------|--------|--------|
| <code>np.trace(A @ B)</code>      | 3.78 µs | 4.37 µs | 17.9 µs | 376 µs  | 2.2 ms | 11 ms  | 66 ms  |
| <code>np.trace(A.T @ B)</code>    | 3.93 µs | 4.47 µs | 17.4 µs | 482 µs  | 2.2 ms | 10 ms  | 76 ms  |
| <code>np.tensordot(A, B)</code>   | 12.8 µs | 12.8 µs | 12.9 µs | 47.4 µs | 44 µs  | 76 µs  | 471 µs |
| <code>np.tensordot(A.T, B)</code> | 13.3 µs | 13.7 µs | 15.9 µs | 511 µs  | 1.6 ms | 6.8 ms | 42 ms  |</p>
<p><code>np.tensordot(A, B)</code> has some overhead for small inputs, but is orders of magnitude so for large inputs. <code>np.tensordot(A.T, B)</code> is a bit faster than <code>np.trace(A @ B)</code> for large inputs, but the transposition overhead hurts.</p>
<p>So, the bad cases one wants to find are:</p>
<ol>
<li>Basic case: <code>np.trace(A.T.dot(B))</code>, <code>np.trace(A.T @ B)</code>, <code>np.trace(A.dot(B.T))</code>, <code>np.trace(A @ B.T)</code> ⇝ use <code>np.tensordot(A, B)</code></li>
<li>Basic case with scalar: <code>np.trace(c*A.T.dot(B))</code> or <code>np.trace(A.T.dot(c*B))</code>, etc. ⇝ use <code>c*np.tensordot(A, B)</code></li>
<li>More complex cases: such as this one <a href="https://github.com/statsmodels/statsmodels/blob/23faea30e30ff759c3c9d3f1d57864b2aa68c827/statsmodels/tsa/vector_ar/vecm.py#L2281"><code>np.trace(ct.T @ c0_inv @ ct @ c0_inv)</code></a> are likely much harder to detect without also generating false positives. (should be <code>tensordot(z, z)</code> with <code>z=c0_inv @ ct</code>, but requires knowledge that <code>c0_inv</code> is symmetric)</li>
</ol>
<p>The last one also does <a href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">another sin, they invert</a> the <code>sigma_u</code> matrix, which is usually bad practice. In this example, one can make the whole thing $O(n^2)$ by using Cholesky factorization instead of inverting and tensordot instead of trace.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/randolf-scholz">@randolf-scholz</a> on 2024-07-18 12:29</div>
            <div class="timeline-body"> Reran the script with torch==2.3.1 and numpy==2.0.0 

<pre><code>import torch as pt
N = 64
A = pt.randn(N, N)
B = pt.randn(N, N)
%timeit pt.trace(A @ B)
%timeit pt.trace(A.T @ B)
%timeit pt.tensordot(A, B)
%timeit pt.tensordot(A.T, B)
</code></pre>
<pre><code>import numpy as np
N = 64
A = np.random.randn(N, N)
B = np.random.randn(N, N)
%timeit np.trace(A @ B)
%timeit np.trace(A.T @ B)
%timeit np.tensordot(A, B)
%timeit np.tensordot(A.T, B)
</code></pre>


<p>Results:</p>
<p>|                      | 64      | 256    | 1024    | 2048   |
|----------------------|--------:|-------:|--------:|-------:|
| np.trace(A @ B)      | 11.7 μs | 852 μs | 6730 μs | 108 ms |
| np.trace(A.T @ B)    | 11.9 μs | 971 μs | 7670 μs | 115 ms |
| np.tensordot(A, B)   | 10.9 μs | 23 μs  |   90 μs | 2 ms   |
| np.tensordot(A.T, B) | 13.5 μs | 120 μs | 2920 μs | 44 ms  |
| pt.trace(A @ B)      | 6.3 μs  | 51 μs  | 2610 μs | 22 ms  |
| pt.trace(A.T @ B)    | 7.9 μs  | 53 μs  | 2650 μs | 23 ms  |
| pt.tensordot(A, B)   | 30.5 μs | 269 μs | 4040 μs | 16 ms  |
| pt.tensordot(A.T, B) | 38.8 μs | 309 μs | 4740 μs | 19 ms  |</p>
<p>In <code>numpy</code> the overhead is patched, but <code>torch</code> is slower until very large arrays. I&#x27;ll open an issue.</p>
</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 18:48:49 UTC
    </footer>
</body>
</html>
