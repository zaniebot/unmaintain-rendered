<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adding new class of static analysis rules using LLMs - astral-sh/ruff #14085</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>Adding new class of static analysis rules using LLMs</h1>

    <div class="meta">
        <span class="state state-open">Open</span>
        <a href="https://github.com/astral-sh/ruff/issues/14085">#14085</a>
        opened by <a href="https://github.com/bbkgh">@bbkgh</a>
        on 2024-11-04 05:56
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header">Issue opened by <a href="https://github.com/bbkgh">@bbkgh</a> on 2024-11-04 05:56</div>
            <div class="timeline-body"><p>Hi, I would like to suggest adding a new class of rules to Ruff that utilizes Local/Remote LLM APIs for running rules over code files. I believe this can greatly improve code quality and result in fewer bugs. Additionally, writing rules in plain text is more convenient. For example, we could add a llm_rules.yaml file to project containing something like this:</p>
<pre><code class="language-yaml">type: ollama/qwen2.5
endpoint: http://localhost:11434
rules:
  - &quot;There must not be any usage of datetime.now(); use timezone.now() from django.utils instead&quot;
  - &quot;There must not be any Typos in comments&quot;
  - &quot;Always use MyCustomObject for calling ServiceX&quot;
</code></pre>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/MichaReiser">@MichaReiser</a> on 2024-11-04 08:30</div>
            <div class="timeline-body"><p>Using LLMs for analysis is certainly interesting but I'm not sure if asking the LLM to do the analysis is the solution because LLMs are much slower than regular rules. Instead, we should explore if it's possible for the LLM to derive the necessary checks once ahead of time that Ruff can then run as part of its engine. This also removes any need to directly integrate with an LLVM in ruff itself.</p>
<p>I also think LLMs should be used very sparsely. E.g. the first rule is already covered by Ruff.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">wish</span> added by @MichaReiser on 2024-11-04 08:30</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">needs-design</span> added by @MichaReiser on 2024-11-04 08:30</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-09 23:34:54 UTC
    </footer>
</body>
</html>
