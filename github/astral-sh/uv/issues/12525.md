---
number: 12525
title: "concurrent `uv run` is very slow even with cache"
type: issue
state: open
author: terrykong
labels:
  - performance
assignees: []
created_at: 2025-03-28T06:38:33Z
updated_at: 2025-04-01T22:16:47Z
url: https://github.com/astral-sh/uv/issues/12525
synced_at: 2026-01-07T12:31:15-06:00
---

# concurrent `uv run` is very slow even with cache

---

_Issue opened by @terrykong on 2025-03-28 06:38_

### Summary

Hi team. I've been playing around with `ray`'s new [uv+ray integration](https://www.anyscale.com/blog/uv-ray-pain-free-python-dependencies-in-clusters) and I've been unable to figure out why `uv` is so slow when there are many concurrent actors launched on a ray cluster.

Here is a simple script I've been using to benchmark creating multiple `.venv`s which is my best guess of what's happening on the ray cluster:

```sh
#!/bin/bash

SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
cd $SCRIPT_DIR

if [[ ! -f /usr/bin/time ]]; then
  apt update && apt install -y time
fi

echo "Cleaning up workspaces"
find workspaces -type d -exec rm -rf {} + 2>/dev/null || rm -rf workspaces
mkdir -p workspaces

N=1
foobar() {
for i in $(seq 1 $N); do
    # Create a temporary directory for this iteration
    TEMP_DIR="${SCRIPT_DIR}/workspaces/${i}"
    mkdir -p "$TEMP_DIR"
    cd "$TEMP_DIR"
    rsync -ah $SCRIPT_DIR/pyproject.toml $SCRIPT_DIR/uv.lock $SCRIPT_DIR/foobar $TEMP_DIR/
    uv venv
    /usr/bin/time -v uv run -v echo helloworld 2>&1 | while IFS= read -r line; do echo "[$(date '+%Y-%m-%d %H:%M:%S')] $line"; done | tee $SCRIPT_DIR/workspaces/log-$i &
done

echo waiting....
wait
}
export -f foobar

time foobar 2>&1 | tee $SCRIPT_DIR/workspaces/log
echo "done!"

echo cleaning up...
for i in $(seq 1 $N); do
    ( rm -rf $SCRIPT_DIR/workspaces/${i} ; echo deleted $i ) &
done
echo 'waiting.... for cleanup'
wait
echo 'alllll done'
```

When I run with `N=1` it takes about 3.72 seconds to execute the `uv run` command, but when I set `N=32` all the `uv run`s take about 56 seconds to run which seems to suggest they are all waiting on something? The debug logs also show a large pause here (between `23:12:47` and ` 23:13:28`):
```
[2025-03-27 23:12:44] DEBUG File already exists (initial attempt), overwriting: /test-workspaces/workspa
ces/21/.venv/lib/python3.12/site-packages/nvidia/__init__.py
[2025-03-27 23:12:44] DEBUG File already exists (subsequent attempt), overwriting: /test-workspaces/work
spaces/21/.venv/lib/python3.12/site-packages/nvidia/__init__.py
[2025-03-27 23:12:47] DEBUG File already exists (subsequent attempt), overwriting: /test-workspaces/work
spaces/21/.venv/lib/python3.12/site-packages/opencensus/common/__init__.py
[2025-03-27 23:13:28] Installed 149 packages in 51.85s
[2025-03-27 23:13:28]  + aiohappyeyeballs==2.6.1
[2025-03-27 23:13:28]  + aiohttp==3.11.14
```

Here is the pyproject.toml for this dummy `foobar` package:

```toml
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "foobar"
requires-python = ">=3.10"
version = "0.0.1"
dependencies = [
"torch==2.6.0",
"transformers",
"vllm",
"ray[default]==2.43.0",
]

[tool.setuptools]
packages = ["foobar"]

[project.optional-dependencies]
emoji = [
    "emoji",
]
pyfiglet = [
    "pyfiglet",
]

# NOTE: using symlink b/c ray temp-dir is on diff file system than uv-cache
[tool.uv]
link-mode = "symlink"
```

I'm very interested in using `uv` with ray, but the user experience for folks launching with ray isn't ideal since it takes minutes to just start the job despite having all the dependencies available in the uv cache.

Is this performance expected?

### Platform

Ubuntu 22.04.5 LTS (Jammy Jellyfish)

### Version

uv 0.6.8

### Python version

Python 3.12.9

---

_Label `bug` added by @terrykong on 2025-03-28 06:38_

---

_Comment by @charliermarsh on 2025-03-28 19:18_

Are you sure it's not just contention on your machine from running 32 tasks in parallel? (Also, if you already have a lockfile, you probably want to be using `--frozen` there.)

---

_Comment by @terrykong on 2025-03-28 20:52_

@charliermarsh Thanks for the `--frozen` tip, setting it does seem to make things a tiny bit better, but still has this large pause.

Regarding the contention, does the performance I observe match your expectations? Conceptually if I symlink the packages required to build a venv assuming everything is cached, shouldn't this be much quicker? Is there something about how `uv` is implemented that causes this contention?

<img width="1141" alt="Image" src="https://github.com/user-attachments/assets/858b5f48-e1ea-4b6d-af49-6a0788885c32" />
I did notice quite a bit of context switches even running one `uv run` (left is running 1; right is running 10 in parallel)

FWIW, the ray docs have this simple example:
```python
import emoji
import ray

@ray.remote
def f():
    return emoji.emojize('Python is :thumbs_up:')
# Execute 1000 copies of f across a cluster.

print(ray.get([f.remote() for _ in range(1000)]))
```
which is actually much worse than my example (1000 venvs...) assuming the same dependencies (which are common for ML applications)

---

_Comment by @charliermarsh on 2025-03-28 20:57_

> Regarding the contention, does the performance I observe match your expectations?

Hmm, not really. There's nothing in uv that should be causing this. It should just be linking from the cache into the various environments. I can try to take a closer look at another time.


---

_Label `bug` removed by @charliermarsh on 2025-03-28 20:57_

---

_Label `question` added by @charliermarsh on 2025-03-28 20:57_

---

_Label `performance` added by @charliermarsh on 2025-03-28 20:57_

---

_Comment by @terrykong on 2025-03-28 21:03_

Thanks, much appreciated! If there's any other data points that I can gather that would help, please let me know

---

_Label `question` removed by @zanieb on 2025-04-01 22:16_

---

_Referenced in [ray-project/ray#50961](../../ray-project/ray/issues/50961.md) on 2025-04-25 06:37_

---
