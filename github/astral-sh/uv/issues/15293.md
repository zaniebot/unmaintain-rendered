---
number: 15293
title: UV fails a PySpark resolution
type: issue
state: closed
author: tigerhawkvok
labels:
  - needs-mre
assignees: []
created_at: 2025-08-14T23:11:44Z
updated_at: 2025-08-15T18:18:37Z
url: https://github.com/astral-sh/uv/issues/15293
synced_at: 2026-01-07T12:31:16-06:00
---

# UV fails a PySpark resolution

---

_Issue opened by @tigerhawkvok on 2025-08-14 23:11_

### Summary

Consider this constraint:

`"pyspark~=3.5.2 ; python_version ~= '3.12.0'"`

This is derived from the Databricks runtime release: https://docs.databricks.com/aws/en/release-notes/runtime/16.4lts#system-environment

> ## Apache Spark
> Databricks Runtime 16.4 LTS includes Apache Spark 3.5.2. [...]
> 
> ## System environment
>
>  [...]
> - Python: 3.12.3

The `setup.cfg` for 3.5.2 and 3.5.6 both allow Python 3.12.x: 

https://github.com/apache/spark/blob/bb7846dd487f259994fdc69e18e03382e3f64f42/python/setup.py#L331

https://github.com/apache/spark/blob/303c18c74664f161b9b969ac343784c088b47593/python/setup.py#L331

but the resolver claims:


```
  × No solution found when resolving dependencies for split (python_full_version == '3.12.*'):                                                                                                                                              
  ╰─▶ Because pyhelpers:databricks depends on pyspark{python_full_version == '3.12.*'}==3.5.6 and pyspark{python_full_version == '3.12.*'}==4.0.0, we can conclude that pyhelpers:databricks's requirements are unsatisfiable.
      And because your project requires pyhelpers:databricks, we can conclude that your project's requirements are unsatisfiable.
```

which, quite clearly, is wrong.

### Platform

windows x64

### Version

uv 0.7.21 (77c771c7f 2025-07-14)

### Python version

Python 3.10.6 (for this build)

---

_Label `bug` added by @tigerhawkvok on 2025-08-14 23:11_

---

_Comment by @charliermarsh on 2025-08-14 23:13_

Can you share your `pyproject.toml`? I believe that error is saying that the `databricks` group depends on both `pyspark==3.5.6` and `pyspark==4.0.0`.

---

_Label `needs-mre` added by @charliermarsh on 2025-08-14 23:13_

---

_Comment by @tigerhawkvok on 2025-08-14 23:27_

I used `migrate-to-uv` to go from this Poetry resolution (that works):


```
pyspark = [
    # We can't support Py3.9 due to `match` statements and issues with
    # Mosaic 0.3.11 builds on Databricks, which bumps minimum DBR to 13.3
    # {version= "~3.3.0", python= "~3.9.0"}, # DBR: 11.3 LTS and DBR: 12.2 LTS
    # {version= "~3.4.0", python= "~3.10.0"}, # DBR: 13.3 LTS # for planning
    # {version= "~3.5.0", python= "~3.10.0"}, # DBR: 14.3 LTS # for planning
    {version= ">=3.4.0, <3.6", python= "~3.10.0"},
    {version= "~3.5.0", python= "~3.11.0"}, # DBR: 15.4 LTS
    {version= "~3.5.2", python= "~3.12.0"}, # DBR: 16.4 LTS
]
```

to this one in uv-land

```
requires-python = ">=3.10,<3.14"
readme = "README.md"
dependencies = [
    "numpy>=1.26.4,<2",
    "pandas[plot, output-formatting, performance, excel, xml, postgresql, sql-other, parquet, feather, aws, compression]>=1.4.0,<2.1",
    "pyarrow>=16",
    "py4j>=0.10.9.5",
    "pyspark>=3.4.0, <3.6 ; python_version ~= '3.10.0'",
    "pyspark~=3.5.0 ; python_version ~= '3.11.0'",
    "pyspark~=3.5.2 ; python_version ~= '3.12.0'",
    "delta-spark>=2.1.0,<3 ; python_version ~= '3.9.0'",
    "delta-spark>=2.1.0 ; python_version >= '3.10'",
    "typing-extensions>=4.8.0",
    "setuptools>=60",
    "shapely~=2.0.6",
    "folium~=0.20.0",
    "toml>=0.10.2,<0.11",
]
```

For Python 3.13+ this might go to pyspark 4 (unclear as there's no LTS runtime yet, but some coworkers use this locally and there's no issue with spark compatibility there), but that shouldn't affect the 3.12 resolution.

---

_Comment by @charliermarsh on 2025-08-15 09:20_

That resolution resolved for me without error. I used this `pyproject.toml`, then ran `uv lock`:

```toml
[project]
name = "foo"
version = "0.1.0"
requires-python = ">=3.10,<3.14"
readme = "README.md"
dependencies = [
    "numpy>=1.26.4,<2",
    "pandas[plot, output-formatting, performance, excel, xml, postgresql, sql-other, parquet, feather, aws, compression]>=1.4.0,<2.1",
    "pyarrow>=16",
    "py4j>=0.10.9.5",
    "pyspark>=3.4.0, <3.6 ; python_version ~= '3.10.0'",
    "pyspark~=3.5.0 ; python_version ~= '3.11.0'",
    "pyspark~=3.5.2 ; python_version ~= '3.12.0'",
    "delta-spark>=2.1.0,<3 ; python_version ~= '3.9.0'",
    "delta-spark>=2.1.0 ; python_version >= '3.10'",
    "typing-extensions>=4.8.0",
    "setuptools>=60",
    "shapely~=2.0.6",
    "folium~=0.20.0",
    "toml>=0.10.2,<0.11",
]
```

```shell
❯ uv lock
Using CPython 3.13.6
Resolved 74 packages in 25.17s
```

Anything I might be missing, to reproduce?


---

_Label `bug` removed by @charliermarsh on 2025-08-15 09:20_

---

_Comment by @tigerhawkvok on 2025-08-15 17:43_

I came back this morning and suddenly it was working. I blame gremlins.

~~I'll end up not using uv for this project as [UV doesn't support wheel inclusions](https://docs.astral.sh/uv/concepts/build-backend/#file-inclusion-and-exclusion) and multi-root doesn't play nicely with workspaces; so I can't wholesale include a `git subtree` as a co-equal member, which [Poetry's package directive](https://python-poetry.org/docs/pyproject/#packages) handles nicely.~~

~~This is a moderately common pattern for me (a sub-project that should be entirely self-contained and runnable, but 90% of the time will be used co-equally / as a dependency of a greater project) that requires the "workspace" object to export when building, though perhaps~~

```
[tool.uv.build-backend]
data = { purelib = "path/to/subtree/src/code" }
```

~~will work, though I'll need to play with that .... seems that specifying a directory includes only that directory's _members_ , rather than the directory itself~~

Welp I didn't realize I can still use the `poetry_core` backend to build, so that's delightful.

---

_Closed by @tigerhawkvok on 2025-08-15 17:43_

---
