---
number: 16115
title: "Dependencies for irrelevant platform markers should be ignored during `uv sync`"
type: issue
state: closed
author: mmisiewicz
labels:
  - bug
assignees: []
created_at: 2025-10-03T14:59:22Z
updated_at: 2025-11-07T17:17:46Z
url: https://github.com/astral-sh/uv/issues/16115
synced_at: 2026-01-07T12:31:16-06:00
---

# Dependencies for irrelevant platform markers should be ignored during `uv sync`

---

_Issue opened by @mmisiewicz on 2025-10-03 14:59_

### Summary

I have a pyproject.toml that supports a project which depends on CUDA Torch. This project needs to work on both amd64 and arm64 hosts. However, torch builds with cuda support for arm64 are not published to pypi, so we host a wheel on an internal server (x86-64 _is_ pushed to pypi available to the internet). I have the following pyproject.toml: 

```
[project]
name = "footer"
version = "0.1.0"
description = "foo"
readme = "README.md"
requires-python = "==3.12.*"
dependencies = [
    "fastapi[standard]>=0.115.6",
    "rich>=13.9.4",
    "sentence-transformers>=3.3.1",
    "starlette-compress>=1.4.0",
    "torch>=2.5.1",
]

# Torch needs to be installed from the nightly repo, since CUDA ARM builds are not currently being pushed to pypi.
[tool.uv.sources]
torch = [
    # gets nightly pytoch cuda for arm linux machines, everyone else uses the regular stable
    { url = "http://internal-server.net/bin/torch-2.5.0a0+gita8d6afb-cp312-cp312-linux_aarch64.whl", marker = "sys_platform == 'linux' and platform_machine == 'aarch64'" },
]

[[tool.uv.index]]
name = "pytorch-nightly"
url = "https://download.pytorch.org/whl/nightly/cu126"
explicit = true
```

The issue is, our host `internal-server.net` went down and exposed the following issue while running `uv sync`:
```
14.69   Caused by: Failed to fetch: `http://internal-server.net/bin/torch-2.5.0a0+gita8d6afb-cp312-cp312-linux_aarch64.whl`
14.69   Caused by: Request failed after 3 retries
14.69   Caused by: error sending request for url (http://internal-server.net/bin/torch-2.5.0a0+gita8d6afb-cp312-cp312-linux_aarch64.whl)
14.69   Caused by: client error (Connect)
14.69   Caused by: tcp connect error
14.69   Caused by: Host is unreachable (os error 113)
------

```

Given that the sync command is running on an amd64 platform, why are dependencies for arm64 being retrieved at all? Since this wheel is about 1GiB size, it's a significant resource draw, and not necessary at all. The offline server exposed this issue. 

Can platform markers be checked before downloading remote wheels? 

### Platform

Ubuntu  Linux 5.15.0-140-generic x86_64 GNU/Linux 

### Version

0.8.22

### Python version

3.12

---

_Label `bug` added by @mmisiewicz on 2025-10-03 14:59_

---

_Comment by @charliermarsh on 2025-11-01 21:03_

In order to create, update, or validate a `uv.lock` file, uv needs to be able to access all of the declared dependencies to extract their metadata -- even those that won't ultimately be installed. So it's correct for uv to look in the `internal-server.net` registry even on non-ARM machines, as the lockfile is intended to be used across _all_ platforms.

If you create a `uv.lock` and check it in, you can (e.g.) install on a non-ARM machine using `uv sync --frozen` and we won't hit that index. In other words, if the lockfile already exists, uv can use it and doesn't require access to dependencies it won't install; but to _create_ (or update, or validate) the lockfile, we need access to all of them.

As an aside, it's not inherently necessary to download entire wheels just to determine their dependencies. If your server supports [PEP 658](https://peps.python.org/pep-0658/), uv can just fetch the `http://internal-server.net/bin/torch-2.5.0a0+gita8d6afb-cp312-cp312-linux_aarch64.whl.metadata` file; or if your server supports range requests, uv can extract it with range requests. (For example, all of PyPI supports PEP 658, so fetching metadata from PyPI does not require downloading wheels.)

---

_Closed by @charliermarsh on 2025-11-01 21:03_

---

_Comment by @mmisiewicz on 2025-11-07 17:17_

Thank you for the incredibly helpful response @charliermarsh ! 

---
