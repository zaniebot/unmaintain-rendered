---
number: 7004
title: Running out of memory with uv pip install
type: issue
state: open
author: atti92
labels:
  - performance
  - needs-mre
assignees: []
created_at: 2024-09-04T10:39:34Z
updated_at: 2025-11-12T18:06:07Z
url: https://github.com/astral-sh/uv/issues/7004
synced_at: 2026-01-07T12:31:15-06:00
---

# Running out of memory with uv pip install

---

_Issue opened by @atti92 on 2024-09-04 10:39_

We are using uv inside CI/CD, and we are constantly hitting Memory kills by the kubernetes executor. 
We don't really want to increase the memory requests to high because it's not needed outside of uv install and it's costly.
We were trying to set concurrency limits, but that didn't really help.

The memory usage seems to increase to high levels, when we have many-many dependencies, and might also increase with the number of extra-indexes.

I'm primarily asking if there is a way to limit memory usage of uv? 

---

_Comment by @charliermarsh on 2024-09-04 13:17_

I think the best way to limit memory would be to limit the number of concurrent builds. Did that not work?

---

_Label `question` added by @charliermarsh on 2024-09-04 13:17_

---

_Comment by @atti92 on 2024-09-04 13:37_

Even setting all 3 concurrency options to 1 results in almost the same memory spike. Also I think we primarily use pre-built packages.

---

_Label `question` removed by @zanieb on 2024-10-21 21:41_

---

_Label `performance` added by @zanieb on 2024-10-21 21:41_

---

_Comment by @zanieb on 2024-10-21 21:41_

Do you have more details on the memory consumption and a reproduction we could use?

---

_Label `needs-mre` added by @zanieb on 2024-10-21 21:41_

---

_Comment by @atti92 on 2024-10-22 09:03_

Hi, thanks for looking into this.

I will try to put together a publicly available repro, but we are using multiple internal indexes and that might make the problem worse. For now I've run a local install with a bunch of public+internal dependencies, while measuring max memory with `/bin/time -v`. These measurements don't seem to justify the kubernetes memory kills (much lower than the limits), but give a baseline comparison between installs.

Excluding internal dependencies:

**Default configuration**
htop snippet during running:
![image](https://github.com/user-attachments/assets/07e18208-edd5-446d-80cc-feda2ae07f59)
```
Command being timed: "uv pip install --index-strategy first-match --no-cache --reinstall -r requirements.txt"
	User time (seconds): 1.67
	System time (seconds): 2.33
	Percent of CPU this job got: 51%
	Maximum resident set size (kbytes): **115544**
	Minor (reclaiming a frame) page faults: 29839
	Voluntary context switches: 123439
	Involuntary context switches: 479
	File system outputs: 646184
```

After adding a secondary public pypi mirror index (anything should work) `--extra-index-url https://mirrors.sustech.edu.cn/pypi/web/simple
```
Command being timed: "uv pip install --no-cache --reinstall --extra-index-url https://mirrors.sustech.edu.cn/pypi/web/simple -r requirements.txt"
	User time (seconds): 3.33
	System time (seconds): 3.55
	Percent of CPU this job got: 39%
	Maximum resident set size (kbytes): **121448**
	Minor (reclaiming a frame) page faults: 52101
	Voluntary context switches: 150218
	Involuntary context switches: 937
	File system outputs: 670448
```

**Adding --index-strategy:**
```
Command being timed: "uv pip install --no-cache --reinstall --index-strategy unsafe-first-match --extra-index-url https://mirrors.sustech.edu.cn/pypi/web/simple -r requirements.txt"
	User time (seconds): 3.73
	System time (seconds): 3.86
	Percent of CPU this job got: 36%
	Maximum resident set size (kbytes): **169200**
	Minor (reclaiming a frame) page faults: 64064
	Voluntary context switches: 152854
	Involuntary context switches: 1171
	File system outputs: 738080
```


**Adding concurrency limits to 1:**  (makes it really slow.
This reduces the memory use a bit.
![image](https://github.com/user-attachments/assets/4aaa4d57-c1bf-41ee-b8ff-faf7f1e0bc19)
```
export UV_CONCURRENT_DOWNLOADS=1 
export UV_CONCURRENT_BUILDS=1 
export UV_CONCURRENT_INSTALLS=1
Command being timed: "uv pip install --no-cache --reinstall --extra-index-url https://mirrors.sustech.edu.cn/pypi/web/simple -r requirements.txt"
	User time (seconds): 3.57
	System time (seconds): 3.49
	Percent of CPU this job got: 5%
	Maximum resident set size (kbytes): **108892**
	Minor (reclaiming a frame) page faults: 48392
	Voluntary context switches: 158458
	Involuntary context switches: 652
	File system outputs: 670448
```
Most of the memory usage seems to add up during the resolution phase. During download the memory usage creeps higher without sign of a reduction. Adding our local indexes and even more dependencies the memory usage goes up by another 100MB.
using ```--index-strategy``` other than the default seems to increase memory usage with each extra index url added.
Running the install locally results in a 280MB total resident memory, although this usually gets killed by kubernetes with a 1GB limit. (does it add up each thread individually?).

Used requirements.txt file for the public tests:
```
click>=7.0
tabulate>=0.8.9
pyyaml>=6.0
cryptography>=42.0.1
packaging>=21.3
python-keycloak~=4.3.0
azure-identity~=1.17.1
msgraph-sdk~=1.5.4
msal~=1.30.0
dataclasses_json>=0.5.7
semver>=2.13.0
urllib3>=2.0.7
rich-click>=1.7.0
durationpy>=0.7
inquirerpy==0.3.4
pytest>=6.2.5
pytest-cov
pytest-mock>=3.6.1
responses>=0.15.0
respx>=0.21.1
requests-mock>=1.9.3
twill
mkdocs>1.2.4
mkdocs-material
mkdocs-material-extensions
pymdown-extensions
mkdocs-click
```

This is our uv config file in CI/CD:
```
$ cat ${UV_CONFIG_FILE}
concurrent-builds = 1
concurrent-downloads = 2
concurrent-installs = 2
[pip]
python = "python3"
prerelease = "allow"
index-strategy = "unsafe-first-match"
link-mode = "copy"
extra-index-url = [
  "REDACTED",
  "REDACTED",
]
```
And also environment variables are set to those same settings (including UV_EXTRA_INDEX_URL)


---

_Comment by @zanieb on 2024-10-22 11:23_

So from your screenshots it looks like we're consuming ~3 GB of virtual memory and when you limit concurrency it goes down to ~500 MB? I think we care more about the physical amounts though (RES), which look like 96 MB and 93 MB respectively. That doesn't seem particularly low, but I'm not sure it's unreasonable. In your failure cases, it looks like we're consuming around 100-150 MB?

> using --index-strategy other than the default seems to increase memory usage with each extra index url added.

This makes sense, we need to store details about the available packages in more indexes.

I wonder if we should add a special limit to the prefetch job, maybe we'd check less package versions then. Or maybe we can free some versions from our mapping early?

> (does it add up each thread individually?).

All the threads should be sharing memory.


---

_Comment by @atti92 on 2024-10-22 13:39_

During local testing RES reached `280MB`, while the same script gets killed in kubernetes around 50% of the time with 1GB limits. 
No idea why. 

It would be nice if you could reduce the memory footprint or just add some optional hard limits to memory usage / dynamic free during runtime.

---

_Comment by @BugsBuggy on 2025-02-27 12:19_

@atti92 did you find a workaround for this issue?

Installing the requirements with uv worked just fine for me. However, calling 
`uv pip install --no-deps .`
results in an OOM in our Kubernetes environment. 

I already tried to set these env variables but that did not help:
```
UV_CONCURRENT_INSTALLS=1
UV_CONCURRENT_BUILDS=1
UV_CONCURRENT_DOWNLOADS=1
UV_NO_CACHE=true
```

---

_Comment by @atti92 on 2025-02-27 12:24_

@BugsBuggy No I haven't It still regularly OOMs, we increased requests limits to 2Gi

---

_Comment by @BugsBuggy on 2025-02-27 13:16_

@zanieb In addition to the settings above tried different things like `UV_SYSTEM_PYTHON=true` to use the global Python instead of a venv to keep memory low. I also tried to clear the cache (https://docs.astral.sh/uv/concepts/cache/#clearing-the-cache).  Nevertheless the Pod fails with an OOM.

This is still the most likely reason and also explains why none of my attempts helped:

> [...] we need to store details about the available packages in more indexes."

Can you consider a CI mode (e.g. via an env variable) or some other settings that would enable uv to run with lower memory and indeed check less package versions?


---

_Comment by @zanieb on 2025-02-27 14:49_

We can look into it, but unless we have a strong understanding of what's driving memory usage we don't know if there will be an easy lever to reduce it.

---

_Referenced in [astral-sh/uv#15279](../../astral-sh/uv/pulls/15279.md) on 2025-08-14 13:54_

---

_Comment by @alisonatwork on 2025-08-18 08:50_

Following up here instead of on HN so it doesn't get lost...

> We run on-prem k8s and do the pip install stage in a 2CPU/4GB Gitlab runner, which feels like it should be sufficient for the uv:python3.12-bookworm image. We have about 100 deps that aside from numpy/pandas/pyarrow are pretty lightweight. No GPU stuff. I tried 2CPU/8GB runners but it still OOMed occasionally so didn't seem worth using up those resources for the normal case. I don't know enough about the uv internals to understand why it's so expensive, but it feels counter-intuitive because the whole venv is "only" around 500MB.

I haven't yet tried running with concurrency limits because it isn't yet annoying enough for us to bother tweaking settings. Because the OOMs are "random" it's also hard for us to be certain if any given change would improve things without doing a bunch of testing, which we don't really have time for right now.

It would be easier to give concurrency limits a try if there is a sense of exactly which step might be the expensive one. One would hope that concurrent-downloads is not expensive since presumably most of the time is spent waiting for the OS to do i/o and then just writing a fixed size buffer to disk, but perhaps that depend on the size of the buffer and how often they are flushed. Not sure if Rust has a zero-copy i/o path? My suspicion is that concurrent-builds could be risky in a worst case scenario, but most of our deps have wheels so there isn't much to compile. concurrent-installs? Doesn't feel like much. What would you recommend trying first?

---

_Comment by @zanieb on 2025-08-18 22:23_

@alisonatwork can you share a reproduction of the requirements you're using? I started hacking on some coverage in CI at #15279 and can could use a low limit to try to determine what's going on.

cc @konstin regarding which concurrency limit is a good place to start with.

---

_Comment by @alisonatwork on 2025-08-19 03:39_

@zanieb I'll have to confer with my boss if it's okay to share a full requirements.txt from one of our projects in a public forum, but you can probably get a good chunk of the way there if you pip freeze a venv with the latest aiohttp, boto3, celery, confluent-kafka, cryptography, pandas, pyarrow, sqlalchemy and various other usual suspects.

I spent about a half hour digging through the uv source code today to try get a handle on what knob to tweak. Bear in mind I don't have any experience with Rust so can't speak to its particular quirks.

I focused on the downloader simply because that is the part of the code with the biggest default concurrency (50).

The flow seems to start at `uv-installer/src/preparer.rs:get_wheel` which enqueues a download/unzip/build task on - I guess - a thread pool of size 50. If Rust threads have 2MB stack then that's 100MB out of the box, assuming the total number of deps >= 50.

Then it seem to split to `uv-distribution/src/distribution_database.rs:get_wheel` which will pick between download with streaming unzip, or fallback to direct download to tmp file (copy through `tokio::io::BufWriter` with default 8kb bufsize).

In any case it will eventually route through to `uv-extract/src/stream.rs:unzip` which read through a `futures::io::BufReader` with 128kb bufsize and write through a `tokio::io::BufWriter` with up to 1MB bufsize. So potentially for very large zip entries we might have another 50x1 = 50MB here (assuming these buffers are heap-allocated). I'm not sure if behind the scenes the entire entry is kept in memory in order for `compute_hash` to work, but let's assume that it is, then maybe also worst case scenario you have one whole file in memory before moving on to the next entry. For Python source the file sizes are trivial, but for deps which include data blobs or compiled binaries it can get hideous, e.g. some of the shared objects in pyarrow which are >= 20MB. Let's be kind and say 10MB worst case, so 50x10 = 500MB.

All this added together still seems like less than a gig of memory usage. So although it does seem possible that the download concurrency is the biggest opportunity for improvement, it still feels like somewhere in the pipeline memory is being wasted. What I will try on our current CI builds is reduce the download concurrency to 20, see if that make a difference.

Update after adjusting the concurrency and adding some logging:
```
$ uv run --show-settings | grep -A4 concurrency
    concurrency: Concurrency {
        downloads: 20,
        builds: 8,
        installs: 8,
    },
```
Seems that our 2-core runner is being over-estimated by Rust's `std::thread::available_parallelism`, which isn't going to help matters. I will reset downloads to 50 and restrict the builds to 2 instead. We only have a handful of libraries that require a build step, but just to make sure we are working from a standard baseline.

---

_Comment by @konstin on 2025-08-19 08:27_

Thanks you for this thorough investigation!

My main question would be when the OOM happens, is it in the prepare phase or in the install phase? You can builds and when the prepare phase is done in the regular logs, with the verbose logs showing when each package starts and ends.

```
$ uv pip install pandas
Resolved 6 packages in 58ms
      Built pandas==2.3.1
Prepared 2 packages in 42.21s
Installed 6 packages in 8ms
 + numpy==2.3.2
 + pandas==2.3.1
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + six==1.17.0
 + tzdata==2025.2
```

For changing the parallelism, the key variables are [UV_CONCURRENT_DOWNLOADS](https://docs.astral.sh/uv/reference/environment/#uv_concurrent_downloads), [UV_CONCURRENT_INSTALLS](https://docs.astral.sh/uv/reference/environment/#uv_concurrent_installs) and [UV_CONCURRENT_BUILDS](https://docs.astral.sh/uv/reference/environment/#uv_concurrent_builds).

I wonder if it's the builds that cause the OOM? Compilers, especially when building the final release artifact can take a lot of memory (uv's own release build has a peak RSS of >4GB). This would explain how we could OOM on an 8GB machine.

We use two different kinds of parallelism: async with tokio and threadpools with rayon. tokio usually only spawns a handful of actual threads, though the futures themselves can grow large, we try to use future boxing to keep their size in bounds. The download and prepare phase mainly uses tokio, while the rayon threadpool is used mainly in the installation phase.

We're using 4MB stacks due to issues with large futures in debug builds. However, in the rayon thread pool with its 50 workers we runs only sync code without excessively deep call stacks, so we can reduce this to 2MB again (or even 1MB) if it turns out to be the culprit. We've been liberal with using large buffered readers and writers everywhere (unbuffered reads and writes are slow), if that's the problem we can audit for duplicate or overly large buffers.

---

_Comment by @alisonatwork on 2025-08-19 09:33_

The build hypothesis seems like a good one! I've looked back through the last 10-15 OOMed builds and they start off something like this:
```
$ uv pip install -r ci-requirements.txt
Resolved 139 packages in 1.96s
Downloading pip (1.7MiB)
Downloading pyarrow (40.8MiB)
Downloading confluent-kafka (3.7MiB)
...
   Building mysqlclient==2.2.7
...
```
Note I am leaving out a bunch of packages, but the point is around ~20 downloads and ~5 builds down the chain, we get the OOM. We aren't building any heavyweight stuff, but even just those handful of small deps without a wheel might indeed blow out the RAM when several are happening in parallel.

Earlier today I switched UV_CONCURRENT_BUILDS and UV_CONCURRENT_INSTALLS both to 2 in one of our projects, to match the logical cores available to the runner. It's not immediately clear from the logs how those settings have impacted the behavior, because the output still prints `Building packagename==x.y.z` for ~5 packages in a row. Looking at the code in `uv-distribution/src/source/mod.rs` it seems like the `on_build_start` events (which write the log) are fired before the tasks are created rather than when they start work (in `uv-dispatch/src/lib.rs:direct_build`), so seems like the `Building packagename` log is not as useful as the `Built packagename==x.y.z` log that happens `on_build_complete`. Previous to the change we were getting a couple of `Built packagename==x.y.z` logs emitted right up against one another, but now there are usually more `Downloading` logs in between, so that does imply that the concurrency is spacing the tasks out a bit better, although with async processing we'll need a bunch more builds to confirm.

I'll let you know if we see another OOM with the new setting. Thanks for the quick response and additional context!

---

_Comment by @alisonatwork on 2025-08-29 04:05_

Just to follow up on this: in the last 10 days we continued to see OOMs during the initial download/build phase of `uv pip install` in 10-20% of our pipelines. We reduced the UV_CONCURRENT_BUILDS to 1 and the OOMs are still happening. Next attempt is to leave the concurrent builds at 1 and try reducing UV_CONCURRENT_DOWNLOADS to 20. I hope our experience doesn't echo that of @BugsBuggy who even with everything set to 1 still got OOMKilled.

---

_Comment by @konstin on 2025-08-29 08:28_

If you turn on verbose mode (`-v` or even `-vv`) can you see any patterns whether there's a specific package/command involved? Are you building any particularly large packages?

---

_Comment by @shg-rtx on 2025-10-24 07:28_

I initially ran into the OOM issue while running in WSL but it seemed like my WSL install was broken, after a complete reinstall it worked again. I was able to reproduce it with the following commands:

```
uv init --bare 
uv add numpy==1.26.4
```

---

_Comment by @alisonatwork on 2025-11-11 09:32_

Another update on this: since our last configuration change, we have not seen this issue again. So what finally worked for our project was:
```
UV_CONCURRENT_BUILDS=1
UV_CONCURRENT_DOWNLOADS=20
```
That said, we always build with the latest and greatest version of `uv`, so it's also possible that sometime in the versions >= 0.8.14 things have improved with regard to memory usage.

I don't have a lot more information to go on at this point, and we don't really have bandwidth to set up a dedicated load test which keep hammering installs with verbose logging turned on to see if there is a pattern to when it fails. But in general it does seem like an issue of "bad luck" during concurrent downloads and builds, perhaps combined with resource constraints on the k8s cluster causing nodes to more aggressively OOM kill.

I still feel like the system itself should keep a better check on how much memory it is using and not depend on the user to randomly change concurrency settings around and hope for the best, but I'm not sure if that's easily possible in Rust. Maybe a safer option would be to pick less aggressive defaults - at least when it detects it's running in a containerized environment - since from the user side it still seems to run installs acceptably fast. I would say it's less annoying for someone to set up a CI pipeline which has slow builds that can be sped up by adjusting the concurrency than for them to have out-of-the-box fast builds that randomly fail.

---

_Comment by @zanieb on 2025-11-12 18:06_

If the builds are the issue, it seems ~impossible for us to account for that. We're building arbitrary third-party packages, they can choose to use as much memory as they want at any given moment, unfortunately.

---
