<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>document that memory exhaustion is possible when using parallelism - BurntSushi/ripgrep #1189</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>document that memory exhaustion is possible when using parallelism</h1>

    <div class="meta">
        <span class="state-icon state-closed"></span>
        <a href="https://github.com/BurntSushi/ripgrep/issues/1189">#1189</a>
        opened by <a href="https://github.com/domenukk">@domenukk</a>
        on 2019-02-07 22:12
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/domenukk">@domenukk</a></div>
            <div class="timeline-body"><h4>What version of ripgrep are you using?</h4>
<p>ripgrep 0.10.0 (rev 8a7db1a918)
-SIMD -AVX (compiled)
+SIMD +AVX (runtime)</p>
<h4>How did you install ripgrep?</h4>
<p>Precompiled msvc binary for Windows-x64</p>
<h4>What operating system are you using ripgrep on?</h4>
<p>Windows 7, some current pach level</p>
<h4>Describe your question, feature request, or bug.</h4>
<p>If I redirect the ripgrep output to a file on windows, the memory usage of <code>rg.exe</code> increases slowly but steady, probably with each found element.
After a few GB, <code>rg.exe</code> then crashes with a segmentation fault.
Inside <code>procmon</code>, I see the results seem to only be flushed to the file after the crash.
A workaround is to force single threading with <code>-j1</code>. This seems to be directly related to <a href="https://github.com/BurntSushi/ripgrep/issues/4">this old issue</a></p>
<p>Or are the reads and hits simply too fast to be written to disk if multi threaded?</p>
<h4>If this is a bug, what are the steps to reproduce the behavior?</h4>
<p>Trying to extract all mail addresses from a large current password leak with 130k files, 8k folders and 1.62TB in total (with different filessizes) crashes rg.</p>
<p>Inside git bash, run:</p>
<pre><code>$ ./rg.exe -uu --no-filename -o '[a-zA-Z0-9.%&amp;’*+/=?^_`{}~-]+@[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*' ./Passwords/ &gt; all_mails.txt
</code></pre>
<p>For obvious reasons, I cannot include the corpus here.</p>
<h4>If this is a bug, what is the actual behavior?</h4>
<pre><code>$ ./rg.exe --debug -uu --no-filename -o '[a-zA-Z0-9.%&amp;’*+/=?^_`{}~-]+@[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*' ./Passwords/ &gt; all_mails.txt
DEBUG|grep_regex::literal|grep-regex\src\literal.rs:110: required literal found: &quot;@&quot;
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
memory allocation of 5795684597665262959 bytes failedSegmentation fault
</code></pre>
<h4>If this is a bug, what is the expected behavior?</h4>
<p>The user should be able to <code>rg</code> multi-threaded on any kind of large dataset.
Maybe a synchronization point when the buffer gets too big or the choice to disable caching (I don't need the output in order, for example) could be options.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from "Buffers never seem to be flushed on multithreading" to "Buffers don't seem to be flushed (quickly enough?) on multithreading" by @domenukk on 2019-02-07 22:14</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 22:26</div>
            <div class="timeline-body"><p>This happens when the search results for a <em>single</em> file exceed the amount of memory available. This is fundamentally a result of combining parallelism and the requirement that the output of each file is not interleaved. It sounds like you said you'd be OK with the output from different files being interleaved, but I'm not keen on adding this option to ripgrep. Instead, you have a few work-arounds available to you, assuming we've correctly diagnosed the problem:</p>
<ul>
<li>Use parallelism via <code>xargs</code> or some other tool, just like you would with standard grep. e.g., <code>find ./ -print0 | xargs -0 rg foo</code>.</li>
<li>Use ripgrep's <code>-m/--max-count</code> flag to limit the number of matching lines it prints per file. This could be quite undesirable since it makes it hard to know whether a match was missed or not.</li>
</ul>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">question</span> added by @BurntSushi on 2019-02-07 22:26</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/domenukk">@domenukk</a> on 2019-02-07 23:33</div>
            <div class="timeline-body"><p>Thank you for your quick response! Awesome!
Indeed there is a file with 80 gb which might match your description of a single large file pretty well.
I guess I'll try my luck with the workarounds then and see what happens.</p>
<p>That being said, it could still be handy to have a &quot;single file, multiple workers, as fast as possible, interleave if you must&quot; grep mode for these rare occasions - as long as a single result always arrives in one piece in the output. But the occasion might be rare enough.</p>
<p>And a different idea: print which files it choked on when it crashed (just to have fewer github support issues)</p>
<p>Keep up the good work ;)</p>
<p>(oh off-topic, that it tries to allocate 5 exabyte seems like a fun overflow somewhere below the rust layer...)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 23:42</div>
            <div class="timeline-body"><p>Rust's standard library doesn't allow one to easily recover from allocation failure, so it's impractical to print the file on which this choked. Of course, I agree it would be nice to improve failure modes, but I think we're stuck here.</p>
<p>Your proposed option is undoubtedly handy, but it's not a good fit since it's an extraordinarily niche feature with some simple work-arounds.</p>
<p>I'm mark this ticket as a doc bug and find a place to add a note about memory exhaustion being possible when parallelism is enabled, and document the work-arounds.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">doc</span> added by @BurntSushi on 2019-02-07 23:42</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">question</span> removed by @BurntSushi on 2019-02-07 23:42</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from "Buffers don't seem to be flushed (quickly enough?) on multithreading" to "document that memory exhaustion is possible when using parallelism" by @BurntSushi on 2019-02-07 23:43</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 23:44</div>
            <div class="timeline-body"><p>Note that memory exhaustion is not unique to ripgrep, or even parallelism. Both grep and ripgrep are subject to memory exhaustion errors when searching files that have a single line that exceeds available memory. For example, something like <code>grep ZQZQZQZQZQ /dev/sda -c -a</code> is one way I've caused this to happen fairly reliably.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by @BurntSushi on 2019-04-14 23:29</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-12 16:42:49 UTC
    </footer>
</body>
</html>
