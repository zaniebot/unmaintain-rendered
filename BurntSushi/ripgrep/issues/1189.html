<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>document that memory exhaustion is possible when using parallelism - BurntSushi/ripgrep #1189</title>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="back-link">
        <a href="../../../index.html">Back to index</a>
    </div>

    <h1>document that memory exhaustion is possible when using parallelism</h1>

    <div class="meta">
        <span class="state-icon state-closed"></span>
        <a href="https://github.com/BurntSushi/ripgrep/issues/1189">#1189</a>
        opened by <a href="https://github.com/domenukk">@domenukk</a>
        on 2019-02-07 22:12
    </div>

    <div class="timeline">
        <div class="timeline-entry">
            <div class="timeline-header"><a href="https://github.com/domenukk">@domenukk</a></div>
            <div class="timeline-body">What version of ripgrep are you using?
<p>ripgrep 0.10.0 (rev 8a7db1a918)
-SIMD -AVX (compiled)
+SIMD +AVX (runtime)</p>
How did you install ripgrep?
<p>Precompiled msvc binary for Windows-x64</p>
What operating system are you using ripgrep on?
<p>Windows 7, some current pach level</p>
Describe your question, feature request, or bug.
<p>If I redirect the ripgrep output to a file on windows, the memory usage of <code>rg.exe</code> increases slowly but steady, probably with each found element.
After a few GB, <code>rg.exe</code> then crashes with a segmentation fault.
Inside <code>procmon</code>, I see the results seem to only be flushed to the file after the crash.
A workaround is to force single threading with <code>-j1</code>. This seems to be directly related to <a href="https://github.com/BurntSushi/ripgrep/issues/4">this old issue</a></p>
<p>Or are the reads and hits simply too fast to be written to disk if multi threaded?</p>
If this is a bug, what are the steps to reproduce the behavior?
<p>Trying to extract all mail addresses from a large current password leak with 130k files, 8k folders and 1.62TB in total (with different filessizes) crashes rg.</p>
<p>Inside git bash, run:</p>
<pre><code>$ ./rg.exe -uu --no-filename -o &#x27;[a-zA-Z0-9.%&amp;’*+/=?^_`{}~-]+@[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*&#x27; ./Passwords/ &gt; all_mails.txt
</code></pre>
<p>For obvious reasons, I cannot include the corpus here.</p>
If this is a bug, what is the actual behavior?
<pre><code>$ ./rg.exe --debug -uu --no-filename -o &#x27;[a-zA-Z0-9.%&amp;’*+/=?^_`{}~-]+@[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*&#x27; ./Passwords/ &gt; all_mails.txt
DEBUG|grep_regex::literal|grep-regex\src\literal.rs:110: required literal found: &quot;@&quot;
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
DEBUG|globset|globset\src\lib.rs:429: built glob set; 0 literals, 0 basenames, 8 extensions, 0 prefixes, 0 suffixes, 0 required extensions, 0 regexes
memory allocation of 5795684597665262959 bytes failedSegmentation fault
</code></pre>
If this is a bug, what is the expected behavior?
<p>The user should be able to <code>rg</code> multi-threaded on any kind of large dataset.
Maybe a synchronization point when the buffer gets too big or the choice to disable caching (I don&#x27;t need the output in order, for example) could be options.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from &quot;Buffers never seem to be flushed on multithreading&quot; to &quot;Buffers don&#x27;t seem to be flushed (quickly enough?) on multithreading&quot; by <a href="https://github.com/domenukk">@domenukk</a> on 2019-02-07 22:14</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 22:26</div>
            <div class="timeline-body"><p>This happens when the search results for a <em>single</em> file exceed the amount of memory available. This is fundamentally a result of combining parallelism and the requirement that the output of each file is not interleaved. It sounds like you said you&#x27;d be OK with the output from different files being interleaved, but I&#x27;m not keen on adding this option to ripgrep. Instead, you have a few work-arounds available to you, assuming we&#x27;ve correctly diagnosed the problem:</p>
<ul>
<li>Use parallelism via <code>xargs</code> or some other tool, just like you would with standard grep. e.g., <code>find ./ -print0 | xargs -0 rg foo</code>.</li>
<li>Use ripgrep&#x27;s <code>-m/--max-count</code> flag to limit the number of matching lines it prints per file. This could be quite undesirable since it makes it hard to know whether a match was missed or not.</li>
</ul>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">question</span> added by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 22:26</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/domenukk">@domenukk</a> on 2019-02-07 23:33</div>
            <div class="timeline-body"><p>Thank you for your quick response! Awesome!
Indeed there is a file with 80 gb which might match your description of a single large file pretty well.
I guess I&#x27;ll try my luck with the workarounds then and see what happens.</p>
<p>That being said, it could still be handy to have a &quot;single file, multiple workers, as fast as possible, interleave if you must&quot; grep mode for these rare occasions - as long as a single result always arrives in one piece in the output. But the occasion might be rare enough.</p>
<p>And a different idea: print which files it choked on when it crashed (just to have fewer github support issues)</p>
<p>Keep up the good work ;)</p>
<p>(oh off-topic, that it tries to allocate 5 exabyte seems like a fun overflow somewhere below the rust layer...)</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 23:42</div>
            <div class="timeline-body"><p>Rust&#x27;s standard library doesn&#x27;t allow one to easily recover from allocation failure, so it&#x27;s impractical to print the file on which this choked. Of course, I agree it would be nice to improve failure modes, but I think we&#x27;re stuck here.</p>
<p>Your proposed option is undoubtedly handy, but it&#x27;s not a good fit since it&#x27;s an extraordinarily niche feature with some simple work-arounds.</p>
<p>I&#x27;m mark this ticket as a doc bug and find a place to add a note about memory exhaustion being possible when parallelism is enabled, and document the work-arounds.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">doc</span> added by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 23:42</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Label <span class="label">question</span> removed by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 23:42</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Renamed from &quot;Buffers don&#x27;t seem to be flushed (quickly enough?) on multithreading&quot; to &quot;document that memory exhaustion is possible when using parallelism&quot; by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 23:43</div>
        </div>
        <div class="timeline-entry">
            <div class="timeline-header">Comment by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-02-07 23:44</div>
            <div class="timeline-body"><p>Note that memory exhaustion is not unique to ripgrep, or even parallelism. Both grep and ripgrep are subject to memory exhaustion errors when searching files that have a single line that exceeds available memory. For example, something like <code>grep ZQZQZQZQZQ /dev/sda -c -a</code> is one way I&#x27;ve caused this to happen fairly reliably.</p>
</div>
        </div>
        <div class="timeline-entry">
            <div class="event">Closed by <a href="https://github.com/BurntSushi">@BurntSushi</a> on 2019-04-14 23:29</div>
        </div>
    </div>

    <footer>
        Synced at 2026-01-20 18:48:08 UTC
    </footer>
</body>
</html>
